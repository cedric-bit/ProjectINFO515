infoh515 big data distributed data management scalable analytics preliminaries dimitrissacharidisgianlucabontempi 20242025 lecture outline generalcourseinformation 1 general course information course objective introducethefundamentalnotionsprinciplesandresearchresultsconcerning modernscalableandfaulttolerantwaysformanagingandanalyzingmassive amountsofdatausingparallelanddistributedsystems keyquestions whatisbigdatawhatarethecharacteristicsofsuchdata whatisacomputecluster howdoclustersstoredata howaretheyprogrammed whatarenotionsofefficiencyfordistributedalgorithms whatisbigdataanalytics howdoyouperformmachinelearningonbigdata 3 competences develop aftersuccessfulcompletionofthiscourseyoushouldbeableto 1 understandthecharacteristicsofbigdataandthechallengestheserepresent 2 knowtheprincipalarchitecturesofbigdatamanagementandanalytics systemsbeabletoexplainthepurposeofeachtheircomponentsandbeable torecognizeandexplainthekeypropertiesstrengthsandlimitationsofeach typeofsystemandtheircomponents 3 understandthekeybottlenecksinmanagingandanalyzingmassiveamountsof dataandbefamiliarwithmodernalgorithmsforovercomingthesebottlenecks usingparallelanddistributedcomputation 4 activelyusethisalgorithmicknowledgeinthedesignandimplementationof applicationsthatsolvecommondatamanagementandanalyticsproblems usingdifferenttypesofbdmas 5 buildaplicationsusingspecificinstancesofeachtypeofbdmas 4 2 acourseonhowtoinstallbigdataframeworks 3 anexhaustiveanddetailedlookintoallpossiblebigdataframeworksthat currentlyexist course 1 acourseonhowtobuildcomputeclusters 5 3 anexhaustiveanddetailedlookintoallpossiblebigdataframeworksthat currentlyexist course 1 acourseonhowtobuildcomputeclusters 2 acourseonhowtoinstallbigdataframeworks 5 course 1 acourseonhowtobuildcomputeclusters 2 acourseonhowtoinstallbigdataframeworks 3 anexhaustiveanddetailedlookintoallpossiblebigdataframeworksthat currentlyexist 5 infoh515 2 parts part1distributedmanagement lecturesbyprof dsacharidisulb dimitrissacharidisulbbe part2scalableanalytics lecturesbyprof gbontempiulb gianlucabontempiulbbe 6 prerequisites required goodprogrammingskills introductorycourseondatamanagement introductorycourseonalgorithmsanddatastructures introductorycourseonmachinelearning atulbinfof422statisticalfoundationsofmachinelearning atvub1002080cnrmachinelearningor4004728dnrtechniquesof artificialintelligence 7 infoh515 organization thecourseisorganizedasamixtureof lectures readingassignments projectwork 8 infoh515 organization part1 distributed management part2 scalableanalytics theory theory fridays10h12h fridays10h12h 14feb21mar 4apr23may ulbsolbosch ulbplainepforum sub4136sdc2206 exercises exercises wednesdays10h12h wednesdays10h12h tba 26feb5mar12mar19mar ulbsolboschtba checkuvandulbschedulesforscheduleandroomupdates httpswwwulbbeenschedules 9 infoh515 syllabus thesyllabusavailableatthevirtualuniversityconsistsof slides associatedreadingassignments coursematerialexercisesreadingassignmentsareallpublishedonthe ulbvirtualuniversity httpsuvulbacbecourseviewphpid124821 checkregularlyforupdates 10 infoh515 evaluation project60offinalscore writtenexam40offinalscore 11

infoh515 big data distributed management lecture 1 introduction hdfs mapreduce dimitrissacharidis lecture outline whatisbigdata howisbigdataused bigdatacenters hdfs mapreduce 1 big data big data like teenage sex everyone talks nobody really knows everyone thinks everyone else everyone claims dan ariely 3 history big three big data data analytics data science 4 data analytics data science history big three big data 4 data analytics data science big three big data history 4 source economist february 25 2010 data deluge eighteen months ago li fung firm manages supply chains retailers saw 100 gigabytes information flow network day amount increased tenfold 2009 american drone aircraft flying iraq afghanistan sent back around 24 years worth video footage new models deployed year produce ten times many data streams predecessors 2011 produce 30 times many source economist february 25 2010 data deluge everywhere look quantity information world soaring according one estimate mankind created 150 exabytes billion gigabytes data 2005 year create 1200 exabytes merely keeping flood storing bits might useful difficult enough analysing spot patterns extract useful information harder still source economist february 25 2010 data deluge 2030s computer data storage may surpass 1 yottabyte 1024 largest number official metric prefix international bureau weights measures bipm paris recommends new namesronna queccaas prefixes 1027and 1030 1 gigabyte 109 bytes 1 terabyte 1012 bytes 1 petabyte 1015 bytes 1 exabyte 1018 bytes 1 zetabyte 1021 bytes 1 yottabyte 1024 bytes 1 ronnabyte 1027 bytes 1 queccabyte 1030 bytes httpswwwscienceorgcontentarticleyouknowkilomegaandgigametricsystemreadyronnaandquecca really problematic assume youre google beginning 2000s want index web really problematic although hard accurately determine size web point time safe say consists hundreds billions individual documents continues grow assume web contain 100 billion documents average document size 4 kb compression web 400 tb size resulting inverted web search index depends specific implementation tends order magnitude original repository source datacenter computer luiz andré barroso google jimmy clidaras google urs hölzle google big data definitions 14 bigdataisatermencompassingtheuseoftechniquestocapturepro cess analyseandvisualize potentially largedatasets reasonable timeframe accessible standard technologies extension platform tools software used purpose collectively calledbigdatatechnologies nessi1 1theeuropeantechnologyplatformdedicatedtosoftwareservicesanddata 5 big data definitions 24 bigdata refers datasetswhosesizeisbeyondtheabilityoftypical databasesoftwaretoolstocapture store manage andanalyze definition intentionally subjective incorporates moving defi nition big dataset needs order considered big data assume technology advances time size datasetsthatqualifyasbigdatawillalsoincrease mckinseyglobalinstitute2 2bigdatathenextfrontierforinnovationcompetitionandproductivityjune2011 6 big data definitions 34 big data data sets voluminous complex tra ditional data processing application software inadequate deal big data challenges include capturing data data storage data analysis search sharing transfer visualization querying updat ingandinformationprivacy lately thetermbigdatatendstorefertotheuseofpredictiveanalyt ics userbehavioranalytics orcertainotheradvanceddataanalytics methodsthatextractvaluefromdata andseldomtoaparticularsize ofdataset wikipedia 7 big data definitions 44 bigdatareferstoextremelylargedatasetsthatcannotbeeasilyman aged processed analyzed using traditional data processing meth ods characterized three vs volume variety velocity describe sheer amount data diversity data types speed generated big data technologies enable businessesandresearcherstogaininsightsandmakedatadrivende cisionsfromthesevastdynamicdatasets chatgpt022025 8 history big three big data data analytics data science 9 history big data data science big three data analytics 9 data analytics definition data analysis also known analysis data data analytics process inspecting cleansing transforming modeling data goal discovering useful information suggesting conclu sionsandsupportingdecisionmaking wikipedia 10 11 history big three big data data analytics data science 12 history big data data analytics big three data science 12 data science definitions 13 datascienceisthestudyofthegeneralizableextractionofknowledge fromdata vasantdhar3 fromthearticle thetermscienceimpliesknowledgegainedthroughsystematicstudy adatascientistrequiresanintegratedskillsetspanningmathematics machinelearningartificialintelligencestatisticsdatabasesand optimizationalongwithadeepunderstandingofthecraftofproblem formulationtoengineereffectivesolutions 3datascienceandpredictionscommunicationsoftheacm56122014 13 data science definitions 23 data science also known datadriven science interdisci plinary field scientific methods processes systems extract knowledge insights data various forms either structuredorunstructured data science concept unify statistics data analysis relatedmethodsinordertounderstandandanalyzeactualphenom ena data employs techniques theories drawn many fieldswithinthebroadareasofmathematicsstatisticsinformationsci enceandcomputerscience wikipedia 14 data science definitions 33 data science field combines statistical analysis machine learning data processing techniques extract valuable insights large complex datasets involves collecting cleaning analysing interpreting data inform decisionmaking solve problems across various industries data scientists use programming mathematicsanddomainexpertisetouncoverpatternsandtrendsin data chatgpt022025 15 conclusion recallthecourseobjective introducethefundamentalnotions principlesandresearchresultsconcerning modernscalableandfaulttolerantwaysfor managingandanalyzingmassiveamountsof datausingparallelanddistributedsystems thecourseishenceconcernedwithbigdata andbigdataanalyticsandthusrelevantfor datascience 16 challenges big data sourcehttpwwwibmbigdatahubcomsitesdefaultfilesinfographic_file 17 4vsofbigdatajpg big data used big data used slidestakenfromhttpswwwslidesharenetdellbigdatausecases3601989 19 big data use cases 1 optimize funnel 5 market basket conversion analysis pricing optimization 2 behavioral analytics 6 predict security threats 3 customer segmentation 7 fraud detection 4 predictive 8 industry specific support 1 optimize funnel conversion big data analytics allows companies track leads entire sales conversion process click adword ad final transaction order uncover insights conversion process improved company industry tmobile communication employees type optimize funnel 38000 conversion purpose tmobile uses multiple indicators billing sentiment analysis order identify customers upgraded higher quality products well identify high lifetime customervalue team focus retaining customers tweet company industry celcom axiata communication berhad employees type optimize funnel enterprise conversion purpose celcom axiata berhad adopted big data solution order improve customer retention boost market share improving marketing campaign process company used realtime data create personalized campaign customer based products offers customer would want need tweet 2 behavioral analytics access data consumer behavior companies learn prompts customer stick around longer well learn customers characteristics purchasing habits order improve marketing efforts boost profits company industry mastercard finance type employees behavioral 67000 analytics purpose 18 billion customers mastercard unique position able analyze behavior customers stores also thousands retailers company teamed mu sigma collect analyze data shoppers behavior provide insights finds retailers benchmarking reports tweet company industry time warner cable entertainment type employees behavioral analytics 34000 customer segmentation purpose services like hulu netflix competing viewers attention time warner collects data frequently customers tune effect bandwidth consumer behavior customer engagement peak usage times order improve service increase profits company also segments customers advertisers correlating viewing habits public datasuch voter registration informationin order launch highly targeted campaigns specific locations demographics tweet company industry nestlé food beverage type employees behavioral 330000 analytics purpose customer complaints pr crises become difficult handle thanks social media better keep track customer sentiment said company online nestle created 247 monitoring centre listen conversations company products social media company actively engage post online order mitigate damage build customer loyalty tweet 3 customer segmentation accessing data consumer multiple sources social media data transaction history companies better segment target customers start make personalized offers customers company industry heineken food beverage type employees customer 64252 segmentation purpose thanks partnerships google facebook heineken access vasts amounts data customers uses create realtime personalized marketing messages one project provides realtime content fans happen watching sponsored event tweet company industry spotify entertainment type employees customer 5000 segmentation behavioral analytics purpose spotify uses data user profiles users playlists historical data music played provide recommendations user combining data millions users spotify able make recommendations even particular user doesnt extensive history site tweet 4 predictive support sensors machinegenerated data companies identify malfunction likely occur company preemptively order parts make repairs order avoid downtime lost profits company industry southwest airlines travel type employees predictive support 45000 purpose southwest analyses sensor data planes order identify patterns indicate potential malfunction safety issue allows airline address potential problems make necessary repairs without interrupting flights putting passengers danger tweet company industry engine yard cloud storage type employees predictive support 130 purpose engine yard provides big data analytics users monitor performance applications real time pinpoint problems infrastructure optimize platform correct performance issues tweet 5 market basket analysis pricing optimization quickly pulling data together multiple sources retailers better optimize product selection pricing well decide target ads company industry cocacola co food type employees market basket 146200 analysis purpose cocacola uses algorithm ensure orange juice consistent taste throughout year algorithm incorporates satellite imagery crop yields consumer preferences details flavours make particular fruit order determine juice blended tweet 6 predict security threats big data analytics track trends security breaches allow companies proactively go threats strike company industry rabobank finance employees type 27000 predict security threats purpose rabobank analysed criminal activities atms determine factors increased risk becoming victimized discovered proximity highways weather conditions season affect risk security threat tweet company industry amazon online retail employees type predict security 110000 threats purpose 15 billion items catalog amazon lot product keep track protect uses cloud system s3 predict items likely stolen better secure warehouses tweet 7 fraud detection financial firms use big data help identify sophisticated fraud schemes combining multiple points data company industry zions bank finance employees type 2700 fraud detection purpose zions bank uses data analytics detect anomalies across channels indicate potential fraud fraud team receives data 140 sourcessome realtimeto monitor activity customer makes mobile banking transaction time branch transaction tweet company industry discovery health insurance employees type 5000 fraud detection purpose discovery health uses big data analytics identify fraudulent claims possible fraudulent prescriptions example identify healthcare provider charging expensive procedure actually performed tweet 8 industry specific virtually every industry invested big data help solve specific challenges industries face healthcare example uses big data improve patient outcomes agriculture uses data boost crop yields company industry kayak travel type employees industry specific 101 purpose kayak uses big data analytics create predictive model tells users price particular flight go within next week system uses one billion search queries find cheapest flights well popular destinations busiest airports algorithm constantly improved tracking flights see predictions correct tweet company industry aurora health care health care employees type 30000 industry specific purpose aurora collects internal well national data order create benchmark healthcare quality also analyzes data groups patients similar medical conditions reveal trends diseases identify right candidates medical research finally realtime data analysis allows aurora predict improve patient outcomes far reduced readmissions 10 percent tweet company industry shell oil employees type 87000 industry specific purpose shell uses sensor data map oil gas wells order increase output boost efficiency operations data received sensors analyzed artificial intelligence rendered 3d 4d maps tweet takeaway examples data stored processed analyzed turn value 20 big datacenters process big data assume youre google beginning 2000s want index web execution environment 1997 bunch computers web index fit single computer redundancy ensure availability execution environment 1999 compute servers consist multiple cpus possibly multiple cores per cpu attached hard disks google corckboard servers collected racks rack highspeed ethernet connections least 1gbps connect servers rack together execution environment currently image google datacenter mons belgium racks connected together central network switches using multigbps redundant links access data racks slower data computersame rack many racks together form data center discussion compute node kept simple design midrange computers much cheaper powerful highrange computers consume less energy google lots characteristics figure source datacenter computer characteristics capacity amount data figure source datacenter computer store per serverrackdatacenter characteristics latency time takes fetch data figure source datacenter computer item asked local machineanother server rackanother server different rack characteristics bandwidth speed data figure source datacenter computer transferred machineanother server rackanother server different rack characteristics conclusion huge storage capacity latency racks 110 latency rack level 110 latency server level bandwidth racks 110 bandwidth rack level ½ 110 bandwidth server level figure source datacenter computer gain parallelism let us consider maximal aggregate bandwidth speed analyze data parallel assuming ideal data distribution servers disks embarrassingly parallel example count number times word belgium appears documents web server multiple cpus read multiple disks parallel server analyze many documents parallel end sum perserver counters done fast gain parallelism let us consider maximal aggregate bandwidth speed analyze data parallel assuming ideal data distribution servers disks component max aggr bandwidth 1 hard disk 100 mbsec 1 gbps server 12 hard disks 12 gbsec 12 gbps rack 80 servers 96 gbsec 768 gbps clusterdatacenter 30 racks 288 tbsec 23 tbps scanning 400tb hence takes 138 secs 23 minutes scanning 400tb sequentially 100 mbsec takes 4629 days challenges 12 scalable software development allow growth without requiring rearchitecting algorithmapplications auto scale challenges 22 faulttolerance 1000s machines failures happen every day faulttolerance typically addressed redundancy andor reexecution annualfailurerateafrforagivenmttf hoursin1year 36524 afr 0876 mttf 106 expectednumberofdiskfailuresperyearinadatacenterwith100000 disks disks disks disksafr1050876876 2 year day faulttolerance motivational example meantimetofailuremttfoftypicalharddisk 1000000hours114years 22 expectednumberofdiskfailuresperyearinadatacenterwith100000 disks disks disks disksafr1050876876 2 year day faulttolerance motivational example meantimetofailuremttfoftypicalharddisk 1000000hours114years annualfailurerateafrforagivenmttf hoursin1year 36524 afr 0876 mttf 106 22 faulttolerance motivational example meantimetofailuremttfoftypicalharddisk 1000000hours114years annualfailurerateafrforagivenmttf hoursin1year 36524 afr 0876 mttf 106 expectednumberofdiskfailuresperyearinadatacenterwith100000 disks disks disks disksafr1050876876 2 year day 22 googles solutions new programming models frameworks distributed scalable data analysis name purpose open source impl google file system distributed file system scalable storage highthroughput retrieval apache hadoop map reduce programming model execution hdfs environment generalpurpose mr distributed batch processing bigtable nosql database apache hbase dremel f1 query language interactive sql apache sparkdrill like analysis structured datasets lots research ongoing highlevel design described series papers implementation available hdfs hdfs hadoop distributed file system hdfs architecture hdfs masterslave architecture master namenode nn manages file system regulates access files clients slaves datanodes dn usually one per server cluster manage storage attached server run files transparently namenode metadatafilename replication factor block ids broken blocks usersdsdat0txt r2 13 default 128 mb usersdsdat1txt r3 2 4 5 blocks replicated across datanodes datanodes replication rackaware dat0txt 1 3 dat1txt 2 4 5 1 2 2 5 1 3 2 4 4 5 3 4 5 hdfs architecture hdfs masterslave architecture master namenode nn manages file system regulates access files clients slaves datanodes dn usually one per server cluster manage storage attached server run optimized namenode metadatafilename replication factor block ids large files usersdsdat0txt r2 13 read throughput usersdsdat1txt r3 2 4 5 appending writes files appendonly datanodes replication ensures durability availability 1 2 2 5 1 3 2 4 throughput 4 5 3 4 5 hdfs common pitfalls hdfs optimized large files single hdfs block 128 mb per default make sense store files smaller 128 mb hdfs files immutable optimized sequential access new files created appended random access file created immutable changing file requires creating hdfs implementation namenode datanode pieces software designed run commodity machines machines typically run gnulinux operating system os hdfs built using java language machine supports java run namenode datanode software usage highly portable java language means hdfs deployed wide range machines source hadoop documentation implies clients need install jar file access hdfs typical hdfs commands binhadoop fs ls binhadoop fs mkdir binhadoop fs copyfromlocal binhadoop fs copytolocal binhadoop fs movetolocal binhadoop fs rm mapreduce mapreduce simplified data processing large clusters reaction complexity designed new abstraction allows us express simple computations trying perform hides messy details parallelization fault tolerance data distribution load balancing library abstraction inspired map reduce primitives present lisp many functional languages realized computations involved applying map operation logical record input order compute set intermediate keyvalue pairs applying reduce operation values shared key order combine derived data appropriately use functional model user specified map reduce operations allows us parallelize large computations easily use reexecution primary mechanism fault tolerance mr computational model mr job specified two functions map reduce input keyvalue pair represents logical record input data source case file could line input source database table could record map key1 value1 listkey2 value2 single input keyvalue pair may result zero output keyvalue pairs mr computational model mr job specified two functions map reduce reduce function called per unique map output key receives list values emitted key reduce key2 listvalue2 listkey3 value3 like map function reduce output zero many keyvalue pairs mr computational model word occurring document web count number occurrences across documents def mapdocid line word line yield word 1 def reduceword listofoccnumbers yield word sumlistofoccnumbers mr opportunity parallelism spawn multiple copies map function called map tasks parallel one keyvalue pair likewise spawn multiple copies reduce function called reduce tasks parallel one unique key output map input map output shufflesort reduce input cat 1 doc1 map dog 1 cat 11 reduce turtle 1 dog 11 reduce doc2 map cat 1 belgium 1 turtle 11 reduce doc3 map du ogrt le 1 1 belgium 1 reduce mr execution hadoop v1 architecture master jobtracker accepts jobs decomposes map reduce tasks schedules remote execution child nodes slave tasktracker accepts tasks jobtracker spawns child processes actual work idea ship computation data namenode jobtracker job1 map task 1 jobtracker accepts mr map task 2 jobs reduce task 1 input hdfs file reduce task 2 map task created job2 datanodes tasktrackers block sent node holding block execution map output written 1 2 2 5 1 3 2 4 local disk 4 5 3 4 5 mr execution hadoop v1 architecture master jobtracker accepts jobs decomposes map reduce tasks schedules remote execution child nodes slave tasktracker accepts tasks jobtracker spawns child processes actual work idea ship computation data namenode jobtracker job1 map task 1 jobtracker creates map task 2 reduce tasks intelligently reduce task 1 reduce task 2 reduce tasks read map job2 datanodes tasktrackers outputs network write output back hdfs 1 2 2 5 1 3 2 4 4 5 3 4 5 mr execution hadoop v1 architecture master jobtracker accepts jobs decomposes map reduce tasks schedules remote execution child nodes slave tasktracker accepts tasks jobtracker spawns child processes actual work idea ship computation data namenode jobtracker job1 map task 1 load balancing map task 2 jobtracker monitors reduce task 1 stragglers may spawn reduce task 2 additional map job2 datanodes tasktrackers datanodes hold block replica whichever node completes first allowed proceed 1 2 2 5 1 3 2 4 others isare killed 4 5 3 4 5 mr execution hadoop v1 architecture master jobtracker accepts jobs decomposes map reduce tasks schedules remote execution child nodes slave tasktracker accepts tasks jobtracker spawns child processes actual work idea ship computation data namenode jobtracker job1 map task 1 failures clusters map task 2 1000s nodes hardware reduce task 1 failures occur frequently reduce task 2 mechanism job2 datanodes tasktrackers load balancing allows cope failures 1 2 2 5 1 3 2 4 4 5 3 4 5 mr scheduling caveats hadoop best run map task node input data resides hdfs hence exploiting data locality sometimes however nodes hosting hdfs block replicas map tasks input split busy case job scheduler look free map slot node rack one blocks occasionally even possible offrack node used results interrack network transfer mr operation detail number map tasks mapper actually equal number splits split logical division input data block physical division data hdfs default block size default split size figure source hadoop definitive guide mr operation detail map task partitions output number partitions equals number reducers run later partition sorted already mapside one partition still many distinct keys reducers copy respective partition mappers merge one big sorted file net result distributed mergesort map reduce phases figure source hadoop definitive guide mr operation detail optionally one specify combine function already performs partial mapside reduction aim decrease size data transferred map key1 value1 listkey2 value2 combine key2 listvalue2 listkey2 value2 reduce key2 listvalue2 listkey3 value3 figure source hadoop definitive guide combiner example word occurring document web count number occurrences across documents def mapdocid line word line yield word 1 def combineword listofones yield word sumlistofones def reduceword listofoccnumbers yield word sumlistofoccnumbers writing mr programs hadoopmriswritteninjava assuchthedefaultwaytowriteamr jobisthroughitsjavaapi becausethiscanbecumbersomehadoopmralsoexposesthe streaminginterfacewhichallowsexecutableswritteninanarbitrary languageshellscriptpythonctoimplementthemapandreduce logic inthatcasetheexecutablereadsitsinputfromstdinandwritesits outputtostdout 25 references sghemawatehgobioffandhtleung thegooglefilesystem httpresearchgooglecomarchivegfssosp2003pdf hdfsarchitecture httphadoopapacheorgdocsstable hadoopprojectdisthadoophdfshdfsdesignhtml jdeanandsghemawat mapreduce simplifieddataprocessingonlarge clusters communicationsoftheacm2008 labarrosojclidarasuhölzle thedatacenterasacomputer httpwwwmorganclaypoolcomdoiabs102200 s00516ed2v01y201306cac024 twhite hadoop thedefinitiveguide oreillymedia2010 26 questions 27 acknowledgements basedoncontentcreatedbyjanhiddersandstijnvansummeren 28

infoh515 bigdata distributedmanagement lecture 2 distributed processing spark dimitrissacharidis outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 296 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 396 supportsefficientexecutionofdifferent kindsofworkloadsbatchstreaming machinelearningandsql whatisspark fileusershiddersdocumentseducationbig data distribu apachesparkisafastinmemory distributeddataprocessingengine 496 1 1 08022018 1334 supportsefficientexecutionofdifferent kindsofworkloadsbatchstreaming machinelearningandsql whatisspark fileusershiddersdocumentseducationbig data distribu apachesparkisafastinmemory distributeddataprocessingengine speed runcomputationsinmemory 100timesfasterthanmapreducewhenrunningfullyinmemory 10timesfasterthanmapreduceevenwhenrunningondisk 596 1 1 08022018 1334 whatisspark fileusershiddersdocumentseducationbig data distribu apachesparkisafastinmemory distributeddataprocessingengine supportsefficientexecutionofdifferent kindsofworkloadsbatchstreaming machinelearningandsql easeofprogramming ageneralprogrammingmodelbasedoncomposingarbitraryoperators allowscombiningdifferentprocessingmodelsseamlesslyinthesameapplication dataclassificationthroughsparkmachinelearninglibrary streamingdatathroughsourceviasparkstreaming queryingtheresultingdatainrealtimethroughsparksql 696 1 1 08022018 1334 averybriefhistoryofspark 2009 sparkwascreatedintheucberkeleyrdlab lateritbecomesamplab createdbymateizahariaduringhisphdstudiesunderion stoicahealsodesignedthecoreschedulingalgorithms usedinapachehadoop 2010 opensourcedunderbsdlicense mateizaharia 2013 sparkwasdonatedtoapachesoftware foundation alsofoundingofdatabricksbyoriginalcreatorsto commercializespark 2014 itbecomesatoplevelapacheproject 2016 spark20releasedegsql2003support 2020 spark30released ionstoica 796 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 896 whatisanrdd rddresilientdistributeddataset sparksabstractionforrepresentingaverylarge dataset rddcollectionofelements rddscancontainanytypesofobjectsas elementsincludinguserdefinedclasses underthehoodsparkwillautomatically distributethedatacontainedinrddsacross yourclusterandparallelizetheoperationsyou performonthem 996 thepartitioningprocessistypicallydone automaticallybyspark usercanandsometimesshouldinfluence partitioningprocess rddbasics rddsaredistributed eachrddisbrokenintomultiplepiecescalled partitionsandthesepartitionsaredivided acrossthenodesinthecluster partitionsareoperatedoninparallel 1096 rddbasics rddsaredistributed eachrddisbrokenintomultiplepiecescalled partitionsandthesepartitionsaredivided acrossthenodesinthecluster partitionsareoperatedoninparallel thepartitioningprocessistypicallydone automaticallybyspark usercanandsometimesshouldinfluence partitioningprocess 1196 rddbasics rddsareimmutable theycannotbechangedaftertheyarecreated immutabilityrulesoutasignificantsetof potentialproblemsduetoupdatesfrom multiplethreadsatonce 1296 rddbasics rdd 1 rrdddd 22 rdd 3 result input transf ttrraannssff action 524 data spark map map filter filter basicworkflowinspark generateinitialrddsfromexternaldata transformationscreatenewrddsfromexistingones actionstransformrddsintonormalprogramminglanguagevalueslistsnumbersfile 1396 rddbasics rddsareresilient rddsareadeterministicfunctionoftheirinputthisplusimmutabilityalsomeansthe rddspartscanberecreatedatanytime incaseanynodeintheclustergoesdownsparkcanrecoverthepartsoftherddsfrom theinputandpickupfromwhereitleftoff sparkdoestheheavyliftingforyoutomakesurethatrddsarefaulttolerant 1496 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 1596 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 1696 howtocreateanrdd12 takeanexistingcollectioninourprogramandpassittosparkcontexts parallelizemethod sc sparkcontextlocal mysparkapplicationname inputintegers listrange1 6 integerrdd scparallelizeinputintegers alltheelementsinthecollectionwillthenbecopiedtoformadistributed datasetthatcanbeoperatedoninparallel veryhandytocreateanrddwithlittleeffort notpracticalwhenworkingwithlargedatasets 1796 howtocreateanrdd22 loadrddsfromexternalstoragebycallingtextfilemethodonsparkcontext sc sparkcontextlocal textfile lines sctextfileinuppercasetext theexternalstorageisusuallyadistributedfilesystemsuchasamazons3orhdfs thereareotherdatasourceswhichcanbeintegratedwithsparkandusedtocreaterdds includingjdbccassandraandelasticsearchetc 1896 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 1996 transformations transformationsareoperationsonrddswhichwillreturnanewrdd thetwomostcommontransformationsarefilterandmap 2096 filtertransformation takesinafunctionandreturnsanrddformedbyselectingthoseelementswhichpass thefilterfunction canbeusedtoremovesomeinvalidrowstocleanuptheinputrddorjustgetasubset oftheinputrddbasedonthefilterfunction filteredlines linesfilterlambda line lenline0 2196 maptransformation takesinafunctionandpasseseachelementintheinputrddthroughthe functionwiththeresultofthefunctionbeingthenewvalueofeachelementin theresultingrdd itcanbeusedtomakehttprequeststoeachurlinourinputrddoritcanbe usedtocalculatethesquarerootofeachnumber urls sctextfileinurlstext ulrsmapmakehttprequest thereturntypeofthemapfunctionisnotnecessarythesameasitsinputtype lines sctextfileinuppercasetext lengths linesmaplambda line lenline 2296 flatmaptransformation flatmapisatransformationtocreateanrddfromanexistingrdd ittakeseachelementfromanexistingrddanditcanproduce01ormany outputsforeachelement 2396 flatmapvsmap12 def mapself f preservespartitioningfalse return new rdd applying function element rdd rdd scparallelizea b b c c c sortedrddmaplambda x xsplitcollect b b c c c def flatmapself f preservespartitioningfalse return new rdd first applying function elements rdd flattening results rdd scparallelizea b b c c c sortedrddflatmaplambda x xsplitcollect b b c c c 2496 flatmapvsmap22 flatmap vs map map 1 1 relationship map rdd rdd flatmap 1 many relationship flatmap rdd rdd 2596 setoperations setoperationswhichareperformedononerdd sample distinct 2696 sample def samplese1f withreplacement fraction seednone return sampled subset rdd param withreplacement elements sampled multiple times replaced sampled param fraction expected size sample fraction rdds size without replacement probability element chosen fraction must 0 1 replacement expected number times element chosen fraction must 0 param seed seed random number generator note guaranteed provide exactly fraction specified total count given classdataframe thesampleoperationwillcreatearandomsamplefromanrdd usefulfortestingpurpose 2796 shuffling physicalmovementofdatabetweenpartitionsiscalledshuffling itoccurs whendatafrommultiplepartitionsneedstobecombinedinordertobuild partitionsforanewrdd distinct def distinctself numpartitionsnone return new rdd containing distinct elements rdd sortedscparallelize1 1 2 3distinctco11ect 1 2 3 thedistincttransformationreturnsthedistinctrowsfromtheinputrdd thedistincttransformationisexpensivebecauseitrequiresshufflingallthe dataacrosspartitionstoensurethatwereceiveonlyonecopy 2896 distinct def distinctself numpartitionsnone return new rdd containing distinct elements rdd sortedscparallelize1 1 2 3distinctco11ect 1 2 3 thedistincttransformationreturnsthedistinctrowsfromtheinputrdd thedistincttransformationisexpensivebecauseitrequiresshufflingallthe dataacrosspartitionstoensurethatwereceiveonlyonecopy shuffling physicalmovementofdatabetweenpartitionsiscalledshuffling itoccurs whendatafrommultiplepartitionsneedstobecombinedinordertobuild partitionsforanewrdd 2996 setoperations setoperationswhichareperformedontworddsandproduceoneresultingrdd union intersection subtract cartesianproduct 3096 union def unionself return union rdd another one unionoperationgivesusbackanrddconsistingofthedata frombothinputrdds b ifthereareanyduplicatesintheinputrddstheresultingrdd ofsparksunionoperationwillcontainduplicatesaswell 3196 intersection def intersectionself return intersection rdd another one output contain duplicate elements even input rdds note method performs shuffle internally intersectionoperationreturnsthecommonelements whichappearinbothinputrdds intersectionoperationremovesallduplicates includingtheduplicatesfromsinglerddbefore b returningtheresults intersectionoperationisquiteexpensivesinceit requiresshufflingallthedataacrosspartitionsto identifycommonelements 3296 subtract def subtractself numpartitionsnone return value cself contained cother subtractoperationtakesinanotherrddasan argumentandreturnsusanrddthatonlycontains elementspresentinthefirstrddandnotinthe b secondrdd subtractoperationrequiresashufflingofallthedata whichcouldbequiteexpensiveforlargedatasets 3396 cartesianproduct def cartesianself return cartesian product rdd another one rdd pairs elements ca b ca cself cb cother 1234abc cartesiantransformationreturnsallpossiblepairs ofaandbwhereaisinthesourcerddandbisin theotherrdd 1a 1b 1c cartesiantransformationcanbeveryhandyifwe 2a 2b 2c wanttocomparethesimilaritybetweenall 3a 3b 3c possiblepairs 3496 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 3596 actions actionsaretheoperationswhichwillreturnafinalvaluetothedriverprogram orpersistdatatoanexternalstoragesystem actionswillforcetheevaluationofthetransformationsrequiredfortherdd theywerecalledon 3696 commonactionsinspark collect count countbyvalue take saveastextfile reduce 3796 collectaction collectoperationretrievestheentirerddandreturnsittothedriverprogram intheformofaregularcollectionorvalue ifyouhaveastringrddwhenyoucallcollectactiononityouwouldgetalist ofstrings thisisquiteusefulifyoursparkprogramhasfilteredrddsdowntoarelatively smallsizeandyouwanttodealwithitlocally theentiredatasetmustfitinmemoryonasinglemachineasitallneedstobe copiedtothedriverwhenthecollectactioniscalled socollectactionshould notbeusedonlargedatasets collectoperationiswidelyusedinunitteststocomparethevalueofourrdd withourexpectedresultaslongastheentirecontentsoftherddcanfitin memory 3896 countandcountbyvalueactions ifyoujustwanttocounthowmanyrowsinanrddthecountoperationisa quickwaytodothat itwouldreturnthecountoftheelements countbyvaluewilllookatuniquevaluesineachrowofyourrddandreturna mapofeachuniquevaluetoitscount countbyvalueisusefulwhenyourrddcontainsduplicaterowsandyouwantto counthowmanyofeachuniquerowvaluesyouhave 3996 takeaction words wordrddtake3 takeactiontakesnelementsfromanrdd takeoperationcanbeveryusefulifyouwouldliketotakeapeekattherddfor unittestsandquickdebugging takewillreturnnelementsfromtherddanditwilltrytoreducethenumberof partitionsitaccessessoitispossiblethatthetakeoperationcouldendup givingusbackabiasedcollectionanditdoesntnecessaryreturntheelements intheorderwemightexpect 4096 saveastextfileaction airportsnameandcityrddsaveastextfileoutairportstext saveastextfilecanbeusedtowritedataouttoadistributedstoragesystem suchashdfsoramazons3oreventhelocalfilesystem 4196 reduceaction def reduceself f reduces elements rdd using specified commutative associative binary operator currently reduces partitions locally product integerrddreducelambda x xy thereduceactiontakesafunctionthatoperatesontwoelementsofthetypein theinputrddandreturnsanewelementofthesametype itreducesthe elementsofthisrddusingthespecifiedbinaryfunction thisfunctionproducesthesameresultwhenrepetitivelyappliedonthesame setofrdddataandreducestoasinglevalue withthereduceactionwecanperformdifferenttypesof aggregations 4296 summary transformationsareoperationsonrddsthatreturnanewrddsuchasmap andfilter actionsareoperationsthatreturnaresulttothedriverprogramorwriteitto storageandkickoffacomputationsuchascountandcollect 4396 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 4496 accummulators accumulatorsarevariablesthatareusedforaggregatinginformationacross theexecutors forexamplewecancalculatehowmanyrecordsarecorruptedorcounteventsthat occurduringjobexecutionfordebuggingpurposes tasksonworkernodescannotaccesstheaccumulatorvalue accumulatorsarewriteonlyvariables thisallowsaccumulatorstobeimplementedefficientlywithouthavingto communicateeveryupdate accum scaccumulator0 accum accumulatorid0 value0 scparallelize1 2 3 4foreachlambda x accumaddx 4596 broadcastvariables broadcastvariablesallowtheprogrammertokeepareadonlyvariablecached oneachmachineratherthanshippingacopyofitwithtasks theycanbeusedforexampletogiveeverynodeacopyofalargeinput datasetinanefficientmanner allbroadcastvariableswillbekeptatalltheworkernodesforuseinoneor moresparkoperations normalvariablesareresentforeachoperation broadcastvar scbroadcast1 2 3 pysparkbroadcastbroadcast object 0x102789f10 broadcastvarvalue 1 2 3 4696 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 4796 pairrdd alotofdatasetsweseeinreallifeexamplesareusuallykeyvaluepairs examples adatasetwhichcontainspassportidsandthenamesofthepassportholders adatasetcontainscoursenamesandalistofstudentsthatenrolledinthecourses thetypicalpatternofthiskindofdatasetisthatiseachrowisonekeymapped toonevalueormultiplevalues sparkprovidesadatastructurecalledpairrddinsteadofregularrddswhich makesworkingwiththiskindofdatasimplerandmoreefficient apairrddsisaparticulartypeofrddthatcanstorekeyvaluepairs 4896 howtocreatepairrdds 1 returnpairrddsfromalistofkeyvaluedatastructurecalledtuple 2 turnaregularrddintoapairrdd 4996 getpairrddsfromtuples pythontuples my_tuple lily 23 name my_tuple0 my_tuple lily 23 age my_tuple1 my_tuple tuplelily 23 convertlistsoftuplestopairrddswithparallellize scparallelizepanda 0 pink 3 5096 transformationsonpairrdds pairrddsareallowedtouseallthetransformationsavailabletoregularrdds andthussupportthesamefunctionsasregularrdds sincepairrddscontaintuplesweneedtopassfunctionsthatoperateon tuplesratherthanonindividualelements 5196 filtertransformation thefiltertransformationthatcanbeappliedtoaregularrddcanalsobe appliedtoapairrdd thefiltertransformationtakesinafunctionandreturnsapairrddformedby selectingthoseelementswhichpassthefilterfunction 5296 mapandmapvaluestransformations themaptransformationalsoworksforpairrdds itcanbeusedtoconvertan rddtoanotherone butmostofthetimewhenworkingwithpairrddswedontwanttomodifythe keyswejustwanttoaccessthevaluepartofourpairrdd sincethisisatypicalpatternsparkprovidesthemapvaluesfunction mapvaluesfunctionwillbeappliedtoeachkeyvaluepairandwillconvertthe valuesbasedonmapvaluesfunctionbutitwillnotchangethekeys 5396 aggregation whenourdatasetisdescribedintheformatof keyvaluepairsitisquite commonthatwewouldliketoaggregatestatisticsacrossallelementswiththe samekey wehavelookedatthereduceactionsonregularrddsandthereisasimilar operationforpairrdditiscalledreducebykey reducebykeyrunsseveralparallelreduceoperationsoneforeachkeyinthe datasetwhereeachoperationcombinesvaluesthathavethesamekey consideringinputdatasetscouldhaveahugenumberofkeysreducebykey operationisnotimplementedasanactionthatreturnsavaluetothedriver program insteaditreturnsanewrddconsistingofeachkeyandthereduced valueforthatkey 5496 reducebykeytransformation 5596 groupbykeytransformation acommonusecaseforpairrddisgroupingourdatabykey forexample viewingallofanaccountstransactionstogether ifourdataisalreadykeyedinthewaywewantgroupbykeywillgroupourdata usingthekeyinourpairrdd letssaytheinputpairrddhaskeysof typekandvaluesof typevifwecall groupbykeyontherddwegetbackapairrddcontainingpairswithfieldsof typekandtypelterablev def groupbykeyself numpartitionsnone partitionfuncportable_hash group values key rdd single sequence hashpartitions resulting rdd numpartitions partitions 5696 sortbykeyandsortbytransformation def sortbykeyself ascendingtrue numpartitionsnone keyfunclambda x x wecansortapairrddaslongasthereisanorderingdefinedonthekey oncewehavesortedourpairrddanysubsequentcallonthesortedpairrdd tocollectorsavewillreturnusordereddata andifwedonotwanttosortbythekeybutbythevaluewecanusesortby def sortbyself keyfunc ascendingtrue numpartitionsnone thesortbytransformationmapsthepairrddtokeyfuncxxtuplesusingthe functionitreceivesasparameterappliessortbykeyandfinallyreturnthe values 5796 rredeudceuthceea mtohuen taomfshouuffnletf oorf gsrhouupbfyfkleey groupbykey partitionedwordpairrdd wordpairrddpartitionby3 lambda x x0 partitionedwordpairrddreducebykeylambda xy xycollect partitionby reducebykey partitionby groupbykey a1 a1 a3 b2 a2 b1 b1 b3 c3 b2 a2 c3 c6 c3 c3 5896 operationswhichcanbenefitfrompartitioning joinjoinsonthekeyofthekeyvaluepairs leftouterjoin rightouterjoin groupbykey reducebykey combinebykey lookup 5996 shuffledhashjoin theshuffledhashjoinisthedefaultimplementationofjoininspark 1 bothpairrddsaresimultaneouslypartitioned viaashufflebasedonthesamedefault partitioner 2 sokeyvaluepairswithsimilarkeysendupin thesamepartition 3 onthatpartitionweperformlocallyaclassical joinalgorithm ifthetworddswerealreadypartitionedusingthesamepartitioner theshufflecanbeavoided ifyoucallpartitionbybeforethejointhepartitionsmightalreadybe colocatedandnonetworktrafficisneeded 6096 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 6196 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 6296 transformationsvsactions transformationsandactionsaredifferentbecauseofthewayhowspark computesrdds eventhoughnewrddscanbedefinedanytimetheyarecomputedbysparkin alazyfashionwhichmeanstheyarenotevaluateduntiltheyareneededfor theresultofanaction 6396 lazyevaluation nothing would happen spark sees textfile statement lines sctextfileinuppercasetext nothing would happen spark sees filter transformation lineswithfriday linesfilterlambda line linestartswithfriday spark starts loading uppercasetext file first action called lineswithfriday rdd lineswithfridayfirst spark scans file first line starting friday detected doesnt even need go entire file 6496 lazyevaluation transformationsonrddsarelazilyevaluatedmeaningthatsparkwillnot begintoexecuteuntilitseesanaction ratherthanthinkingofanrddascontainingspecificdataitmightbebetterto thinkofeachrddasconsistingof instructionsonhowtocomputethedata thatwebuildupthroughtransformations sparkuseslazyevaluationtoreducethenumberofpassesithastotakeover ourdatabygroupingoperationstogether 6596 therddlineagegraph r00 scparallelizerange10 r01 scparallelizerange0 100 10 r10 r00cartesianr00 r11 r00maplambda x x x r12 r00zipr01 r13 r01keybylambda x x20 r20 scunionr10 r11 r12 r13 r00 r01 r10 r11 r12 r13 r20 6696 widetransformations transformationsthatrequireashufflephase intersection distinct reducebykey groupbykey join cartesian repartition coalesce 6796 narrowtransformations transformationsthatdonotneedashufflephase aregroupedbysparkinasinglestagewhichiscalledpipelining map flatmap mappartition filter sample union 6896 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 6996 persistence sometimeswewouldliketocallactionsonthesamerddmultipletimes ifwedothisnaivelyrddsandallofitsdependenciesarerecomputedeach timeanactioniscalledontherdd thiscanbeveryexpensiveespeciallyforsomeiterativealgorithmswhich wouldcallactionsonthesamedatasetmanytimes ifyouwanttoreuseanrddinmultipleactionsyoucanalsoasksparkto persistbycallingthepersistmethodontherdd whenyoupersistanrddthefirsttimeitiscomputedinanactionitwillbe keptinmemoryacrossthenodes rdd scparallelizeb c rddpersistis_cached true 7096 differentstoragelevel storagelevel meaning storerddasdeserializedjavaobjectsinthejvmiftherdddoesnotfitin memory_only memorysomepartitionswillnotbecachedandwillberecomputedonthefly eachtimetheyreneededthisisthedefaultlevel storerddasdeserializedjavaobjectsinthejvmiftherdddoesnotfitin memory_and_disk memorystorethepartitionsthatdontfitondiskandreadthemfromthere whentheyreneeded storerddasserializedjavaobjectsonebytearrayperpartitionthisis memory_only_ser generallymorespaceefficientthandeserializedobjectsespeciallywhenusing javaandscala afastserializerbutmorecpuintensivetoread memory_and_disk_ser similartomemory_only_serbutspillpartitionsthatdontfitinmemoryto javaandscala diskinsteadofrecomputingthemontheflyeachtimetheyreneeded dlsk_only storetherddpartitionsonlyondisk 7196 whichstoragelevelweshouldchoose sparksstoragelevelsaremeanttoprovidedifferenttradeoffsbetween memoryusageandcpuefficiency iftherddscanfitcomfortablywiththedefaultstoragelevelmemory_onlyis theidealoption thisisthemostcpuefficientoptionallowingoperationson therddstorunasfastaspossible ifnottryusingmemory_only_sertomaketheobjectsmuchmore spaceefficientbutstillreasonablyfasttoaccess dontsavetodiskunlessthefunctionsthatcomputedyourdatasetsare expensiveortheyfilterasignificantamountofthedata 7296 whatwouldhappenifyouattempttocachetoomuchdatatofitinmemory sparkwillevictoldpartitionsautomaticallyusingaleastrecentlyusedcachepolicy forthememory_onlystoragelevelsparkwillrecomputethesepartitionsthenext timetheyareneeded forthememory_and_diskstoragelevelsparkwillwritethesepartitionstodisk ineithercaseyoursparkjobwontbreakevenifyouasksparktocachetoomuchdata cachingunnecessarydatacancausesparktoevictusefuldataandleadtolonger recomputationtime 7396 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 7496 sparkmasterslavearchitecture spark masterslave architecture worker node executor cache task task driver program sparkcontext worker node executor cache task task 7596 driver program executors driver program 2 new york new 1 york 1 new york us u ans 1 1 nn e w 2 4 us york 3 history history 1 us 1 history 1 new york new 2 new jersey new york york 1 h oi fs ory 21 new jersey 2 metro jersey 1 jersey 1 new york metro 1 metro 1 metro 2 new york new 1 york 1 7696 runningsparkinlocalmode spark masterslave architecture allcomponentsruninthesameprocessonthelocalmachine worker node executor cache driver program task task sparkcontext worker node executor cache task task 7796 runningsparkinclustermode componentsmayrunondifferentmachines spark masterslave architecture slavemachine worker node mastermachine executor cache driver program task task sparkcontext worker node executor cache task task slavemachine 7896 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 7996 runningsparkinclustermode spark masterslave architecture worker node executor cache task task driver program sparkcontext worker node executor cache task task 8096 runningsparkwithaclustermanager running spark cluster mode worker node executor cache task task driver program cluster manager sparkcontext worker node executor cache task task 8196 clustermanager theclustermanagerisapluggablecomponentinspark sparkispackagedwithabuiltinclustermanagercalledthestandalonecluster manager youcanuseothertypesofclustermanagerssuchas hadoopyarnaresourcemanagementandschedulingtoolforahadoopmapreduce cluster apachemesoscentralizedfaulttolerantclustermanagerandglobalresourcemanager foryourentiredatacenter theclustermanagerabstractsawaytheunderlyingclusterenvironmentsothat youcanusethesameunifiedhighlevelsparkapitowritesparkprogramwhich canrunondifferentclusters youcanusesparksubmittosubmitanapplicationtothecluster 8296 runningsparkapplicationsonacluster theusersubmitsanapplicationusingthesparksubmitscript availableinsparksbindirectory sparksubmitlaunchesthedriverprogramandinvokesthemainmethod specifiedbytheuser thedriverprogramcontactstheclustermanagertoaskforresourcestostart executors theclustermanagerlaunchesexecutorsonbehalfofthedriverprogram thedriverprocessrunsthroughtheuserapplication basedontherddor dataframeoperationsintheprogramthedriversendsworktoexecutorsinthe formoftasks tasksarerunonexecutorprocessestocomputeandsaveresults ifthedriversmainmethodexitsoritcallssparkcontextstopitwillterminate theexecutors 8396 sparksubmitoptions typicalusage sparksubmit executormemory 20g totalexecutorcores 100 pathtoexamplespy 8496 benefitsofsparksubmit wecanrunsparkapplicationsfromacommandlineorexecutethescript periodicallyusingacronjoborotherschedulingservice sparksubmitscriptisanavailablescriptonanyoperatingsystemthat supportsjava youcandevelopyoursparkapplicationonawindowsmachineanduploadthepython scripttoalinuxclusterandrunthesparksubmitscriptonthelinuxcluster 8596 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 8696 theapachesparkecosystem spark mllib graphx spark sparkr streaming machine graph sql ronspark streaming learning computation apache spark core api r sql python scala java 8796 sparksql sparkpackagedesignedforworkingwithrelationaldatawhichisbuiltontopof sparkcore providesansqllikeinterfaceforworkingwithstructureddata offerstwospecialtypesofrdds dataframestablelikestructureweaklytypedscalajavapythonr datasetsstronglytypedcollectionsonlyavailableinscalaandjava fairlysophisticatedoptimizerforsqlqueries sparksqlcatalyst 8896 sparkstreaming runningontopofsparksparkstreamingprovidesanapiformanipulating datastreamsthatcloselymatchthesparkcoresrddapi sparkstreamingdiscretizesthestreamingdataintotinysubsecond microbatches enablespowerfulinteractiveandanalyticalapplicationsacrossbothstreaming andhistoricaldatawhileinheritingsparkseaseofuseandfaulttolerance characteristics 8996 sparkmllib builtontopofsparkmllibisascalablemachinelearninglibrarythatcontains highqualityalgorithms usableinjavascalaandpythonaspartofsparkapplications consistsof commonlearningalgorithmsandutilitiesincludingclassification regressionclusteringcollaborativefilteringdimensionalityreductionetcetera 9096 sparkgraphx agraphcomputationenginebuiltontopofsparkthatenablesusersto interactivelycreatetransformandreasonaboutgraphstructureddataatscale extendsthesparkrddbyintroducinganewgraphabstraction adirected multigraphwithpropertiesattachedtoeachvertexandedge 9196 outline introduction resilientdistributeddatasetsrdds operationsonrdds creation transformations actions specialglobalvariables pairrdds sparkprogramexecution lazyevaluation cachingandpersistence masterslavearchitecture runningsparkinacluster theapachesparkecosystem furtherreading 9296 furtherreading excellentrecentbooks learningspark2ndedition2020 writtenbyjulessdamjibrooke wenigtathagatadasdennylee sparkinaction2ndedition2020 writtenbyjeangeorgesperrin highperformancespark2017written byholdenkarauandrachelwarren advancedanalyticswithspark2nd edition2017writtenbysandyryza urilasersonseanowenandjosh wills 9396 furtherreading theinternalsofapachespark2021writtenbyjaceklaskowski 9496 questions 9596 acknowledgements basedoncontentcreatedbyjanhiddersandstijnvansummeren 9696

infoh515 big data distributed management lecture 3 distributed stream processing dimitrissacharidis remember vs challenges big data sourcehttpwwwibmbigdatahubcomsitesdefaultfilesinfographic_file 4vsofbigdatajpg dataisnotjustbigitisalsofast facebookusersupload900millionphotosday652000photosminute twitterprocesses500milliontweetsday350000tweetsminute thenysegenerates1tbofdatapertradingsession24gbmin andrequiresnearrealtimeanalysis inelectronictradinga1millisecondadvantagecanbeworth100milliontoa majorbrokeragefirm inretailoutofstockdetectionispreferableinsecondsorminutesratherthan daysorweeks need deal velocity definition velocityreferstothespeedatwhichdataisbeingproducedcapturedorre freshed andrequiresnearrealtimeanalysis inelectronictradinga1millisecondadvantagecanbeworth100milliontoa majorbrokeragefirm inretailoutofstockdetectionispreferableinsecondsorminutesratherthan daysorweeks need deal velocity definition velocityreferstothespeedatwhichdataisbeingproducedcapturedorre freshed dataisnotjustbigitisalsofast facebookusersupload900millionphotosday652000photosminute twitterprocesses500milliontweetsday350000tweetsminute thenysegenerates1tbofdatapertradingsession24gbmin need deal velocity definition velocityreferstothespeedatwhichdataisbeingproducedcapturedorre freshed dataisnotjustbigitisalsofast facebookusersupload900millionphotosday652000photosminute twitterprocesses500milliontweetsday350000tweetsminute thenysegenerates1tbofdatapertradingsession24gbmin andrequiresnearrealtimeanalysis inelectronictradinga1millisecondadvantagecanbeworth100milliontoa majorbrokeragefirm inretailoutofstockdetectionispreferableinsecondsorminutesratherthan daysorweeks internet things therearemoredevices connectedtotheinternetthan peopleonearth sensorsarewillbe everywhere howcansystemsingestthis continuousdatastream sourcehttpsblogsciscocomdiversitytheinternetofthingsinfographic internet things therearemoredevices connectedtotheinternetthan peopleonearth sensorsarewillbe everywhere howcansystemsingestthis continuousdatastream sourcehttpsblogsciscocomdiversitytheinternetofthingsinfographic bigdatasoftwareframeworksthatallowanalysisofhighvelocitydataarecalleddata streamingframeworkswewilldiscusstwowithfundamentallydifferentarchitectures twitterstormheron sparkstreaming tupleatatime minibatching lecture outline evenifanapplicationdoesnotrequirerealtime analysisuseofdatastreamingiswidespread becauseoftheλarchitecture twitterstormheron sparkstreaming tupleatatime minibatching lecture outline evenifanapplicationdoesnotrequirerealtime analysisuseofdatastreamingiswidespread becauseoftheλarchitecture bigdatasoftwareframeworksthatallowanalysisofhighvelocitydataarecalleddata streamingframeworkswewilldiscusstwowithfundamentallydifferentarchitectures sparkstreaming minibatching lecture outline evenifanapplicationdoesnotrequirerealtime analysisuseofdatastreamingiswidespread becauseoftheλarchitecture bigdatasoftwareframeworksthatallowanalysisofhighvelocitydataarecalleddata streamingframeworkswewilldiscusstwowithfundamentallydifferentarchitectures twitterstormheron tupleatatime lecture outline evenifanapplicationdoesnotrequirerealtime analysisuseofdatastreamingiswidespread becauseoftheλarchitecture bigdatasoftwareframeworksthatallowanalysisofhighvelocitydataarecalleddata streamingframeworkswewilldiscusstwowithfundamentallydifferentarchitectures twitterstormheron sparkstreaming tupleatatime minibatching lecture outline λarchitecture hence good way architect big data system data system system manages storage querying data lifetime measured years encompassing every version application ever exist every hardware failure every human mistake every made nathan marz data system system manages storage querying data lifetime measured years encompassing every version application ever exist every hardware failure every human mistake every made nathan marz hence good way architect big data system problem mutable data 12 traditionallywemutatethedatabasetostoreonlymostrecentdata person location person location sally philadelphia sally newyork bob chicago bob chicago sallymovestonewyork problem mutable data 22 mutabledataiscorruptable bugswillbedeployedtoproductionsoftware overthelifetimeofadatasystem humanscanerrthereforeoperationalmistakes willbemadeatsomepoint youmustdesigntosafeguardagainstdata corruption problem mutable data 22 mutabledataiscorruptable bugswillbedeployedtoproductionsoftware overthelifetimeofadatasystem humanscanerrthereforeoperationalmistakes willbemadeatsomepoint youmustdesigntosafeguardagainstdata corruption examplesoferrors deployasoftwarebugthatincrementscountersbytwoinsteadofone accidentallydeletedatafromadatabase incorrectlymodifyadataitemsallydidnotmovetonewyorkbobdid problem mutable data 22 mutabledataiscorruptable bugswillbedeployedtoproductionsoftware overthelifetimeofadatasystem humanscanerrthereforeoperationalmistakes willbemadeatsomepoint youmustdesigntosafeguardagainstdata corruption keyobservation aslongasanerrordoesnotloseorcorruptgooddatawe canfixwhatwentwrong immutability person location mutabledataiscorruptable therefore sally philadelphia keepallmasterdataimmutable bob chicago animmutabledatasystemcapturesa historicalrecordofevents person location eacheventhappensataparticulartime sally newyork andremainstrueforever bob chicago youcanonlyappendneweventsnever sallymovestonewyork deleteormodifyexistingeventrecords filteronthetimestamptoseewhatistrue ataparticularmomentintime immutability person location time mutabledataiscorruptable therefore sally philadelphia 1318358 keepallmasterdataimmutable bob chicago 1237921 animmutabledatasystemcapturesa historicalrecordofevents person location time eacheventhappensataparticulartime sally philadelphia 1318358 andremainstrueforever bob chicago 1237921 sally newyork 1338812 youcanonlyappendneweventsnever deleteormodifyexistingeventrecords sallymovestonewyork filteronthetimestamptoseewhatistrue ataparticularmomentintime adatasystemanswersissupposedtoanswerquestionsbasedondata thatwasacquiredinthepast sowhymodifyacquireddata bigdatatechnologiesallowtostoreallrawdatasothatitcanbeused latertogainnewinsightsandcreatenewdatadrivenproductswhichwe haventyetthoughtof immutability makes sense bigdatatechnologiesallowtostoreallrawdatasothatitcanbeused latertogainnewinsightsandcreatenewdatadrivenproductswhichwe haventyetthoughtof immutability makes sense adatasystemanswersissupposedtoanswerquestionsbasedondata thatwasacquiredinthepast sowhymodifyacquireddata immutability makes sense adatasystemanswersissupposedtoanswerquestionsbasedondata thatwasacquiredinthepast sowhymodifyacquireddata bigdatatechnologiesallowtostoreallrawdatasothatitcanbeused latertogainnewinsightsandcreatenewdatadrivenproductswhichwe haventyetthoughtof desired properties big data system robustandfaulttoleranceunderbothmachineandhumanfailures scalable maintainperformanceunderincreasingloadbyaddingresources answerpredefinedquerieswithlowlatency supportadhocquerying lowlatencyupdates extensiblegeneral aqueryhencetransformsourimmutablerawdataintousefulinformation problem alldatapetabytescale howdowegetsmalllatency query functionall data locationinformationdatabase webanalyticsdatabase howmanypeopleliveina howmanypageviewson particularlocation september2nd wheredoessallylive howmanyuniquevisitorsover time whatisthemostpopularlocation insummer query functionall data locationinformationdatabase webanalyticsdatabase howmanypeopleliveina howmanypageviewson particularlocation september2nd wheredoessallylive howmanyuniquevisitorsover time whatisthemostpopularlocation insummer aqueryhencetransformsourimmutablerawdataintousefulinformation problem alldatapetabytescale howdowegetsmalllatency precomputed batch views url hour unique foocomblog 1 203 foocomblog 2 402 foocomblog 3 130 foocomblog 4 239 foocomblog 5 391 batchhovwietwocofmunpcuttieonviaelwlsdataqueryfastfunctionbatchview howtocomputequeriesfromviews usebatchprocessingframeworukseabatchviewdatabasethat ishighlyavailable scalable supportsbatchwritesfrombatchframework solution precomputeviewsfromswuphpiocrhtstfhaestarannsdwomersintdoexpedreredaedfisnleodwqlauteenrciyescan querying bereadcomputedwithlowlatency note randomwritesnotnecessary towards λarchitecture query immutable application data user url time howmanyuniquepeople 19216821 foocomblog 1318358 19216848 foocomblog 1318458 visitedthisdomaineach 19216848 foocomabout 1318898 19225069 oofcomitem 1318898 hourforthepastthree 19225069 foocomblog 1818123 days problem alldatapetabytescale howdowegetsmalllatency computingqueryanswersontheflyhashighlatencyhours batchhovwietwocofmunpcuttieonviaelwlsdataqueryfastfunctionbatchview howtocomputequeriesfromviews usebatchprocessingframeworukseabatchviewdatabasethat ishighlyavailable scalable supportsbatchwritesfrombatchframework supportsfastrandomindexedreadslowlatency querying note randomwritesnotnecessary towards λarchitecture precomputed query immutable batch application data views user url time url hour unique howmanyuniquepeople 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 visitedthisdomaineach 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 hourforthepastthree 19225069 foocomblog 1818123 foocomblog 5 391 days problem alldatapetabytescale howdowegetsmalllatency computingqueryanswersontheflyhashighlatencyhours solution precomputeviewsfromwhichtheanswerstopredefinedqueriescan bereadcomputedwithlowlatency solution precomputeviewsfromwhichtheanswerstopredefinedqueriescan bereadcomputedwithlowlatency batchviewfunctionalldataqueryfastfunctionbatchview usebatchprocessingframeworukseabatchviewdatabasethat problem alldatapetabytescaleis hhigohwlydavoaiwlaeblegetsmalllatency scalable computingqueryanswersontheflyhashighlatencyhours supportsbatchwritesfrombatchframework supportsfastrandomindexedreadslowlatency querying note randomwritesnotnecessary towards λarchitecture precomputed query immutable batch application data views user url time url hour unique howmanyuniquepeople 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 visitedthisdomaineach 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 hourforthepastthree 19225069 foocomblog 1818123 foocomblog 5 391 days howtocomputeviews howtocomputequeriesfromviews solution precomputeviewsfromwhichtheanswerstopredefinedqueriescan bereadcomputedwithlowlatency howtocomputeviews queryfastfunctionbatchview howtocomputequeriesfromviews useabatchviewdatabasethat problem alldatapetabytescaleis hhigohwlydavoaiwlaeblegetsmalllatency scalable computingqueryanswersontheflyhashighlatencyhours supportsbatchwritesfrombatchframework supportsfastrandomindexedreadslowlatency querying note randomwritesnotnecessary towards λarchitecture precomputed query immutable batch application data views user url time url hour unique howmanyuniquepeople 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 visitedthisdomaineach 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 hourforthepastthree 19225069 foocomblog 1818123 foocomblog 5 391 days batchviewfunctionalldata usebatchprocessingframework solution precomputeviewsfromwhichtheanswerstopredefinedqueriescan bereadcomputedwithlowlatency howtocomputeviews queryfastfunctionbatchview howtocomputequeriesfromviews useabatchviewdatabasethat problem alldatapetabytescaleis hhigohwlydavoaiwlaeblegetsmalllatency scalable computingqueryanswersontheflyhashighlatencyhours supportsbatchwritesfrombatchframework supportsfastrandomindexedreadslowlatency querying note randomwritesnotnecessary towards λarchitecture precomputed query immutable batch application data views user url time url hour unique howmanyuniquepeople 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 visitedthisdomaineach 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 hourforthepastthree 19225069 foocomblog 1818123 foocomblog 5 391 days batchviewfunctionalldata usebatchprocessingframework solution precomputeviewsfromwhichtheanswerstopredefinedqueriescan bereadcomputedwithlowlatency batchhovwietwocofmunpcuttieonviaelwlsdata howtocomputequeriesfromviews usebatchprocessingframework problem alldatapetabytescale howdowegetsmalllatency computingqueryanswersontheflyhashighlatencyhours towards λarchitecture precomputed query immutable batch application data views user url time url hour unique howmanyuniquepeople 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 visitedthisdomaineach 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 hourforthepastthree 19225069 foocomblog 1818123 foocomblog 5 391 days queryfastfunctionbatchview useabatchviewdatabasethat ishighlyavailable scalable supportsbatchwritesfrombatchframework supportsfastrandomindexedreadslowlatency querying note randomwritesnotnecessary answertryandincrementalizethebatchcomputationasmuchaspossible precomputed batch views user url time url hour unique 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 19225069 foocomblog 1818123 foocomblog 5 391 15221412 goocomidx 2010911 foocomblog 6 203 18222812 foocomblog 2012918 foocomblog 7 402 19216848 foocomabout 2318898 foocomblog 8 130 19225069 oofcomitem 2318898 foocomblog 9 239 yet notquitetherawdatachangesallthetimehowdowekeepthebatchviewsuptodate precomputed immutable batch data views user url time url hour unique 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 19225069 foocomblog 1818123 foocomblog 5 391 answertryandincrementalizethebatchcomputationasmuchaspossible precomputed batch views user url time url hour unique 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 19225069 foocomblog 1818123 foocomblog 5 391 foocomblog 6 203 foocomblog 7 402 foocomblog 8 130 foocomblog 9 239 yet notquitetherawdatachangesallthetimehowdowekeepthebatchviewsuptodate precomputed immutable batch data views user url time url hour unique 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 19225069 foocomblog 1818123 foocomblog 5 391 15221412 goocomidx 2010911 18222812 foocomblog 2012918 19216848 foocomabout 2318898 19225069 oofcomitem 2318898 precomputed batch views user url time url hour unique 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 19225069 foocomblog 1818123 foocomblog 5 391 yet notquitetherawdatachangesallthetimehowdowekeepthebatchviewsuptodate answertryandincrementalizethebatchcomputationasmuchaspossible precomputed immutable batch data views user url time url hour unique 19216821 foocomblog 1318358 foocomblog 1 203 19216848 foocomblog 1318458 foocomblog 2 402 19216848 foocomabout 1318898 foocomblog 3 130 19225069 oofcomitem 1318898 foocomblog 4 239 19225069 foocomblog 1818123 foocomblog 5 391 15221412 goocomidx 2010911 foocomblog 6 203 18222812 foocomblog 2012918 foocomblog 7 402 19216848 foocomabout 2318898 foocomblog 8 130 19225069 oofcomitem 2318898 foocomblog 9 239 solutiontreatfreshdataseparatelyrealtimestreamprocessing yet stillnotquiteevenincrementalbatchcomputationcanberelativelyslow solutiontreatfreshdataseparatelyrealtimestreamprocessing yet stillnotquiteevenincrementalbatchcomputationcanberelativelyslow yet stillnotquiteevenincrementalbatchcomputationcanberelativelyslow solutiontreatfreshdataseparatelyrealtimestreamprocessing realtimeviewfunctionrealtimeviewnewdata exactifpossibleotherwiseapproximate seenextlecture approximaterealtimeviewsbecomeexactbatchviewovertime realtimeviewdatabasemustsupportrandomwrites morecomplex adding realtime stream processing precomputed immutable batch data views incoming query data application streamprocessing precomputed realtime views adding realtime stream processing precomputed immutable batch data views incoming query data application streamprocessing precomputed r ealtimeviewfunctionrealtimeviewnewdata realtime exactifpossibleotherwiseapproximate views seenextlecture approximaterealtimeviewsbecomeexactbatchviewovertime realtimeviewdatabasemustsupportrandomwrites morecomplex realtimeviewfunctionrealtimeviewnewdata exactifpossibleotherwiseapproximate seenextlecture approximaterealtimeviewsbecomeexactbatchviewovertime realtimeviewdatabasemustsupportrandomwrites morecomplex adding realtime stream processing precomputed immutable batch data views incoming query data application streamprocessing precomputed realtime views adding realtime stream processing summary λarchitecture theλarchitectureisabigdata architecturethatdistinguishesbetween threelayers figuresourcebigdatabook summary λarchitecture theλarchitectureisabigdata architecturethatdistinguishesbetween threelayers thebatchlayer keepsanimmutablehistoricallogof alldataalsoknownasthedatalake computesbatchviewsfromthis masterdata allowsadhocbuthighlatency querying figuresourcebigdatabook summary λarchitecture theλarchitectureisabigdata architecturethatdistinguishesbetween threelayers theservinglayer storesthebatchviews usestheseviewstorespondto predefinedqueries figuresourcebigdatabook summary λarchitecture theλarchitectureisabigdata architecturethatdistinguishesbetween threelayers thespeedlayer computesrealtimeviewsonstreaming newdata usestheseviewstoaugmentthe batchviewsforrespondingto predefinedqueries figuresourcebigdatabook summary λarchitecture theλarchitectureisabigdata architecturethatdistinguishesbetween threelayers theλarchitectureisameta architecture concreteframeworksfor eachlayercanbechosenonacaseby casebasis figuresourcebigdatabook oftenforgotten layers figuresourcehttpswwwslidesharenetgschmutz bigdataandfastdatalambdaarchitectureinaction oftenforgotten layers figuresourcehttpswwwslidesharenetgschmutz bigdataandfastdatalambdaarchitectureinaction distribution layer message queues clientknowswhenupdateisdone spikescanoverloadthedatabase two ways processing updates 12 client client database client synchronous directconnectionclientdatabase clientwaitsuntilupdatecomplete two ways processing updates 12 client client database client synchronous directconnectionclientdatabase clientwaitsuntilupdatecomplete clientknowswhenupdateisdone spikescanoverloadthedatabase preferable speed layer loadspikesareeasilyhandled specialmechanismrequiredtoacknowledgeupdateisprocessed two ways processing updates 22 client update stream client database queue processor client asynchronous clientsubmitsupdatetoaqueueandcontinuesworkwithoutwaiting updatesareprocessedbystreamprocessorwhenpossible preferable speed layer two ways processing updates 22 client update stream client database queue processor client asynchronous clientsubmitsupdatetoaqueueandcontinuesworkwithoutwaiting updatesareprocessedbystreamprocessorwhenpossible loadspikesareeasilyhandled specialmechanismrequiredtoacknowledgeupdateisprocessed two ways processing updates 22 client update stream client database queue processor client preferable speed layer asynchronous clientsubmitsupdatetoaqueueandcontinuesworkwithoutwaiting updatesareprocessedbystreamprocessorwhenpossible loadspikesareeasilyhandled specialmechanismrequiredtoacknowledgeupdateisprocessed two types message queues consumer producer queue consumer producer consumer singleconsumption alsoknownaspointtopointqueuemessagebusmessagequeue producerssenderspushmessagestoqueue consumersreceiverspopmessagesfromqueue parallellismthroughroundrobindistributionoverconsumers everymessageisconsumedbyonlyoneconsumerbydefaultamessageisdeleted fromthequeuewhenconsumerfetchesmessagefaulttoleranceproblem two types message queues broker topic1 consumer producer consumer topic2 producer consumer publishsubscribemessaging thequeueisorganizedintotopics abrokerisresponsibleforqueuemaintenanceandmessagedelivery producerspublisherssenderspushmessagestotopicsonqueue consumerssubscribersreceiversreceivemessagesfromthetopicsthattheyaresubscribedtothe brokerdecideswhenoldmessagescanbedeleted everymessageisprocessedbyallconsumerssubscribedtothemessagetopic scalabilityparallellism two types message queues broker topic1 consumer producer consumer topic2 producer consumer publishsubscribemessaging thequeueisorganizedintotopics abrokerisresponsibleforqueuemaintenanceandmessagedelivery producerspublisherssenderspushmessagestotopicsonqueue consumerssubscribersreceiversreceivemessagesfromthetopicsthattheyaresubscribedtothe brokerdecideswhenoldmessagescanbedeleted everymessageisprocessedbyallconsumerssubscribedtothemessagetopic scalabilityparallellism apache kafka apachekafkaisadistributedmessaging streamingplatformbasedonpartitioned publishsubscribe topicsarepartitioned partitionsaredistributedforscalability andreplicatedforfaulttolerance apache kafka apachekafkaisadistributedmessaging streamingplatformbasedonpartitioned publishsubscribe topicsarepartitioned partitionsaredistributedforscalability andreplicatedforfaulttolerance apache kafka apachekafkaisadistributedmessaging streamingplatformbasedonpartitioned publishsubscribe topicsarepartitioned partitionsaredistributedforscalability andreplicatedforfaulttolerance distributedproduction producerscanchoosetopushamessage ontopicxtoaparticularpartitionofx loadbalancesproduction apache kafka apachekafkaisadistributedmessaging streamingplatformbasedonpartitioned publishsubscribe topicsarepartitioned partitionsaredistributedforscalability andreplicatedforfaulttolerance hybridconsumptionmodel consumersarecollectedintoconsumer groups everymessagetotopicxisprocessedby eachconsumergroupsubscribedtox publishsubscribe howevereachmessageishandledby onlyoneconsumerinsideaconsumer grouppointtopointwhichallows loadbalancingconsumption apache kafka apachekafkaisadistributedmessaging streamingplatformbasedonpartitioned publishsubscribe topicsarepartitioned partitionsaredistributedforscalability andreplicatedforfaulttolerance somespecifics kafkaconsumptionispullbased consumerscanchoosetoretrieve messagesinbatch allmessagesarestoredondiskand retainedforaconfigurableperiod consumerscanhencereplaymessagesif needbe soyoucanalsothinkofitasadistributed filesystemfortemporaryfiles speed layer tupleatatime processing apache stormtwitter heron apachestorm distributedandfaulttolerant realtimecomputation developedatbacktypetwitteropen sourcedin2011 hasbeensupersededbyheronat twittersince2014opensourcedby apache heronhasthesameprogramming modelbutdifferentimplementation storm concepts tuple tuple tuple tuple tuple tuple stream tuple stream coreunitofdata unboundedsequenceoftuples immutablesetofkeyvaluepairs storm concepts spoutdatasource emitsstreams example akafkaspoutonaparticulartopic storm concepts boltcorefunctionofstreamingcomputation receivetuplesandprocessthemeg writetoadatastore lookupatupleinadatastore performarbitrarycomputation usuallybutnotnecessarilyemitadditionalstreams f x fx h hfxgx g x gx x storm concepts topologydagofspoutsboltsandstreams dataflowrepresentationofcomputation tuplesarepushedthroughthedagstreamingcomputationstartingfrom spouts storm concepts f x fx h hfxgx g x gx x topologydagofspoutsboltsandstreams dataflowrepresentationofcomputation tuplesarepushedthroughthedagstreamingcomputationstartingfrom spouts thestormschedulerisresponsiblefor schedulingthesetasksacrossthe physicalmachinesinthecluster storm concepts tasks paralleldistributedexecutionofspouts andbolts spoutsandboltsexecuteasmanytasks acrossthecluster theprogrammerdefinesthenumberof tasksforeachspoutboltwhendefining thetopologycanbereconfiguredat runtime storm concepts tasks paralleldistributedexecutionofspouts andbolts spoutsandboltsexecuteasmanytasks acrossthecluster theprogrammerdefinesthenumberof tasksforeachspoutboltwhendefining thetopologycanbereconfiguredat runtime thestormschedulerisresponsiblefor schedulingthesetasksacrossthe physicalmachinesinthecluster shufflegroupingpicktaskin roundrobinfashion fieldgroupingconsistenthashingona subsetoftuplefields allgroupingsendtoalltasks globalgroupingpicktaskwithlowestid answerdependsonthegroupingspecified bythetopologyprogrammer streamgrouping storm concepts questionwhenatupleisemittedwhat taskdoesitgoto shufflegroupingpicktaskin roundrobinfashion fieldgroupingconsistenthashingona subsetoftuplefields allgroupingsendtoalltasks globalgroupingpicktaskwithlowestid streamgrouping storm concepts questionwhenatupleisemittedwhat taskdoesitgoto answerdependsonthegroupingspecified bythetopologyprogrammer fieldgroupingconsistenthashingona subsetoftuplefields allgroupingsendtoalltasks globalgroupingpicktaskwithlowestid storm concepts questionwhenatupleisemittedwhat taskdoesitgoto answerdependsonthegroupingspecified bythetopologyprogrammer streamgrouping shufflegroupingpicktaskin roundrobinfashion allgroupingsendtoalltasks globalgroupingpicktaskwithlowestid storm concepts questionwhenatupleisemittedwhat taskdoesitgoto answerdependsonthegroupingspecified bythetopologyprogrammer streamgrouping shufflegroupingpicktaskin roundrobinfashion fieldgroupingconsistenthashingona subsetoftuplefields globalgroupingpicktaskwithlowestid storm concepts questionwhenatupleisemittedwhat taskdoesitgoto answerdependsonthegroupingspecified bythetopologyprogrammer streamgrouping shufflegroupingpicktaskin roundrobinfashion fieldgroupingconsistenthashingona subsetoftuplefields allgroupingsendtoalltasks storm concepts questionwhenatupleisemittedwhat taskdoesitgoto answerdependsonthegroupingspecified bythetopologyprogrammer streamgrouping shufflegroupingpicktaskin roundrobinfashion fieldgroupingconsistenthashingona subsetoftuplefields allgroupingsendtoalltasks globalgroupingpicktaskwithlowestid lifecycleapi coreapi reliabilityapi storm api spout api public interface ispout extends serializable void openmap conf topologycontext context spoutoutputcollector collector void close void activate void deactivate void nexttuple void ackobject msgid void failobject msgid coreapi reliabilityapi storm api spout api public interface ispout extends serializable void openmap conf topologycontext context spoutoutputcollector collector void close lifecycleapi void activate void deactivate void nexttuple void ackobject msgid void failobject msgid lifecycleapi reliabilityapi storm api spout api public interface ispout extends serializable void openmap conf topologycontext context spoutoutputcollector collector void close void activate void deactivate void nexttuple coreapi void ackobject msgid void failobject msgid lifecycleapi coreapi storm api spout api public interface ispout extends serializable void openmap conf topologycontext context spoutoutputcollector collector void close void activate void deactivate void nexttuple void ackobject msgid reliabilityapi void failobject msgid lifecycleapi coreapi storm api bolt api public interface ibolt extends serializable void preparemap stormconf topologycontext context outputcollector collector void cleanup void executetuple input public interface ibasicbolt extends icomponent void preparemap stormconf topologycontext context void cleanup void executetuple input basicoutputcollector collector coreapi storm api bolt api public interface ibolt extends serializable void preparemap stormconf topologycontext context outputcollector collector lifecycleapi void cleanup void executetuple input public interface ibasicbolt extends icomponent void preparemap stormconf topologycontext context void cleanup void executetuple input basicoutputcollector collector lifecycleapi storm api bolt api public interface ibolt extends serializable void preparemap stormconf topologycontext context outputcollector collector void cleanup void executetuple input coreapi public interface ibasicbolt extends icomponent void preparemap stormconf topologycontext context void cleanup void executetuple input basicoutputcollector collector coreapi reliabilityapi storm api bolt output api public interface ioutputcollector extends ierrorreporter listinteger emitstring streamid collectiontuple anchors listobject tuple void emitdirectint taskid string streamid collectiontuple anchors listobject tuple void acktuple input void failtuple input reliabilityapi storm api bolt output api public interface ioutputcollector extends ierrorreporter listinteger emitstring streamid collectiontuple anchors listobject tuple void emitdirectint taskid coreapi string streamid collectiontuple anchors listobject tuple void acktuple input void failtuple input coreapi storm api bolt output api public interface ioutputcollector extends ierrorreporter listinteger emitstring streamid collectiontuple anchors listobject tuple void emitdirectint taskid string streamid collectiontuple anchors listobject tuple void acktuple input reliabilityapi void failtuple input storm word count example java sentencesspout wordsplitterbolt wordcountbolt shuffle partitionbyword contents removed topologybuilder builder new topologybuilder buildersetspoutspout1 new kafkaspout buildersetboltsplitterbolt new splitsentencebolt 8 shufflegroupingspout1 buildersetboltcountbolt new wordcountbolt 12 fieldsgroupingsplitterbolt new fieldsword storm word count example cont public class splitsentencebolt implements irichbolt private outputcollector _collector public void preparemap stormconf topologycontext context outputcollector collector _collector collector public void executetuple tuple string sentence tuplegetstring0 string word sentencesplit _collectoremittuple new valuesword _collectoracktuple public void declareoutputfieldsoutputfieldsdeclarer declarer declarerdeclarenew fieldsword storm word count example cont public class wordcountbolt extends basebasicbolt mapstring integer counts new hashmapstring integer public void executetuple tuple basicoutputcollector collector string word tuplegetstring0 integer count countsgetword count null count 0 count countsputword count collectoremitnew valuesword count public void declareoutputfieldsoutputfieldsdeclarer declarer declarerdeclarenew fieldsword count anchors tuple parent confirmtuplecorrectlyprocessed word guaranteeing message processing public class splitsentencebolt implements irichbolt private outputcollector _collector public void preparemap stormconf topologycontext context outputcollector collector _collector collector public void executetuple tuple string sentence tuplegetstring0 string word sentencesplit _collectoremittuple new valuesword _collectoracktuple public void declareoutputfieldsoutputfieldsdeclarer declarer declarerdeclarenew fieldsword confirmtuplecorrectlyprocessed guaranteeing message processing public class splitsentencebolt implements irichbolt private outputcollector _collector public void preparemap stormconf topologycontext context outputcollector collector _collector collector public void executetuple tuple string sentence tuplegetstring0 string word sentencesplit _collectoremittuple new valuesword anchors tuple parent _collectoracktuple word public void declareoutputfieldsoutputfieldsdeclarer declarer declarerdeclarenew fieldsword anchors tuple parent word guaranteeing message processing public class splitsentencebolt implements irichbolt private outputcollector _collector public void preparemap stormconf topologycontext context outputcollector collector _collector collector public void executetuple tuple string sentence tuplegetstring0 string word sentencesplit _collectoremittuple new valuesword _collectoracktuple confirmtuplecorrectlyprocessed public void declareoutputfieldsoutputfieldsdeclarer declarer declarerdeclarenew fieldsword guaranteeing message processing aspouttupleisnotfullyprocesseduntilalltuplesthatdescendfromithave beencompletelyprocessed thedescendantrelationshipencodedbyemitisrecordedinatupletree ifthetupletreeisnotcompletedwithinaspecifiedtimeoutthespouttupleis replayed thisisknownasatleastonceprocessingsemantics question isreplayingatuplealwayssafe conclusion apachestormheron distributedandfaulttolerant realtimecomputation tupleatatimeprocessingmodel noteworthy twitterusedadomainspecific languagedslcalledsummingbird tospecifycomputationsina declarativemannerandcompilethis bothtomrandstormheron topologies speed layer minibatching spark streaming sparkstreaming distributedandfaulttolerantnear realtimecomputation developedasaseparatelibraryontopof sparkin2015 reusessparkconceptsandprogramming modelrddsfunctionaltransformations failurerecoverythroughreexecution keydifferencewithstorm processesstreamtuplesinminibatches hasexactlyoncesemantics minibatching spark streaming minibatching sparkstreamingpartitionstheinputstreamintodisjointtimeintervals thedataineachtimeintervalbecomesaminibatchrddthestreamof minibatchrddsiscalledadiscretizedstreamdstream minibatchrddsarenormalrdds thereforenormalsparkoperatorscanbe usedtotransformactonminibatchrddspossiblyjointlywithnormalrdds eachtransformedminibatchrddcanthenbesavedtohdfsorstoredinadb orcommunicatedtoamessagequeue spark streaming word count example scala create local streamingcontext two working thread batch interval 1 second master requires 2 cores prevent starvation scenario val conf new sparkconfsetmasterlocal2setappnamenetworkwordcount val ssc new streamingcontextconf seconds1 create dstream connect hostnameport like localhost9999 val lines sscsockettextstreamlocalhost 9999 split line words val words linesflatmap_split count word batch val pairs wordsmapword word 1 val wordcounts pairsreducebykey_ _ print first ten elements rdd generated dstream console wordcountsprint sscstart start computation sscawaittermination wait computation terminate spark streaming concepts dstream streamofminibatchrddsintervalwidthisconfigurable operationsonthestreamflatmap filter areperformedoneach minibatchrddinthedstreamseparately theresultisanewdstream spark streaming concepts operationsondstreams print printthefirst10itemsofeachminibatchrddinthedstream debugging saveastextfiles saveeachminibatchrddasaseparatetextfile foreachrddfunc applyarbitraryfunctionfunctoeachrddinthestream maintaining state updatestatebykeyupdatefn updatefn newvalues oldstate newstate questionassumethatwewanttomaintainthecountofeverywordseensofar andupdatethiscountwhenevernewdataarrives howdowedothis answer useupdatestatebykeyormapwithstatebothonpairdstreams updatestatebykey example scala create dstream connect hostnameport like localhost9999 val lines sscsockettextstreamlocalhost 9999 split line words val words linesflatmap_split count word batch val pairs wordsmapword word 1 create dstream rdd total count every word ever seen val globalcount pairsupdatestatebykey vals totalcount totalcountmatch already state key update case sometotal valssum total otherwise create state case none valssum mapwithstate example scala create dstream connect hostnameport like localhost9999 val lines sscsockettextstreamlocalhost 9999 split line words val words linesflatmap_split count word batch val pairs wordsmapword word 1 specify stateupdate function mapwithstate word key need update state val mappingfunc word string one optionint state stateint val sum onegetorelse0 stategetoptiongetorelse0 val output word sum stateupdatesum updates count key word output need output new total word create dstream rdd total count every word ever seen statespec allows finegrained tuning partitions val globalcount pairsmapwithstatestatespecfunctionmappingfunc reduce last 30 seconds data every 10 seconds val windowedwordcounts pairsreducebykeyandwindow aintbint b seconds30 seconds10 answer usewindowingoperations everywindowhasalength3infigureandslidinginterval2inthefigurebothmustbe multiplesofthebatchinterval windowlengththedurationofthewindow slidingintervaltheintervalatwhichthewindowoperationisperformed windowing questionassumethatwewanttomaintainarunningcountofeverywordseenoverthepast 10minutesandupdatethisrunningcountwhenevernewdataarriveshowdowedothis reduce last 30 seconds data every 10 seconds val windowedwordcounts pairsreducebykeyandwindow aintbint b seconds30 seconds10 windowing questionassumethatwewanttomaintainarunningcountofeverywordseenoverthepast 10minutesandupdatethisrunningcountwhenevernewdataarriveshowdowedothis answer usewindowingoperations everywindowhasalength3infigureandslidinginterval2inthefigurebothmustbe multiplesofthebatchinterval windowlengththedurationofthewindow slidingintervaltheintervalatwhichthewindowoperationisperformed windowing reduce last 30 seconds data every 10 seconds val windowedwordcounts pairsreducebykeyandwindow aintbint b seconds30 seconds10 questionassumethatwewanttomaintainarunningcountofeverywordseenoverthepast 10minutesandupdatethisrunningcountwhenevernewdataarriveshowdowedothis answer usewindowingoperations everywindowhasalength3infigureandslidinginterval2inthefigurebothmustbe multiplesofthebatchinterval windowlengththedurationofthewindow slidingintervaltheintervalatwhichthewindowoperationisperformed faulttolerance semantics example lineage graph eachovalisanrdd andeachbluecircleanrddpartition edges indicatelineagedependency remember rddsareimmutableanddeterministicallyrecomputablegiventherddslineagegraph graphoftransformations likewiseifwetracklineageofoperationsonminibatchrddsinadstreamthenwecan recomputeanysuchrddwhenneeded faulttolerance semantics example lineage graph eachovalisanrdd andeachbluecircleanrddpartition edges indicatelineagedependency somecaveats lineagegraphmaybecomeverybigespeciallywhenstateismaintained addcheckpointingtoallowtruncatingthelineagegraph recomputationmeansthatinputdatamustbeavailabletoensurethissparkreplicates inputstreamdata enablewriteaheadloggingtoensurenodataloss faulttolerance semantics example lineage graph eachovalisanrdd andeachbluecircleanrddpartition edges indicatelineagedependency possibleprocessingguarantees atmostonceeachrecordwillbeeitherprocessedonceornotprocessedatall atleastonceeachrecordwillbeprocessedoneormoretimestheremaybeduplicates exactlyonceeachrecordwillbeprocessedexactlyoncenodatawillbelostandno datawillbeprocessedmultipletimes faulttolerance semantics example lineage graph eachovalisanrdd andeachbluecircleanrddpartition edges indicatelineagedependency sparkstreamingprovidesexactlyoncesemantics butthisisonlyfordatatransformations ifyouwantendtoendexactlyoncesemanticsyourdatasourceandyourdatasink shouldalsohavethissemanticswhichisthecaseegwithkafka references nmarzjwarren bigdata principlesandbestpracticesofscalablerealtime datasystemsmanningpublicationsapril2015 oboykinsritchiioconnelandjimmylin summingbird aframeworkfor integratingbatchandonlinemapreducecomputations proceedingsofvldb 2014 skulkarnietal twitterheron streamprocessingatscale sigmodconference 2015 mzahariaetal discretizedstreams faulttolerantstreamingcomputationat scale sospconference2013 questions acknowledgements basedoncontentcreatedbystijn vansummerenandonmaterialtaken fromthebookbigdata principlesand bestpracticesofscalablerealtimedata systemsbynathanmarzandjames warrenapril2015manning publicationsapril2015

infoh515 big data distributed management lecture 4 stream big data algorithms dimitris sacharidis 20232024 introduction many applications generate huge amounts data need processed fly data abundant store allcompute exactly 2 introduction processing data becomes challenge requirements results generated fly memory scale sublinear constanttime processing incoming data note distributed computing solve problem 1000 computers speed computation 1000 times 3 lecture outline frequent items filtering similarity search distinct counting 4 frequent items also called heavy hitters 5 warming problem frequent items identify frequent items occur qn times stream n items q 01 items arrive one one store inaccessible later much space needed exact solution frequent items counting every item impossible eg pairs people phone know front combinations frequent example 30 items 8 6 5 others 3 frequency q 20 need output frequent items worst case question much space needed exact solution answer worst case least linear n exact ωn lognn bits n number possible items n length total stream sketch proof full proof see httpswwwcsumdedusamir498karppdf let q 50 suppose already seen n2 1 elements stream configuration item count pairs seen far two distinct configurations eg top bottom figure algorithm needs able discern ie different memory state otherwise common future gives different results per configuration algorithm would wrong least one two configurations hence need least logconfigurations space configuration 1 endstate conf 1 result conf 1 2 6 1 common future 2 1 configurations 2 4 configuration 2 1 endstate conf 2 result conf 2 1 1 5 2 2 nothing 1 2 frequent items set solution given stream identify frequent items occur 20 time remove 5 elements different color get frequent item still hence removing 5 elements different color gives us smaller set keep frequent items done repeatedly longer possible remove 5 distinct color answer subset remaining 4 colors frequent items stream implementation identify frequent items occur qn times stream n items q 01 summary item arrives count summary update count count 1 else add 1 summary summary 1q decrease count pairs summary remove pairs count 0 frequent items summary algorithm karp et al problem find items exceeding frequency qn space 1 q counters counters logn bits o1 q logn space time per update o1 concession false positives 2pass karp et al propose data structure allows o1 worst case solution 2 lossy counting frequencies want frequencies bound false positives lossy counting algorithm manku et el start simplified version gradually extend manku gurmeet singh rajeev motwani approximate frequency counts data streams proceedings 28th international conference large data bases vldb endowment 2002 lossy counting frequencies following algorithm finds superset qfrequent items initialization none items counter item enters time counter count else count 1 start items counter count start 1 q delete counter query time return items counter lossy counting frequencies example 20 start freq 1 1 100 lossy counting frequencies example 20 start freq 1 1 50 2 1 100 lossy counting frequencies example 20 start freq 1 1 20 2 1 25 3 2 66 4 1 50 lossy counting frequencies example 20 start freq 1 1 17 2 1 20 3 2 50 4 1 33 6 1 100 lossy counting frequencies example 20 start freq 2 2 25 3 2 29 6 1 25 8 2 100 lossy counting frequencies example 20 start freq 2 1 25 17 4 29 27 1 25 8 6 26 19 3 25 lossy counting correctness work recorded frequent stream imagine marking recorded occurs recording starts stopped becomes infrequent since start recording infrequent infrequent recorded recorded whole stream partitioned parts frequent è frequent whole stream lossy counting space requirements let n length stream q minimal frequency threshold let k1q item summary appears among last k items appears twice among last 2k items appears x times among last xk items appears θn times among last n items lossy counting space requirements divide stream blocks size k 1q k candidates k candidates k candidates k candidates consume consume consume consume 4 elements 3 elements 2 elements 1 element constellation maximum number candidates p p p p q q q q mmmnnnooo j j k k l l b c e f g h k4 different k3 different k2 different k different appears appears appears appears 4 times 3 times 2 times 1 time lossy counting space requirements hence total space requirement worst case ki k lognk i1nk recall k 1q worst case space requirement 1q lognq candidates o1q lognq counters maintain lossy counting guarantee run algorithm e q threshold guaranteed point time true frequency interval count n count ne recorded recorded recorded less en occurrences report items count ³ q e n items reported frequency least q e items frequency q reported frequent items summary karps algorithm o1q counters false negatives may false positives lossy counting 1e logne space worst case usually much better maximum error e counts false negatives false positives range q e q exist many algorithms eg cmsketch 25 frequent items applications automatically block iptraffic pairs addresses taking 1 bandwidth using lossy counting set threshold 11 ε0001 θ001 1000 logn1000 counters worst case 6000 counters 1000000000 stream length constant time per item implemented inside router 26 frequent items applications automatically block iptraffic pairs addresses taking 1 bandwidth using lossy counting set threshold 11 ε0001 θ001 1000 logn1000 counters worst case 6000 counters 1000000000 stream length constant time per item implemented inside router give words frequency 001 collection books karp 2 scans maintaining 10000 strings 27 filtering 28 filtering suppose interested stream elements subset telecom network calls calls group users specific tariff plan internet packages traffic certain suspicious websites allow mails large whitelist email addresses stream possibly captured distributed way selection may periodically changing think blacklisted pairs ip addresses filtering problem setting stream items x target set items keep x x depending size keeping whole set keys filtering may take much memory use bitstring b length instead hash function hx modulo ith bit bi set 1 x hx denote bhx1 test membership checking bhy say possible outcomes bhy 1 0 0 0 1 1 1 0 0 1 filtering analysis false negatives bit bhy 1 may false positives though 𝒙 𝑨 𝒚 𝒏𝒐𝒕 𝒊𝒏 𝑨 bhxbhy test incorrectly yield true calculate false positive probability reason follows let length bitstring n number elements bhy 1 0 0 0 1 1 1 0 0 1 probability single element leaves bhy untouched 𝑚 1 𝑚 filtering analysis false negatives bit bhy 1 may false positives though 𝒙 𝑨 𝒚 𝒏𝒐𝒕 𝒊𝒏 𝑨 bhxbhy test incorrectly yield true calculate false positive probability reason follows let length bitstring n number elements bhy 1 0 0 0 1 1 1 0 0 1 probability every element leaves bhy untouched 𝑚 1 𝑚 filtering analysis false negatives bit bhy 1 may false positives though 𝒙 𝑨 𝒚 𝒏𝒐𝒕 𝒊𝒏 𝑨 bhxbhy test incorrectly yield true calculate false positive probability reason follows let length bitstring n number elements bhy 1 0 0 0 1 1 1 0 0 1 probability element sets bhy 1 𝑚 1 1 𝑚 filtering analysis false negatives bit bhy 1 may false positives though 𝒙 𝑨 𝒚 𝒏𝒐𝒕 𝒊𝒏 𝑨 bhxbhy test incorrectly yield true calculate false positive probability reason follows let length bitstring n number elements bhy 1 0 0 0 1 1 1 0 0 1 probability element sets bhy 1 𝑚 1 𝑃 𝐹𝑃 1 𝑚 filtering analysis false negatives bit bhy 1 may false positives though 𝒙 𝑨 𝒚 𝒏𝒐𝒕 𝒊𝒏 𝑨 bhxbhy test incorrectly yield true calculate false positive probability reason follows let length bitstring n number elements bhy 1 0 0 0 1 1 1 0 0 1 probability element sets bhy 1 𝑚 1 1 𝑃 𝐹𝑃 1 1 1 𝑚 𝑚 filtering analysis false negatives bit bhy 1 may false positives though 𝒙 𝑨 𝒚 𝒏𝒐𝒕 𝒊𝒏 𝑨 bhxbhy test incorrectly yield true calculate false positive probability reason follows let length bitstring n number elements bhy 1 0 0 0 1 1 1 0 0 1 1 1 𝑒 probability element sets bhy 1 𝑚 𝑚 1 1 𝑃 𝐹𝑃 1 1 1 𝑚 𝑚 filtering analysis false negatives bit bhy 1 may false positives though 𝒙 𝑨 𝒚 𝒏𝒐𝒕 𝒊𝒏 𝑨 bhxbhy test incorrectly yield true calculate false positive probability reason follows let length bitstring n number elements bhy 1 0 0 0 1 1 1 0 0 1 probability element sets bhy 1 𝑚 1 1 𝑃 𝐹𝑃 1 1 1 1 𝑒 𝑚 𝑚 filtering analysis false negatives bit bhy 1 may false positives though 𝒙 𝑨 𝒚 𝒏𝒐𝒕 𝒊𝒏 𝑨 bhxbhy test incorrectly yield true let length bitstring n number elements 𝑚 1 1 𝑃 𝐹𝑃 1 1 1 1 𝑒 𝑚 𝑚 hence want reduce probability false positive 2 𝜀 need bits bitstring 34678 filtering analysis bloom filter much better however using multiple k hash functions element 𝐴 set bits bh j bh bh 1 2 1 0 0 0 1 1 1 0 0 1 bh bh 3 k membership test 𝑦 𝐴 return true bits 1 bloom filter analysis k1 𝑃 𝐹𝑃𝑘 1 1 𝑒 need 1 every k positions assumes independent probabilities k 𝑃 𝐹𝑃 1 𝑒 different hash values realistic km shown want reduce probability false positive 𝜀 need 𝑛 ln𝜀 ln 𝜀 1 𝑚 k log ln2 ln 2 𝜀 bloom filter analysis bloom filters usage bloom filters great filtering especially distributed setting google chrome bloom filters filter malicious urls blacklists distributed bloom filter case hit full check url google bigtable apache cassandra bloom filters avoid costly disk lookups nonexisting keys bloom filters avoid costly disk lookups bloom filter speed alexmadon english wikipedia httpcommonswikimediaorgwikifilebloom_filter_speedsvgmediafilebloom_filter_speedsvg maintaining large sets bloom filters great filtering however usually suitable item arrives stream add bloom filter b use b compute property seen stream memory requirements bloom filter grow linear size set bloom filter suitable represent mediumsize set huge domain possible values eg set blacklisted ipaddresses distinct counting 46 counting number distinct items problem give number distinct unique items stream highly useful property number distinct visitors website estimate cardinality projecting relation onto subset attributes neither heavy hitters filters suited counting number distinct items stream distinct counting many people attend presentation 48 distinct counting attempt 1a many people attend presentation n0 whenever person p enters room p è p n1 exact algorithm n number complexity distinct people space n lenidentifier p time per person entry logn 49 distinct counting attempt 1b many people attend presentation n0 whenever person p enters room hp è hp n1 hp denotes hash identifier p near exact algorithm rangeh large enough complexity space logn time per person entry ologn 50 distinct counting solution satisfactory space n logn completely unacceptable time logn per entry barely acceptable introduce alternative hyperloglog sketch space loglogn constant update time approximate hll based idea flajoletmartin fm sketches 51 flajoletmartin sketches main idea many people attend presentation 0035 085 087 076 065 pick random hash function assigns every person earth number 0 1 52 flajoletmartin sketches main idea many people attend presentation 0035 085 087 076 065 compute number everyone entering room maintain maximum numbers seen call 53 flajoletmartin sketches main idea know largest number depends number elements many times entered maxhp hp hp hp maxhp hp 1 1 1 2 1 2 higher number elements n higher expectation 𝑃 𝑆 𝑥 𝑥 6 𝑁 76 𝐸 𝑆 𝑥 𝑁 𝑥 𝑑𝑥 𝑁 1 reverseengineer estimate n seeing 𝑁 67 54 flajoletmartin sketches main idea still number issues though first issue far away es accidentally one person high hash number may lead huge overestimations use multiple hash functions instead k independent hash functions h h give 1 k 1 k compute mean lower variance 𝑉𝑎𝑟 55 flajoletmartin sketches main idea second issue large n quantity quickly become indistinguishable 1 flajoletmartin uses following solution take binary representation hx 110101000 look number 0s tail 3 keep maximum r number 0s tail probability finding tail r zeros goes 1 n 2 goes 0 n 2 thus 2r almost always around n 56 fmsketches easy parallelise problem easily parallelizable max maxa maxb maxaèb local computation max h max h 1 k e b r u max h max h u b r e 1 k global maximum substream max h max h stream 1 k substrea max h max h 1 k substrea max h max h 1 k variant hyperloglog sketch workhorse comes cardinality counting avoids need many hashfunctions reduce error use first bits hashfunction split stream use last bits maintain fmsketch substream 𝟏𝟎𝟒 standard error size summary 𝒎 independent stream size hence loglogn dependence stream length demo httpcontentresearchneustarbizbloghllhtml similarity search 59 similarity search common operation find similar customers find similar documents eg plagiarism checker locality sensitive hashing wellknown technique quickly find nearduplicates illustrate principle jaccard coefficient measures distance sets 𝐴 𝐵 𝐽 𝐴 𝐵 𝐴 𝐵 60 jaccard coefficient 𝐴 𝐵 𝐽 𝐴 𝐵 𝐴 𝐵 indicates similar sets b example jabccd 14 jabcbcd 24 used eg detect near duplicates set ngrams document 1 b set ngrams document 2 similarity search jaccard index example recommendation needs made user u characterize users set items bought find users bought similar items recommend items bought users u bought abc abcd abd efg adf 62 similarity search jaccard index example recommendation needs made user u characterize users set items bought find users bought similar items recommend items bought users u bought abc abcd abd recommend efg adf 63 similarity search naïve algorithm user u db compute sim jitemsuitemsu sim threshold return itemsu complexity db highdimensional queries indexing methods inverted indices longer efficient need another indexing mechanism 64 jaccard coefficient let b subsets universal set v h function mapping elements v 12v example à 1 cà 2 à 3 b à 4 let minhash min min ha h aîa pr min min b h h açb aèb jab implicitly assume rangeh b 198 54 76 132 37 146 154 75 localitysensitive hashing call function min localitysensitive jaccard h independent localitysensitive functions combined independent functions h h 1 signature set vector min min min h1 h2 hm estimating jab vector b b vector b 1 1 let e b e estimator jab 66 jaccard coefficient example v b c e h h h h b 1 2 3 4 1 2 5 2 b b c b 2 5 2 4 c b c e c 3 1 4 5 4 4 1 3 1 2 2 2 e 5 3 3 1 b 2 1 1 3 c 1 1 2 1 jab 14 estimate 0 jac 12 estimate 12 jbc 25 estimate 14 localitysensitive hashing first illustrate principle jaccard index minhashing create signatures sets à 123 235 576 67 56 b à 123 3456 56 67 867 jab estimated number entries signature corresponds signature signature matrix set column represents set rows represents different hash partition b bands r rows per band b bands one signature signature matrix partition b bands divide matrix b bands r rows band hash portion column hash table k buckets make k large possible candidate column pairs hash bucket 1 band tune b r catch similar pairs nonsimilar pairs hashing bands columns 2 6 probably identical candidate pair buckets columns 6 7 surely different 1 2 3 4 5 6 7 b bands r rows matrix simplifying assumption enough buckets columns unlikely hash bucket unless identical particular band hereafter assume bucket means identical band assumption needed simplify analysis correctness algorithm example bands assume following case suppose 100000 columns 100k docs signatures 100 integers rows choose b 20 bands r 5 integersband goal find pairs documents least 08 similar c c 80 similar 1 2 find pairs ³ s08 similarity set b20 r5 assume simc c 08 1 2 since simc c ³ want c c candidate pair want 1 2 1 2 hash least 1 common bucket least one band identical probability c c identical one particular 1 2 band 085 0328 probability c c similar 20 bands 1032820 1 2 000035 ie 13000th 80similar column pairs false negatives miss would find 99965 pairs truly similar documents c c 30 similar 1 2 find pairs ³ s08 similarity set b20 r5 assume simc c 03 1 2 since simc c want c c hash 1 2 1 2 common buckets bands different probability c c identical one particular band 035 1 2 000243 probability c c identical least 1 20 bands 1 1 1 2 00024320 00474 words approximately 474 pairs docs similarity 03 end becoming candidate pairs false positives since examine candidate pairs turn similarity threshold lsh involves tradeoff pick number minhashes rows number bands b number rows r per band balance false positivesnegatives example 15 bands 5 rows number false positives would go number false negatives would go analysis lsh want probability sharing bucket similarity simc c two sets 1 2 dlohserht ytiralimis probability 1 chance 1 band 1 row gives probability remember sharing probability bucket equal hashvalues similarity similarity simc c two sets 1 2 b bands r rowsband columns c c similarity 1 2 pick band r rows prob rows band equal tr prob row band unequal 1 tr prob band identical 1 trb prob least 1 band identical 1 1 trb b bands r rows gives least bands one band identical identical probability 1b1r 1 1 r b sharing bucket rows row band band equal unequal similarity tsimc c two sets 1 2 example b 20 r 5 similarity threshold approx 055 prob least 1 band identical 11trb 2 006 3 047 note 4 186 steep gap around 5 470 threshold 6 802 7 975 8 9996 picking r b scurve picking r b get best scurve 50 hashfunctions r5 b10 1 09 08 07 06 05 04 03 02 red area false negative rate 01 blue area false positive rate 0 0 01 02 03 04 05 06 07 08 09 1 similarity tekcub gnirahs borp lsh summary tune b r get almost pairs similar signatures eliminate pairs similar signatures check main memory candidate pairs really similar signatures localitysensitive hashing reflects fact want hashfunctions take locality account alike two points likely hash bucket case study detecting wikiplagiarism make nearduplicate detection system wikipedia articles divide articles chapters 22901574 documents 84 case study detecting wikiplagiarism create shingles use minhash represent documents shingles 4 consecutive words hashed 32 bits 85 case study detecting wikiplagiarism create shingles use minhash represent documents shingles 4 consecutive words hashed 32 bits comparing two shingles takes 091 miliseconds hence finding near duplicate comparing documents collection query document takes almost 6 hours 86 case study detecting wikiplagiarism hence lsh used hashes used 20 50 hashes per sketch different experiments 87 case study detecting wikiplagiarism hence lsh used hashes used 20 50 hashes per sketch different experiments math 5 bands 4 rows per band p hxhy jxy 90 119045 0995 p hxhy jxy 70 117045 075 88 case study detecting wikiplagiarism hence lsh used hashes used 20 50 hashes per sketch different experiments math 7 bands 7 rows per band p hxhy jxy 90 119077 0989 p hxhy jxy 70 117077 045 89 case study detecting wikiplagiarism index creation time 90 case study detecting wikiplagiarism query time recall without index 6 hours 91 summary similarity search lsh indexing technique similarity search particularly useful highdimensional data extended similaritydistance measures key ingredient must exist hashfunctions h p hxhy increases simxy hashfunctions combined reach needed sensitivity 92 summary new tools frequent items filtering similarity search distinct counting willing rely approximate results many costly operations big datasets executed efficiently 93 acknowledgement based content created toon calders

infoh515 big data distributed management lecture 5 nosql databases dimitrissacharidis 20232024 outline overviewofnosql consistencyavailabilitypartition concurrencycontrolprotocols thetwophasecommitprotocol2pc multiversionconcurrencycontrolmvcc thepaxosprotocol keyvaluestores nosqlrebuttal 1 overviewofnosql dbms history 1970 1972 codd paper introduces relational model 1980 rise rdbms 1990 2000 2010 rdbms pros simple intuitive model often convenient reallife data models useful settings xml semistructured data rdf semantic web data elegant mathematical foundation set multiset theory relational algebra calculi allows efficient algorithms acid industrial strength implementations available rdbms cons impedance mismatch impedance mismatch httpswwwthoughtworkscominsightsblognosqldatabasesoverview developers want rdbms offers dbms history 1970 1972 codd paper introduces relational model 1980 rise rdbms 1990 object oriented 2000 2010 dbms history 1970 1972 codd paper introduces relational model 1980 nt rise rdbms n 1990 object oriented still 2000 b r 2010 rdbms cons impedance mismatch built distributed data management deploying clusters hard single point failure performance tailored specialized applications scaling scale cannot scale scale vs scale grow capacity getting powerful machine expensive scale grow capacity adding components buy new faster machine replace old dbms history 1970 1972 codd paper introduces relational model 1980 rise rdbms 1990 object oriented 2000 nosql 2010 nosql side note term introduced accidentally hash tag gathering practically means sql hard define common characteristics non relational clusterfriendly open source manage largescale data inspired modern web challenges schemaless nosql landscape essentially four types keyvalue stores document stores column families graph databases keyvalue stores simple data model everything stored inside keyvalue pairs document stores documentoriented data model store contents every document json id 1001 customer_id 7231 items product_id 4555 quantity 10 product_id 1213 quantity 1 id 1002 customer_id 231 items product_id 55 quantity 34 column families uses tables rows columns names format columns vary row row table graph databases graph data model rdf comparison nosql related systems basedontablemadebybillhoweuofwashington year system scaleto primary secondary transactions joins integrity views language datamodel mylabel paper 1000s index index analytics constraints algebra 1971 rdbms tables sqllike 2003 memcached keyval lookup 2004 mapreduce keyval mr 2005 couchdb record mr document filtermr 2006 bigtablehbase record compatwmr extrecord filtermr 2007 mongodb ecrecord document filter 2007 dynamo keyval lookup 2007 simpledb extrecord filter 2008 pig tables ralike 2008 hive tables sqllike 2008 cassandra ecrecord keyval filter 2009 voldemort ecrecord keyval lookup 2009 riak ecrecord mr keyval filter 2010 dremel tables sqllike 2011 megastore entitygroups tables filter 2011 tenzing tables sqllike 2011 sparkshark tables sqllike 2012 spanner tables sqllike 2012 accumulo record compatwmr extrecord filter 2013 impala tables sqllike scaleto1000sscaletothatmanymachines joinsanalyticsrelativecomplexcomputations primaryindexcanlookupbykeyusingindex integrityconstraintshardschema secondaryindexlookupbynonkeyattribute viewsmultipleviewsonsamedata transactionssupportskindoftransactions languagehigherlevelquerylanguage 3 nosql related systems year system scaleto primary secondary transactions joins integrity views language datamodel mylabel paper 1000s index index analytics constraints algebra 1971 rdbms tables sqllike 2003 memcached keyval lookup 2004 mapreduce keyval mr 2005 couchdb record mr document filtermr 2006 bigtablehbase record compatwmr extrecord filtermr 2007 mongodb ecrecord document filter 2007 dynamo keyval lookup 2007 simpledb extrecord filter 2008 pig tables ralike 2008 hive tables sqllike 2008 cassandra ecrecord keyval filter 2009 voldemort ecrecord keyval lookup 2009 riak ecrecord mr keyval filter 2010 dremel tables sqllike 2011 megastore entitygroups tables filter 2011 tenzing tables sqllike 2011 sparkshark tables sqllike 2012 spanner tables sqllike 2012 accumulo record compatwmr extrecord filter 2013 impala tables sqllike rdbmshavemanyfeaturesbutdidnotscalewellbeyond10sof machines theproblemwasnotsomuchreadoperationsbutlargeworkloadsof manysmallupdates 4 types nosql databases year system scaleto primary secondary transactions joins integrity views language datamodel mylabel paper 1000s index index analytics constraints algebra 1971 rdbms tables sqllike 2003 memcached keyval lookup 2004 mapreduce keyval mr 2005 couchdb record mr document filtermr 2006 bigtablehbase record compatwmr extrecord filtermr 2007 mongodb ecrecord document filter 2007 dynamo keyval lookup 2007 simpledb extrecord filter 2008 pig tables ralike 2008 hive tables sqllike 2008 cassandra ecrecord keyval filter 2009 voldemort ecrecord keyval lookup 2009 riak ecrecord mr keyval filter 2010 dremel tables sqllike 2011 megastore entitygroups tables filter 2011 tenzing tables sqllike 2011 sparkshark tables sqllike 2012 spanner tables sqllike 2012 accumulo record compatwmr extrecord filter 2013 impala tables sqllike columnfamilies clusteringfrom rickcattell2011scalablesqlandnosqldatastoressigmod documentstores rec394may20111227doi httpsdoiorg10114519789151978919 keyvaluestores 5 relationships nosql databases memcached 2003 2004 couchdb 2005 bigtable 2006 dynamo mongodb 2007 cassandra 2008 voldemort riak 2009 2010 directinfluencesharedfeatures megastore 2011 compatible implementationof accumulo spanner 2012 6 drivers behind nosql technology year source system scaleto primary secondary transactions joins integrity views language datamodel mylabel paper 1000s index index analytics constraints algebra 1971 many rdbms tables sqllike 2003 memcached keyval lookup 2004 google mapreduce keyval mr 2005 couchbase couchdb record mr document filtermr 2006 google bigtablehbase record compatwmr extrecord filtermr 2007 10gen mongodb ecrecord document filter 2007 amazon dynamo keyval lookup 2007 amazon simpledb extrecord filter 2008 amazon pig tables ralike 2008 facebook hive tables sqllike 2008 facebook cassandra ecrecord keyval filter 2009 voldemort ecrecord keyval lookup 2009 basho riak ecrecord mr keyval filter 2010 google dremel tables sqllike 2011 google megastore entitygroups tables filter 2011 google tenzing tables sqllike 2011 berkeley sparkshark tables sqllike 2012 google spanner tables sqllike 2012 accumulo accumulo record compatwmr extrecord filter 2013 cloudera impala tables sqllike 7 consistencyavailabilitypartition distribution replication wewanthighavailabilityreplication wewantconsistencyupdatepropagation 9 example user joe user sue friends sue friends joekai status imsleepy status headedtonewbondflick wall wall user kai friends sue status donefortonight wall writeoperation updatesuesstatus whoseesthenewstatuswho seestheoldone sqldatabase everyonemustseethesamethingeitheroldornewno matterhowlongittakes nosqldatabase forlargeapplicationswecannotaffordtowaitthatlong anditmaynotmatterthatmuchanyway 10 acid transactions thegoldenstandardfortransactionmanagementinrelationaldbmss recallwhatacidstandsfor atomicity eitheralleffectsofthetransactionarerecordedinthedatabase ornoneare consistency afterthetransactionthedatabasemustsatisfyallvalidityrules isolation atransactionmusthavetheimpressionitistheonlyonerunning intermsofvaluesithasreadandtheresultingstateofthedatabase thisisalsoreferredtoasserializabilityietheanswerstoqueriesandthefinal stateofthedatabasemustbeasifthetransactionswereseriallyexecuted durability onceatransactionhasbeencommitteditstayscommitted howtoachievethisforlargedistributeddatasets 11 2requesttoupdate 3success 2requesttoupdate 2requesttoupdate 3success 3fail endsinaninconsistentstate acid distributed datasets 1userupdates theirstatus coordinator subordinate1 subordinate2 subordinate3 12 3success 3success 3fail endsinaninconsistentstate acid distributed datasets 1userupdates 2requesttoupdate theirstatus coordinator 2requesttoupdate subordinate1 2requesttoupdate subordinate2 subordinate3 12 3success 3success 3fail endsinaninconsistentstate acid distributed datasets 1userupdates 2requesttoupdate theirstatus coordinator 2requesttoupdate subordinate1 2requesttoupdate subordinate2 subordinate3 12 acid distributed datasets 1userupdates 2requesttoupdate theirstatus 3success coordinator 2requesttoupdate subordinate1 2requesttoupdate 3success subordinate2 subordinate3 3fail endsinaninconsistentstate 12 consistency web era acid properties always desirable web applications different needs applications rdbms designed low predictable response time latency scalability elasticity low cost high availability flexible schemas semistructured data geographic distribution multiple data centers web applications usually without transactions strong consistency integrity complex queries cap theorem consistency system consistent state operation clients see data strong consistency acid vs eventual consistency base availability system always downtime node failure tolerance clients find available replica softwarehardware upgrade tolerance partition tolerance system continues function even split disconnected subsets eg due network errors additionremoval nodes reads writes well cap theorem e brewer n lynch shareddata system 2 3 properties achieved given moment time cap theorem ca single site clusters easier ensure nodes always contact eg 2pc partition occurs system blocks cp data may inaccessible availability sacrificed rest still consistentaccurate eg shardeddatabase ap system still available partitioning data returned may inaccurate ie availability partition tolerance important strict consistency eg dns caches masterslave replication need conflict resolution strategy cap theorem conventionaldatabasesassumenopartitioningclusterswere assumedtobesmallandlocal nosqlsystemsmaysacrificeconsistency originalpapers ericabrewer2000towardsrobustdistributedsystemsabstractinproceedingsofthenineteenth annualacmsymposiumonprinciplesofdistributedcomputingpodc00acmnewyorknyusa7 doihttpdxdoiorg101145343477343502 sethgilbertandnancylynch2002brewersconjectureandthefeasibilityofconsistentavailable partitiontolerantwebservicessigactnews332june20025159doi httpsdoiorg101145564585564601 13 eventual consistency thetermoriginatesinthecontextofdistributedoperationsystems itwastherenotseenasacompromisebutactuallythebestwayof dealingwithupdateconflicts originalreferencewheretermwascoined bterrymmtheimerkarinpetersenajdemersmjspreitzerandchhauser1995managing updateconflictsinbayouaweaklyconnectedreplicatedstoragesystemsigopsopersystrev295 december1995172182doihttpdxdoiorg101145224057224070 webelievethatapplicationsmustbeawarethattheymayreadweakly consistentdataandalsothat theirwriteoperationsmayconflictwith thoseofotherusersandapplications moreoverapplicationsmustbeinvolvedinthedetectionandresolu tionofconflictssincethesenaturallydependonthesemanticsofthe application 14 eventual consistency themeaning inabsenceofupdatesallreplicasconvergetowardsidenticalcopies whattheapplicationseesinthemeantimeissensitivetoreplication mechanicsanddifficulttopredict contrastwithrdbmsimmediateorstrongconsistencybutthere maybedeadlocks 15 transaction support nosql datastores year system scaleto primary secondary transactions joins integrity views language datamodel mylabel paper 1000s index index analytics constraints algebra 2003 memcached keyval lookup 2005 couchdb record mr document filtermr 2006 bigtablehbase record compatwmr extrecord filtermr 2007 mongodb ecrecord document filter 2007 dynamo keyval lookup 2008 cassandra ecrecord keyval filter 2009 voldemort ecrecord keyval lookup 2009 riak ecrecord mr keyval filter 2011 megastore entitygroups tables filter 2012 spanner tables sqllike 2012 accumulo record compatwmr extrecord filter recordstrongconsistencywithinrecords eceventualconsistency entitygroupsconsistentwithinpredefinedsetsofrelatedrecords 16 cap triangle bigtable p dynamo hbase riak hypertable voldemort megastore cassandra spanner couchdb accumulo c rdbms 17 concurrencycontrolprotocols concurrency control protocols p multiversion concurrency paxos control c twophasecommit 19 twophase commit protocol phase1 coordinatorsendspreparetocommit subordinatesmakesuretheycandosonomatterwhat andwritethedecisiontothelocallogtotoleratefailureafterthispoint subordinatesreplyreadytocommitornotabletocommit phase2 ifallsubordinateshavesentreadytocommit thenthecoordinatorsendscommittoall elseitsendsaborttoall inbothcasesthecoordinatorwritesthedecisiontothelocallog ifasubordinatereceivescommitorabortthenit writesthereceivedmessagetothelocallogand doestheupdateifitreceivedcommitandreleasestheresources 20 5writecommittolog 6commit 2prepare 1updaterequest endsinaconsi 37 tw wer ri n te et c rso tt cm tt oeit l ot golog 4readytocommit 2prepare 6commit 2prepare 7writecommittolog 4readytocommit 3writertctolog 6commit 4readytocommit 7writecommittolog 3writertctolog 2pc example one coordinator subordinate1 subordinate2 subordinate3 21 5writecommittolog 6commit 2prepare endsinaconsi s7 tw eri nte tc tm teit tolog 3writertctolog 4readytocommit 2prepare 6commit 2prepare 7writecommittolog 4readytocommit 3writertctolog 6commit 4readytocommit 7writecommittolog 3writertctolog 2pc example one 1updaterequest coordinator subordinate1 subordinate2 subordinate3 21 5writecommittolog 6commit endsinaconsi s7 tw eri nte tc tm teit tolog 3writertctolog 4readytocommit 6commit 7writecommittolog 4readytocommit 3writertctolog 6commit 4readytocommit 7writecommittolog 3writertctolog 2pc example one 2prepare 1updaterequest coordinator subordinate1 2prepare 2prepare subordinate2 subordinate3 21 5writecommittolog 6commit endsinaconsi s7 tw eri nte tc tm teit tolog 4readytocommit 6commit 7writecommittolog 4readytocommit 6commit 4readytocommit 7writecommittolog 2pc example one 2prepare 1updaterequest 3writertctolog coordinator subordinate1 2prepare 2prepare 3writertctolog subordinate2 3writertctolog subordinate3 21 5writecommittolog 6commit endsinaconsi s7 tw eri nte tc tm teit tolog 6commit 7writecommittolog 6commit 7writecommittolog 2pc example one 2prepare 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 2prepare 4readytocommit 3writertctolog subordinate2 4readytocommit 3writertctolog subordinate3 21 6commit endsinaconsi s7 tw eri nte tc tm teit tolog 6commit 7writecommittolog 6commit 7writecommittolog 2pc example one 5writecommittolog 2prepare 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 2prepare 4readytocommit 3writertctolog subordinate2 4readytocommit 3writertctolog subordinate3 21 endsinaconsi s7 tw eri nte tc tm teit tolog 7writecommittolog 7writecommittolog 2pc example one 5writecommittolog 6commit 2prepare 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 6commit 2prepare 4readytocommit 3writertctolog 6commit subordinate2 4readytocommit 3writertctolog subordinate3 21 endsinaconsistentstate 2pc example one 5writecommittolog 6commit 2prepare 7writecommittolog 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 6commit 2prepare 7writecommittolog 4readytocommit 3writertctolog 6commit subordinate2 4readytocommit 7writecommittolog 3writertctolog subordinate3 21 endsinaconsistentstate 2pc example one 5writecommittolog 6commit 2prepare 7writecommittolog 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 6commit 2prepare 7writecommittolog 4readytocommit 3writertctolog 6commit subordinate2 4readytocommit 7writecommittolog 3writertctolog subordinate3 21 2pc example one 5writecommittolog 6commit 2prepare 7writecommittolog 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 6commit 2prepare 7writecommittolog 4readytocommit 3writertctolog 6commit subordinate2 4readytocommit 7writecommittolog 3writertctolog subordinate3 endsinaconsistentstate 21 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator 5writerollbacktolog goesdownforawhileafterreadytocommit 6rollback 2prepare 7writerollbacktolog 1updaterequest 3writertctolog 4readytocommit 2prepare 6rollback 2prepare 7writerollbacktolog 4readytocommit 3writertctolog 6rollback 4cannotcommit 7writerollbacktolog 3writertctolog 2pc example two coordinator subordinate1 subordinate2 subordinate3 22 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator 5writerollbacktolog goesdownforawhileafterreadytocommit 6rollback 2prepare 7writerollbacktolog 3writertctolog 4readytocommit 2prepare 6rollback 2prepare 7writerollbacktolog 4readytocommit 3writertctolog 6rollback 4cannotcommit 7writerollbacktolog 3writertctolog 2pc example two 1updaterequest coordinator subordinate1 subordinate2 subordinate3 22 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator 5writerollbacktolog goesdownforawhileafterreadytocommit 6rollback 7writerollbacktolog 3writertctolog 4readytocommit 6rollback 7writerollbacktolog 4readytocommit 3writertctolog 6rollback 4cannotcommit 7writerollbacktolog 3writertctolog 2pc example two 2prepare 1updaterequest coordinator subordinate1 2prepare 2prepare subordinate2 subordinate3 22 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator 5writerollbacktolog goesdownforawhileafterreadytocommit 6rollback 7writerollbacktolog 4readytocommit 6rollback 7writerollbacktolog 4readytocommit 6rollback 4cannotcommit 7writerollbacktolog 2pc example two 2prepare 1updaterequest 3writertctolog coordinator subordinate1 2prepare 2prepare 3writertctolog subordinate2 3writertctolog subordinate3 22 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator 5writerollbacktolog goesdownforawhileafterreadytocommit 6rollback 7writerollbacktolog 6rollback 7writerollbacktolog 6rollback 7writerollbacktolog 2pc example two 2prepare 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 2prepare 4readytocommit 3writertctolog subordinate2 4cannotcommit 3writertctolog subordinate3 22 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator goesdownforawhileafterreadytocommit 6rollback 7writerollbacktolog 6rollback 7writerollbacktolog 6rollback 7writerollbacktolog 2pc example two 5writerollbacktolog 2prepare 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 2prepare 4readytocommit 3writertctolog subordinate2 4cannotcommit 3writertctolog subordinate3 22 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator goesdownforawhileafterreadytocommit 7writerollbacktolog 7writerollbacktolog 7writerollbacktolog 2pc example two 5writerollbacktolog 6rollback 2prepare 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 6rollback 2prepare 4readytocommit 3writertctolog 6rollback subordinate2 4cannotcommit 3writertctolog subordinate3 22 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator goesdownforawhileafterreadytocommit 2pc example two 5writerollbacktolog 6rollback 2prepare 7writerollbacktolog 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 6rollback 2prepare 7writerollbacktolog 4readytocommit 3writertctolog 6rollback subordinate2 4cannotcommit 7writerollbacktolog 3writertctolog subordinate3 22 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator goesdownforawhileafterreadytocommit 2pc example two 5writerollbacktolog 6rollback 2prepare 7writerollbacktolog 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 6rollback 2prepare 7writerollbacktolog 4readytocommit 3writertctolog 6rollback subordinate2 4cannotcommit 7writerollbacktolog 3writertctolog subordinate3 22 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator goesdownforawhileafterreadytocommit 2pc example two 5writerollbacktolog 6rollback 2prepare 7writerollbacktolog 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 6rollback 2prepare 7writerollbacktolog 4readytocommit 3writertctolog 6rollback subordinate2 4cannotcommit 7writerollbacktolog 3writertctolog subordinate3 endsinaconsistentstate 22 2pc example two 5writerollbacktolog 6rollback 2prepare 7writerollbacktolog 1updaterequest 3writertctolog 4readytocommit coordinator subordinate1 2prepare 6rollback 2prepare 7writerollbacktolog 4readytocommit 3writertctolog 6rollback subordinate2 4cannotcommit 7writerollbacktolog 3writertctolog subordinate3 endsinaconsistentstate 2pccanfreezepartsofyourdatabaseforalongtimeifthecoordinator goesdownforawhileafterreadytocommit 22 multiversion concurrency control mvcc implementedinquiteafewsqlandnosqldatabases couchdbibmdb2hbasemariadbmysqloraclepostgresqlsaphana thisapproachmaintainsanumberofversionsofadataitemand allocatestherightversiontoareadoperationofatransaction thusunlikeothermechanismsareadoperationinthismechanismis neverrejected sideeffect significantlymorestorageramanddiskisrequiredtomaintainmultiple versions tocheckunlimitedgrowthofversionsagarbagecollectionisrunwhen certaincriteriaaresatisfied 23 mvcc basics everytransactiontisassumedtohaveatimestamptst wewillpretendthisiswhenthetransactionwasatomicallyexecuted assumex x x aretheversionsofadataitemxcreatedbyawrite 1 2 n operationsoftransactions witheachx weassociatetwotimestamps read_tsx thelargestofthetimestampsoftransactionsthathave successfullyreadversionx write_tsx thetimestampofthetransactionthatwrotethevalueofversion x anewversionofx iscreatedonlybyawriteoperation write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 24 mvcc writerule toensureserializabilitythefollowingruleforwritingisused assumetransactiontattemptstowritex letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst ifread_tsxtstthenreject elsecreateanewversionx andread_tsxwrite_tsxtst j j j write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 25 mvcc writerule toensureserializabilitythefollowingruleforwritingisused assumetransactiontattemptstowritex letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst ifread_tsxtstthenreject elsecreateanewversionx andread_tsxwrite_tsxtst j j j write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 25 mvcc writerule toensureserializabilitythefollowingruleforwritingisused assumetransactiontattemptstowritex letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst ifread_tsxtstthenreject elsecreateanewversionx andread_tsxwrite_tsxtst j j j write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 25 mvcc writerule toensureserializabilitythefollowingruleforwritingisused assumetransactiontattemptstowritex letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst ifread_tsxtstthenreject elsecreateanewversionx andread_tsxwrite_tsxtst j j j write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 25 mvcc writerule toensureserializabilitythefollowingruleforwritingisused assumetransactiontattemptstowritex letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst ifread_tsxtstthenreject elsecreateanewversionx andread_tsxwrite_tsxtst j j j write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 25 mvcc readrule toensureserializabilitythefollowingruleforreadingisused assumetransactiontattemptstoreadx letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst returnthevalueofx tot setread_tsxtothelargestoftstandread_tsx note thisruleneverrejectsareadoperation write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 26 mvcc readrule toensureserializabilitythefollowingruleforreadingisused assumetransactiontattemptstoreadx letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst returnthevalueofx tot setread_tsxtothelargestoftstandread_tsx note thisruleneverrejectsareadoperation write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 26 mvcc readrule toensureserializabilitythefollowingruleforreadingisused assumetransactiontattemptstoreadx letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst returnthevalueofx tot setread_tsxtothelargestoftstandread_tsx note thisruleneverrejectsareadoperation write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 26 mvcc readrule toensureserializabilitythefollowingruleforreadingisused assumetransactiontattemptstoreadx letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst returnthevalueofx tot setread_tsxtothelargestoftstandread_tsx note thisruleneverrejectsareadoperation write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 26 mvcc readrule toensureserializabilitythefollowingruleforreadingisused assumetransactiontattemptstoreadx letx betheversionofxwiththehighestwrite_tsxthatisalsoless thanorequaltotst returnthevalueofx tot setread_tsxtothelargestoftstandread_tsx note thisruleneverrejectsareadoperation write_tsx2write_tsx5 write_tsx1 write_tsx3 read_tsx2 read_tsx5 read_tsx1 read_tsx3 26 paxos protocol oneofthemostwellknownandwidelyusedagreement protocolstoletagroupofagentscollectivelyagreeon something evenifsomeagentsdisappeartemporarilyormessagesare delayedandorlost leslielamport1998theparttimeparliamentacmtranscomputsyst162may 1998133169doihttpdxdoiorg101145279227279229 usedin google chubbypaxosbaseddistributedlockservice lat x e mostgoogleservicesusechubbydirectlyorindirectly yahoo zookeeperpaxosbaseddistributedlockservice zookeeperisopensourceandintegrateswithhadoop 27 paxos basics thedifferentrolesthatparticipantscanplay proposers suggestsvaluesforconsiderationbyacceptors egnodesthatwantalocktoupdatethesamerecord acceptors considersthevaluesproposedbyproposers rendersanacceptrejectdecision egasetofnodesassignedforarbitrationofupdateconflicts learners learnsthechosenvalue egnodesthatstoretherecordandwillregisterthelockifthereis agreement anodecanactinmorethanoneroleatthesametime 28 paxos protocol phase 1 phase1aprepare aproposersendsapreparemessagewithaproposal numberntoamajorityoftheacceptors thenumbernmustincreasewitheachround phase1bpromise theacceptorchecksifnishigherthananyproposal numberreceivedfromanyproposer ifsotheacceptorreturnsapromise messagewithmanduwheremanduaretheproposalnumberandvalueof thehighestnumberedproposalacceptedbyitsofar thisimpliesapromisetoignoreanyproposalwithanumbern 29 paxos protocol phase 2 phase2aacceptrequest theproposerwaitsuntilithasreceivedpromises fromamajorityoftheacceptorsandthenchecksifanyofthemalready acceptedavalueearlier ifsothenvissettotheuofthepromisemessage withthehighestm thenanacceptrequestmessagewithnandvissenttothe acceptorsthatresponded phase2baccept ifanacceptorreceivesanacceptrequestmessagewithn andvitchecksifitpromisedtoignoreallproposalswithproposalnumbern notitacceptstheproposalandsendsanacceptmessagetotheproposerand allthelearners concludeorretry ifalearnergetsacceptmessagesfromamajorityofthe acceptorsadecisionhasbeenreached ifaproposernoticesthatthisisnot happeningitmayinitiateanewroundwithhighern 30 paxos example five nodes athensand ephesusreceive update act proposer nodes act acceptors update reach consensus paxos example paxos example athenshas reached quorum 3 nodes moves phase 2 paxos example paxos example ephesushas also reached quorum 3 nodes moves phase 2 paxos example paxos example paxos example ephesushad reached quorum crashed athensbegins another round phase 1 higher generation number 2a paxos example athenshas reached quorum must accept already accepted value elanor highest generation elanor chosen value quorum accepted however nodes know yet paxos example athenshas crashed cyrenehas received value act proposer paxos example cyrenesees elanor accepted value highest generation number moves commit value alive nodes act learners paxos example consensus reached elanor accepted value keyvaluestores storage model simple interface insertwrite value associated key putkey value getread data associated key value getkey keyvalue stores similar distributed hash tables dht main idea partition set keyvalues across multiple machines key value key questions putkey value store new key value tuple getkey value associated given key stored providing fault tolerance scalability consistency directorybased architecture designated node called master mapping keys machines nodes store values associated keys masterdirectory putk14 v14 k5 n2 k14 n3 4 1 k105 n50 v 4 1 k ut p k5 v5 k14 v14 k105 v105 n n n n 1 2 3 50 directorybased architecture designated node called master mapping keys machines nodes store values associated keys masterdirectory getk14 k5 n2 k14 n3 v14 k105 n50 4 1 k 4 et v1 g k5 v5 k14 v14 k105 v105 n n n n 1 2 3 50 recursive vs iterative recursive access previous slides master handles requests iterativeaccess data node directly communicates requester masterdirectory putk14 v14 k5 n2 k14 n3 n3 k105 n50 putk14 v14 k5 v5 k14 v14 k105 v105 n n n n 1 2 3 50 recursive vs iterative recursive access previous slides master handles requests iterativeaccess data node directly communicates requester masterdirectory getk14 k5 n2 n3 k14 n3 v14 k105 n50 getk14 k5 v5 k14 v14 k105 v105 n n n n 1 2 3 50 recursive vs iterative recursive access pros faster typically masterdirectory closer nodes easier maintain consistency masterdirectory serialize putsgets cons scalability bottleneck values go masterdirectory iterative access pros scalable cons slower harder enforce data consistency fault tolerance replicate value several nodes usually place replicas different racks datacenter guard rack failures masterdirectory putk14 v14 k5 n2 n1 n3 k14 n1n3 putk14 p vu 1t 4k 14 v14 n1 k105 n50 k14 v14 k5 v5 k14 v14 k105 v105 n n n n 1 2 3 50 fault tolerance recursive previous slide vs iterative replication masterdirectory putk14 v14 k5 n2 n1 n3 k14 n1n3 utk14 v14 putk14 v14 k105 n50 p k14 v14 k5 v5 k14 v14 k105 v105 n n n n 1 2 3 50 fault tolerance recursive access iterative replication masterdirectory putk14 v14 k5 n2 k14 n1n3 k105 n50 putk14 v14 p utk 1 4 v 1 4 k14 v14 k5 v5 k14 v14 k105 v105 n n n n 1 2 3 50 scalability storage use nodes number requests serve requests nodes value stored parallel master replicate popular value nodes masterdirectory scalability replicate directory partition directory different keys served different mastersdirectories scalability load balancing metadata directory keep track storage availability node preferentially insert new values nodes storage available happens new node added cannot insert new values new node move values heavy loaded nodes new node happens node fails need replicate values fail node nodes consistency need make sure value replicated correctly know value replicated every node wait acknowledgements every node happens node fails replication pick another node try happens node slow slow entire put pick another node general multiple replicas slow puts fast gets consistency concurrent updates ie puts key may need make sure updates happen order masterdirectory putk14 v14 k5 n2 k14 n1n3 putk14 v14 putk14 putk14 v14 v14 reach n1 n3 k105 n50 p p reverse order p utk 1 4 pv u1 t4 k 1 4 v 1 4 u tk 1 4 vu tk 1 4 v w ha ut de es fi ng ee dt k14 return 1 1 4 4 kk1144 vv1144 k5 v5 kk1144 vv1144 k105 v105 n n n n 1 2 3 50 consistency models atomicconsistency linearizability readswrites getsputs replicas appear single underlying replica single system image think one updated time transactions eventualconsistency given enough time updates propagate system one weakest form consistency used many systems practice many others causal consistency operation event causes another operation b thread observes b sequential consistency writes variables different threads seen order threads strong consistency accesses seen nodes order sequentially scaling directory challenge directory contains number entries equal number key value tuples system tens hundreds billions entries system consistent hashing associate node unique id uni dimensional space 02m1 partition space across machines assume keys unidimensional space key value stored node smallest id larger key consistent hashing supposeweusethefollowingmethodtopartitionasetof keyvalue pairsovernservers weusethefunction hashkeymodulokeyni todeterminethatthepairkeyvalueisstoredonservers observation ifnchangesweneedtorepartitionallthepairs withconsistenthashingwesolvethisasfollows afixedhashfunctionhmapsboththekeysandtheserversipstoalarge addressspaceaeg02641 aisorganizedasaringscannedinclockwiseorder ifserversisfolloweddirectlybys ontheringthenallthekeysinrange hshsaremappedtos 32 illustration finding object example keyk ismappedtoservers keyk ismappedtoservers etc 1 3 2 1 2 k 2 map 1 3 map 4 k 1 33 illustration adding server aserverisadded alocalrehashingissufficient 2 k 2 5 keysthatneed 1 tobemoved froms2tos5 3 4 k 1 34 refinements todealwithserverfailureuse replicationegputacopyonthenext3 s1 s2 serversonthering k2 tobalancetheloadmapaserverto s5 severalpointsontheringcalledvirtual s1 nodes virtualnodes morevirtualpointsmeansmoreload s3 ofs1 alsousefuliftheserverfailssinceitsdata willbespreadoverseveralservers s4 alsousefulifserversdifferincapabilities k1 s1 whichistheruleinlargescalesystems 35 distributedindexingbasedonconsistenthashing mainquestion whereisthehashdirectorywithserverlocations severalpossibleanswers onaspecificmasternodeactingasaloadbalancer example cachingsystems raisesscalabilityissues eachnoderecordsitssuccessoronthering mayrequireon messagesforroutingqueriesnotresilienttofailures eachnoderecordspositionof1st2nd4th8th16th followerinring ensuresolognmessagesforroutingqueriesconvenienttradeoff forhighlydynamicnetworksegp2p fullduplicationofthehashdirectoryateachnode ensures1 messageforroutingheavymaintenanceprotocolwhichcanbe achievedthroughgossipingbroadcastofanyeventaffectingthe networktopology 36 history consistent hashing paperbycstheoreticiansfrommitthatintroducedch kargerlehmanleightonpanigrahylevineandlewinconsistenthashingandrandomtreesdistributed cachingprotocolsforrelievinghotspotsontheworldwidewebstoc1997 consistenthashinggavebirthtoakamaitechnologies foundedbydannylewinandtomleightonin1998 akamaiscontentdeliverynetworkisoneofthelargestdistributed computingplatforms nowmarketcapis12bandithas6200employees managingwebpresenceofmanymajorcompanies 2001 theconceptofdistributedhashtabledhtisproposedfor locatingfilesandchwasrepurposed nowusedindynamocouchbasecassandravoldemortriak 37 dynamodb p2p keyvalue store amazon 2007 context requirements amazon infrastructure tens thousands servers network components located many data centers around world commodity hardware used component failure standard mode operation amazon uses highly decentralized loosely coupled service oriented architecture consisting hundreds services low latency high throughput simple query model unique keys blobs schema multiaccess scale elasticity simple api getkey returning list objects context putkey context object return value key object values interpreted handled opaque array bytes voldemort keyvalue store initially developed still used linkedin inspired amazons dynamo features written java simple data model simple efficient queries joins complex queries constraints foreign keys etc performance queries predicted well p2p scaleout elastic consistent hashing keyspace eventual consistency high availability pluggable storage eg berkeleydb memory mysql api getkey returning value object putkey value writing objectvalue deletekey deleting object keys values complex compound objects well consisting lists maps nosqlrebuttal joins may may matter whydowenotneedsupportforjoins wecanprecomputemanyjoinsbynestingthedataegclusteringall commentsonablogpostwiththatblogpost whydoweneedsupportforjoins dictatesaccesspathandmakesotheraccesshardandorinefficient asksprogrammertopickjoinalgorithmwherethismightchangeintime eghowtofindallcommentsbysueonblogpostsbyjim 1 findalcommentsbysueandforeachretrievetheblogpostandcheckwhichare byjim 2 findalblogpostsbyjimandforeachretrievethecommentsandcheckwhichare bysue 3 selecttheblogpostsbyjimselectthecommentsbysuesorteachonblogidand mergethemwhileselectingthematchesakasortmergejoin whichisbest 39 nosql criticism analysisbymichaelstonebraker michaelstonebraker2011stonebrakeronnosqlandenterprisescommunacm548august2011 1011doihttpsdoiorg10114519785421978546 twovaluepropositionsforusingnosql performance istartedwithmysqlbuthadahardtimescalingitout inadistributedenvironment flexibility mydatadoesntconformtoarigidschema 40 nosql criticism flexibility argument whoarethecustomersofnosql lotsofstartups whyveryfewenterprises mostapplicationsaretraditionaloltponstructureddataafewother applicationatthefringesbutconsideredlessimportant 41 nosql criticism flexibility argument noacidequalsnointerest screwingupmissioncriticaldataisanonono lowlevelquerylanguageisdeath remembercodasyl nosqlmeansnostandards onetypicallargeenterprisehas10000databases theseneedaccepted standards 42 questions 43 acknowledgements basedoncontentcreatedbystijnvansummerennikosmamoulisjohann gamperpanagiotisbourosandunmeshjoshi 44

infoh515 big data distributed management lecture 6 parallel processing dimitrissacharidis 20232024 performance bottlenecks context limitations data parallellism story far possible analyze huge data sets exploiting data parallellism partition distribute data cluster consisting many machines machines operate parallel part data communicate network compute final result 1 story far possible analyze huge data sets exploiting data parallellism partition distribute data cluster consisting many machines machines operate parallel part data communicate network compute final result performance bottlenecks context limitations data parallellism 1 lecture outline wheresthebottleneck thebulksynchronousparallelbspmodel bspapplication thinklikeavertex speedupandscaleup scalabilitybutatwhatcost 2 wheres bottleneck definition definition throughputisthetotalamountofworkdoneinagiventime examples 1tbofdataanalyzedin1h291mbsthroughput 100mbcopiedfromdiskatodiskbin10s10mbsthroughput figuresourcehttpperfmatrixblogspotbe201612 latencybandwidththroughputresponsetimehtml 4 definition definition latencyalsoknownasresponsetimeisthetimebetweenthestartandcom pletionofanevent examples ifittakes1htoanalyze1terabytecompletelythelatencyis1h adiskseekonarotationaldisktakes10mswhichequalsthelatency figuresourcehttpperfmatrixblogspotbe201612 latencybandwidththroughputresponsetimehtml 4 1000000 100 important latency numbers l1cachereference 05ns branchmispredict 5ns l2cachereference 7ns mutexlockunlock 25ns mainmemoryreference 100ns compress1kbyteswithzippy 3000ns 3µs send2kbytesover1gbpsnetwork 10000ns 10µs read4krandomlyfromssd 150000ns 150µs read1mbsequentiallyfrommemory 250000ns 250µs roundtripwithinsamedatacenter 500000ns 500µs read1mbsequentiallyfromssd 1000000ns 1ms diskseek 10000000ns 10ms read1mbsequentiallyfromdisk 20000000ns 20ms sendpacketuseuropeus 150000000ns 150ms 5 originalcompilationbyjeffdeanpeternorvigwcontributionsfromjoehellersteinerikmeijker rememberns109sµs106sms103s 1000000 important latency numbers l1cachereference 05ns branchmispredict 5ns l2cachereference 7ns mutexlockunlock 25ns mainmemoryreference 100ns compress1kbyteswithzippy 3000ns 3µs send2kbytesover1gbpsnetwork 10000ns 10µs read4krandomlyfromssd 150000ns 150µs read1mbsequentiallyfrommemory 250000ns 250µs roundtripwithinsamedatacenter 500000ns 500µs read1mbsequentiallyfromssd 1000000ns 1ms diskseek 10000000ns 10ms read1mbsequentiallyfromdisk 20000000ns 20ms sendpacketuseuropeus 150000000ns 150ms 100 5 originalcompilationbyjeffdeanpeternorvigwcontributionsfromjoehellersteinerikmeijker rememberns109sµs106sms103s 100 important latency numbers l1cachereference 05ns branchmispredict 5ns l2cachereference 7ns mutexlockunlock 25ns mainmemoryreference 100ns compress1kbyteswithzippy 3000ns 3µs send2kbytesover1gbpsnetwork 10000ns 10µs read4krandomlyfromssd 150000ns 150µs read1mbsequentiallyfrommemory 250000ns 250µs roundtripwithinsamedatacenter 500000ns 500µs read1mbsequentiallyfromssd 1000000ns 1ms diskseek 10000000ns 10ms read1mbsequentiallyfromdisk 20000000ns 20ms sendpacketuseuropeus 150000000ns 150ms 1000000 5 originalcompilationbyjeffdeanpeternorvigwcontributionsfromjoehellersteinerikmeijker rememberns109sµs106sms103s mean togetabetterintuitionoftheorders ofmagnitudedifferencesinthese latenciesletshumanizetheduration method multiplyalldurationsbyabillion thenwecanassociateeachlatency numbertoahumanactivity 6 humanized latency numbers humanizeddurationsgroupedbymagnitude minute l1cachereference 05s oneheartbeat branchmispredict 5s yawn l2cachereference 7s longyawn mutexlockunlock 25s makingacoffee hour mainmemoryreference 100s brushingyourteeth compress1kbyteswithzippy 50min onetvshowepisode 7 humanized latency numbers day send2kbytesover1gbpsnetwork 55hr workdayafternoon week read4krandomlyfromssd 17days aweekend read1mbsequentiallyfrommemory 29days alongweekend roundtripwithinsamedatacenter 58days asmallvacation read1mbsequentiallyfromssd 116days twoweeks 8 humanized latency numbers year diskseek 165weeks asemesteratulb read1mbsequentiallyfromdisk 78months almostafullpregnancy decade sendpacketuseuropeus 48years thelengthofyourstudies 9 latency conclusion fast slow slowest 10 diskread networktransfer diskwrite note latenciesaccumulateifyouhaveasequenceofjobs latency mapreduce spottheimportantlatenciesinasinglemrjob 11 note latenciesaccumulateifyouhaveasequenceofjobs latency mapreduce spottheimportantlatenciesinasinglemrjob diskread networktransfer diskwrite 11 latency mapreduce spottheimportantlatenciesinasinglemrjob diskread networktransfer diskwrite note latenciesaccumulateifyouhaveasequenceofjobs 11 memoryordisk networktransfer memoryordisk latency spark spottheimportantlatenciesinasparkprogram figuresourcesparkinactionbook 12 latency spark spottheimportantlatenciesinasparkprogram memoryordisk networktransfer memoryordisk figuresourcesparkinactionbook 12 answer letsinvestigate question isthisalwayspossible conclusions mr spark sparkisbetterforiterativealgorithmstypicalinmachinelearning tryandavoidoperationsthatcauseashuffleinbothmrandspark instanceprefermaponlymrjobsbecarefulhowyoupartitioninspark 13 answer letsinvestigate conclusions mr spark sparkisbetterforiterativealgorithmstypicalinmachinelearning tryandavoidoperationsthatcauseashuffleinbothmrandspark instanceprefermaponlymrjobsbecarefulhowyoupartitioninspark question isthisalwayspossible 13 conclusions mr spark sparkisbetterforiterativealgorithmstypicalinmachinelearning tryandavoidoperationsthatcauseashuffleinbothmrandspark instanceprefermaponlymrjobsbecarefulhowyoupartitioninspark question isthisalwayspossible answer letsinvestigate 13 bulk synchronous parallel bsp model questionhowdoyouinvestigatethecomputationalcomplexityofaproblem thatismeanttobesolvedbyaparallelcomputer parallel computer definition aparallelcomputerconsistsofasetofprocessorssuchasaclusterofpcs thatworktogethertosolveacomputationalproblem twotypes sharedmemoryparallelcomputersegmulticorecomputera supercomputer sharednothingclusterofmachinesaka distributedmemoryparallel computeregabigdatacomputecluster 15 parallel computer definition aparallelcomputerconsistsofasetofprocessorssuchasaclusterofpcs thatworktogethertosolveacomputationalproblem twotypes sharedmemoryparallelcomputersegmulticorecomputera supercomputer sharednothingclusterofmachinesaka distributedmemoryparallel computeregabigdatacomputecluster questionhowdoyouinvestigatethecomputationalcomplexityofaproblem thatismeanttobesolvedbyaparallelcomputer 15 parallel computer abstract model communication network cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 p p p p p bulksynchronousparallelbspcomputer proposedbyleslievaliant1989 purpose provideasimpleyetpracticalframeworkforgeneralpurposeparallelcomputing inordertosupportthecreationofarchitectureindependentandscalableparallel software 16 bsp computer model parallel computer abspcomputerconsistsofacollectionof processorseachwithitsownmemoryitis communication henceadistributedmemorycomputer network pointtopointcommunicationbetween processorsisenabledbyacommunication cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 networkwhichistreatedasablackbox cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 accesstoownmemoryisfasttoremote p p p p p memoryslower uniformtimeaccesstoallremotememories 17 acomputationphaseprocessorscomputeasynchronouslyondatainlocalmemory acommunicationphaseduringwhichprocessorscansenddatatootherprocessors asynchronizationbarrierprocessorssynchronizeandwaituntilallprocessorshave finishedcomputationcommunication oncethebarrierispassedallprocessorshavereceivedalldatasenttotheminthe communicationphaseandthisdataisnowhencelocallyavailableforthenextsuperstep computation bsp computer theexecutionofanalgorithmonabspconsistsofasequenceofsupersteps eachsuperstepconsistsof 18 acommunicationphaseduringwhichprocessorscansenddatatootherprocessors asynchronizationbarrierprocessorssynchronizeandwaituntilallprocessorshave finishedcomputationcommunication oncethebarrierispassedallprocessorshavereceivedalldatasenttotheminthe communicationphaseandthisdataisnowhencelocallyavailableforthenextsuperstep computation bsp computer theexecutionofanalgorithmonabspconsistsofasequenceofsupersteps eachsuperstepconsistsof acomputationphaseprocessorscomputeasynchronouslyondatainlocalmemory 18 asynchronizationbarrierprocessorssynchronizeandwaituntilallprocessorshave finishedcomputationcommunication oncethebarrierispassedallprocessorshavereceivedalldatasenttotheminthe communicationphaseandthisdataisnowhencelocallyavailableforthenextsuperstep computation bsp computer theexecutionofanalgorithmonabspconsistsofasequenceofsupersteps eachsuperstepconsistsof acomputationphaseprocessorscomputeasynchronouslyondatainlocalmemory acommunicationphaseduringwhichprocessorscansenddatatootherprocessors 18 computation bsp computer theexecutionofanalgorithmonabspconsistsofasequenceofsupersteps eachsuperstepconsistsof acomputationphaseprocessorscomputeasynchronouslyondatainlocalmemory acommunicationphaseduringwhichprocessorscansenddatatootherprocessors asynchronizationbarrierprocessorssynchronizeandwaituntilallprocessorshave finishedcomputationcommunication oncethebarrierispassedallprocessorshavereceivedalldatasenttotheminthe communicationphaseandthisdataisnowhencelocallyavailableforthenextsuperstep 18 w isthemaximumnumberoflocal operationsperformedbyaprocessorin superstepi h isthemaximumnumberofdata unitssentorreceivedbyaprocessor duringsuperstepi g isthethroughputratiotime requiredtocommunicateonedataunit l isthecommunicationlatency1 questionwhatdoesthiscost represent bsp cost model basicarithmeticoperationsandlocalmemory accesseshaveunitcosteg1timeunit costc ofasuperstepi c w h g l 1modelssetupcostofcommunicationsandsynchronization 19 h isthemaximumnumberofdata questionwhatdoesthiscost unitssentorreceivedbyaprocessor represent duringsuperstepi g isthethroughputratiotime requiredtocommunicateonedataunit l isthecommunicationlatency1 bsp cost model basicarithmeticoperationsandlocalmemory accesseshaveunitcosteg1timeunit costc ofasuperstepi c w h g l w isthemaximumnumberoflocal operationsperformedbyaprocessorin superstepi 1modelssetupcostofcommunicationsandsynchronization 19 questionwhatdoesthiscost represent g isthethroughputratiotime requiredtocommunicateonedataunit l isthecommunicationlatency1 bsp cost model basicarithmeticoperationsandlocalmemory accesseshaveunitcosteg1timeunit costc ofasuperstepi c w h g l w isthemaximumnumberoflocal operationsperformedbyaprocessorin superstepi h isthemaximumnumberofdata unitssentorreceivedbyaprocessor duringsuperstepi 1modelssetupcostofcommunicationsandsynchronization 19 questionwhatdoesthiscost represent l isthecommunicationlatency1 bsp cost model basicarithmeticoperationsandlocalmemory accesseshaveunitcosteg1timeunit costc ofasuperstepi c w h g l w isthemaximumnumberoflocal operationsperformedbyaprocessorin superstepi h isthemaximumnumberofdata unitssentorreceivedbyaprocessor duringsuperstepi g isthethroughputratiotime requiredtocommunicateonedataunit 1modelssetupcostofcommunicationsandsynchronization 19 questionwhatdoesthiscost represent bsp cost model basicarithmeticoperationsandlocalmemory accesseshaveunitcosteg1timeunit costc ofasuperstepi c w h g l w isthemaximumnumberoflocal operationsperformedbyaprocessorin superstepi h isthemaximumnumberofdata unitssentorreceivedbyaprocessor duringsuperstepi g isthethroughputratiotime requiredtocommunicateonedataunit l isthecommunicationlatency1 1modelssetupcostofcommunicationsandsynchronization 19 bsp cost model basicarithmeticoperationsandlocalmemory accesseshaveunitcosteg1timeunit costc ofasuperstepi c w h g l w isthemaximumnumberoflocal operationsperformedbyaprocessorin superstepi h isthemaximumnumberofdata questionwhatdoesthiscost unitssentorreceivedbyaprocessor represent duringsuperstepi g isthethroughputratiotime requiredtocommunicateonedataunit l isthecommunicationlatency1 1modelssetupcostofcommunicationsandsynchronization 19 bsp cost model costcofacomputationconsistingofssupersteps c w h gl i1 w hgsl i1 i1 whgsl 20 bsp algorithm design costcofacomputationconsistingofssupersteps cwhgsl upperlowerboundsareknownforthebspcostofmanyproblems ifyoueveryneedasmartalgorithmforsolvingaproblemindistributedfashionhavea lookattheliterature anthentryandimplementthebspalgorithminyourfavoritebigdataframework 21 questionwhichofthesetwoisthemostsequential bsp algorithm design whendesigningabspalgorithmthereis oftenatradeoffbetweencommunication andsynchronization simplisticexample communicateasinglevaluetoallprocessors method1 broadcastthevaluetoallprocessors hop so1 method2 organizethepprocessorsinabalancedbinarytree hsologp 22 bsp algorithm design whendesigningabspalgorithmthereis oftenatradeoffbetweencommunication andsynchronization simplisticexample communicateasinglevaluetoallprocessors method1 broadcastthevaluetoallprocessors hop so1 method2 organizethepprocessorsinabalancedbinarytree hsologp questionwhichofthesetwoisthemostsequential 22 costanalysis cid24 cid25 totalcost n p p1 p z1 g2zl z h w bsp algorithm design otherexample 12 0 4 7 1 2 15 11 3 2 0 1 2 3 4 5 6 7 8 9 innerproductoftwovectorsxandy distributetheelementsofxandy 1 9 2 0 1 12 1 2 3 8 0 1 2 3 4 5 6 7 8 9 overthepprocessorssuchthatx andy areonthesameprocessors 22 8 23 22 foreachi locallymultiplyandsumthedataat 22 8 23 22 22 8 23 22 22 8 23 22 22 8 23 22 eachprocessor 75 75 75 75 broadcastthesevalues parallelinnerproductover4processors eachprocessorthencomputesthe finalresult 23 cid24 cid25 totalcost n p p1 p z1 g2zl z h w bsp algorithm design otherexample 12 0 4 7 1 2 15 11 3 2 0 1 2 3 4 5 6 7 8 9 innerproductoftwovectorsxandy distributetheelementsofxandy 1 9 2 0 1 12 1 2 3 8 0 1 2 3 4 5 6 7 8 9 overthepprocessorssuchthatx andy areonthesameprocessors 22 8 23 22 foreachi locallymultiplyandsumthedataat 22 8 23 22 22 8 23 22 22 8 23 22 22 8 23 22 eachprocessor 75 75 75 75 broadcastthesevalues parallelinnerproductover4processors eachprocessorthencomputesthe finalresult costanalysis superstep1 l localworktomultiplyandsum w n 1 p broadcast h p1 1 23 cid24 cid25 totalcost n p p1 p z1 g2zl z h w bsp algorithm design otherexample 12 0 4 7 1 2 15 11 3 2 0 1 2 3 4 5 6 7 8 9 innerproductoftwovectorsxandy distributetheelementsofxandy 1 9 2 0 1 12 1 2 3 8 0 1 2 3 4 5 6 7 8 9 overthepprocessorssuchthatx andy areonthesameprocessors 22 8 23 22 foreachi locallymultiplyandsumthedataat 22 8 23 22 22 8 23 22 22 8 23 22 22 8 23 22 eachprocessor 75 75 75 75 broadcastthesevalues parallelinnerproductover4processors eachprocessorthencomputesthe finalresult costanalysis superstep2 localworktosumpartialresults w p1 2 nocommunication h 0 2 23 bsp algorithm design otherexample 12 0 4 7 1 2 15 11 3 2 0 1 2 3 4 5 6 7 8 9 innerproductoftwovectorsxandy distributetheelementsofxandy 1 9 2 0 1 12 1 2 3 8 0 1 2 3 4 5 6 7 8 9 overthepprocessorssuchthatx andy areonthesameprocessors 22 8 23 22 foreachi locallymultiplyandsumthedataat 22 8 23 22 22 8 23 22 22 8 23 22 22 8 23 22 eachprocessor 75 75 75 75 broadcastthesevalues parallelinnerproductover4processors eachprocessorthencomputesthe finalresult costanalysis cid24 cid25 totalcost n p p1 p z1 g2zl z h w 23 superstep1 superstep2 onemrjobconsistsoftwosupersteps 1forthemapphase1forthereduce phase sequencesofmrjobshencegiveyouawaytoimplementabsp algorithm bsp mapreduce spotthesuperstepsinasinglemrjob 24 bsp mapreduce spotthesuperstepsinasinglemrjob superstep1 superstep2 onemrjobconsistsoftwosupersteps 1forthemapphase1forthereduce phase sequencesofmrjobshencegiveyouawaytoimplementabsp algorithm 24 superstep1 superstep2 bsp spark spotthesuperstepsinasparkprogram figuresourcesparkinactionbook 25 bsp spark spotthesuperstepsinasparkprogram superstep1 superstep2 figuresourcesparkinactionbook 25 animportantcomment thecommunicationnetworkofabspcomputerneednotbeacomputernetworkitis justacommunicationchannel ifyouinterpretthecommunicationnetworkasasharedmemorywithremotememory accessasfastaslocalmemorythenthismodelsasharedmemoryparallelcomputer pram bsp conclusion abspcomputermodelsadistributedmemory parallelcomputer communication itallowsanalysisofparallelalgorithmstaking network intoaccountthecostofparallellocal computationcommunicationand cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 synchronization cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 thebspcomputermodelisverygeneral p p p p p thereforealgorithmsdesignedforabsp computerareportabletheycanberun efficientlyonmanydifferentparallel computersprogrammingframeworks 26 bsp conclusion abspcomputermodelsadistributedmemory parallelcomputer communication itallowsanalysisofparallelalgorithmstaking network intoaccountthecostofparallellocal computationcommunicationand cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 synchronization cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 thebspcomputermodelisverygeneral p p p p p thereforealgorithmsdesignedforabsp computerareportabletheycanberun efficientlyonmanydifferentparallel computersprogrammingframeworks animportantcomment thecommunicationnetworkofabspcomputerneednotbeacomputernetworkitis justacommunicationchannel ifyouinterpretthecommunicationnetworkasasharedmemorywithremotememory accessasfastaslocalmemorythenthismodelsasharedmemoryparallelcomputer pram 26 bsp application think like vertex applications recommendation cybersecurity pagerank frauddetection websearch clustering big graph analytics lotsofbigdataanalyticsinvolveanalysis ofbiggraphs socialnetworks biologicalnetworks mobilecallnetworks worldwideweb customermerchantgraphsamazon ebay 28 big graph analytics lotsofbigdataanalyticsinvolveanalysis ofbiggraphs socialnetworks biologicalnetworks mobilecallnetworks worldwideweb customermerchantgraphsamazon ebay applications recommendation cybersecurity pagerank frauddetection websearch clustering 28 thepagerankp ofapageiisgivenby x p p j n jbi j whereb isthesetofpageslinkingtoiandn j isthenumberoflinksonpagej example pagerank example 1 2 pagerankgooglesfamousalgorithmfor measuringtheauthorityofawebpagebased ontheunderlyingnetworkofhyperlinks 3 6 5 4 29 example pagerank example 1 2 pagerankgooglesfamousalgorithmfor measuringtheauthorityofawebpagebased ontheunderlyingnetworkofhyperlinks 3 thepagerankp ofapageiisgivenby 6 5 x p p j n 4 jbi j whereb isthesetofpageslinkingtoiandn j isthenumberoflinksonpagej 29 example pagerank example 1 2 pagerankgooglesfamousalgorithmfor measuringtheauthorityofawebpagebased ontheunderlyingnetworkofhyperlinks 3 thepagerankp ofapageiisgivenby 6 5 x p p j n 4 jbi j whereb isthesetofpageslinkingtoiandn j isthenumberoflinksonpagej 1 1 1 p p p p p 1 3 3 2 2 1 3 3 1 1 1 p p p p p 3 2 1 4 2 5 2 6 1 1 1 1 p p p p p p 5 3 3 2 4 6 2 4 2 5 29 example pagerank example 1 2 pagerankgooglesfamousalgorithmfor measuringtheauthorityofawebpagebased ontheunderlyingnetworkofhyperlinks 3 thepagerankp ofapageiisgivenby 6 5 x p p j n 4 jbi j whereb isthesetofpageslinkingtoiandn j isthenumberoflinksonpagej 2 3 2 3 2 3 p 0 0 1 0 0 0 p 6 17 6 3 7 6 17 6p 7 61 0 1 0 0 07 6p 7 6 27 62 3 7 6 27 6 6 6p 37 7 76 6 621 0 0 0 0 07 7 76 6 6p 37 7 7 6p 7 60 0 0 0 1 17 6p 7 6 47 6 2 27 6 47 4p 5 40 0 1 1 0 05 4p 5 5 3 2 5 p 0 0 0 1 1 0 p 6 2 2 6 29 onecanarguethatthealgorithmisnotimmediatelyclear alsowherearethe communicationsynchronizationbottlenecks example pagerank textbookapproachtopagerankinmrorspark fromthewebhyperlinkgraphonecanconstructamatrixmthatessentially capturesthetransitionprobabilitiesm 1 fromnodejtonodei2 ij nj thepagerankcanthenbeobtainedbymultiplyinganinitialpagerankvectorby mpoweriterationpmkp 0 2fortechnicalreasonstheactualmatrixusedisslightlydifferent 30 example pagerank textbookapproachtopagerankinmrorspark fromthewebhyperlinkgraphonecanconstructamatrixmthatessentially capturesthetransitionprobabilitiesm 1 fromnodejtonodei2 ij nj thepagerankcanthenbeobtainedbymultiplyinganinitialpagerankvectorby mpoweriterationpmkp 0 onecanarguethatthealgorithmisnotimmediatelyclear alsowherearethe communicationsynchronizationbottlenecks 2fortechnicalreasonstheactualmatrixusedisslightlydifferent 30 ifk0theninitializepageranktoarandomnumberelseupdatepagerank basedonneighboursmessages pagerank sumreceivedmessages sendupdatedpageranktoeachneighbour send pageranknumber links neighbours simple butwedonthaveasmanyprocessorsasvertices vertexcentric bsp bspversionofpagerank considerthateachvertexisavirtualprocessorwhichlocallyknowsitsoutgoing edges thenforksuperstepseachvertexoperatesasfollows 31 sendupdatedpageranktoeachneighbour send pageranknumber links neighbours simple butwedonthaveasmanyprocessorsasvertices vertexcentric bsp bspversionofpagerank considerthateachvertexisavirtualprocessorwhichlocallyknowsitsoutgoing edges thenforksuperstepseachvertexoperatesasfollows ifk0theninitializepageranktoarandomnumberelseupdatepagerank basedonneighboursmessages pagerank sumreceivedmessages 31 simple butwedonthaveasmanyprocessorsasvertices vertexcentric bsp bspversionofpagerank considerthateachvertexisavirtualprocessorwhichlocallyknowsitsoutgoing edges thenforksuperstepseachvertexoperatesasfollows ifk0theninitializepageranktoarandomnumberelseupdatepagerank basedonneighboursmessages pagerank sumreceivedmessages sendupdatedpageranktoeachneighbour send pageranknumber links neighbours 31 vertexcentric bsp bspversionofpagerank considerthateachvertexisavirtualprocessorwhichlocallyknowsitsoutgoing edges thenforksuperstepseachvertexoperatesasfollows ifk0theninitializepageranktoarandomnumberelseupdatepagerank basedonneighboursmessages pagerank sumreceivedmessages sendupdatedpageranktoeachneighbour send pageranknumber links neighbours simple butwedonthaveasmanyprocessorsasvertices 31 worker1 worker2 worker3 ideafirstproposedbygoogleinthepaperpregelasystemforlargescalegraph processingsigmod2010alsoprovidesfaulttolerancemechanism opensourceimplementationbyapachegiraphalsosupportedinsparkgraphx commercialandveryefficientimplementationbygraphlablateracquiredbyapple vertexcentric bsp problem wehavemoreverticesthanreal processorsworkers solution assigneachvertextoarealprocessor worker eg byhashpartitioningor somemorecleverformofpartitioning ineachsupersteptherealworkers accumulatethemessagessentbythe verticestootherverticesand communicatethesetothecorresponding worker1 worker2 worker3 workers 32 ideafirstproposedbygoogleinthepaperpregelasystemforlargescalegraph processingsigmod2010alsoprovidesfaulttolerancemechanism opensourceimplementationbyapachegiraphalsosupportedinsparkgraphx commercialandveryefficientimplementationbygraphlablateracquiredbyapple vertexcentric bsp problem wehavemoreverticesthanreal processorsworkers solution assigneachvertextoarealprocessor worker eg byhashpartitioningor somemorecleverformofpartitioning ineachsupersteptherealworkers accumulatethemessagessentbythe verticestootherverticesand communicatethesetothecorresponding wwoorrkkeerr11 wwoorrkkeerr22 wwoorrkkeerr33 workers 32 vertexcentric bsp problem wehavemoreverticesthanreal processorsworkers solution assigneachvertextoarealprocessor worker eg byhashpartitioningor somemorecleverformofpartitioning ineachsupersteptherealworkers accumulatethemessagessentbythe verticestootherverticesand communicatethesetothecorresponding wwoorrkkeerr11 wwoorrkkeerr22 wwoorrkkeerr33 workers ideafirstproposedbygoogleinthepaperpregelasystemforlargescalegraph processingsigmod2010alsoprovidesfaulttolerancemechanism opensourceimplementationbyapachegiraphalsosupportedinsparkgraphx commercialandveryefficientimplementationbygraphlablateracquiredbyapple 32 howeveraswellseeinthenext sectiontheobtainedimplementations neednotbethemostefficientones vertexcentric bsp vertexcentricbsp themajorityofgraphalgorithmsare iterativeandtraversethegraphin someway vertexcentricbspgivesanaturalway ofexpressingthesealgorithmsina parallelfashionbythinkinglikea vertex 33 vertexcentric bsp vertexcentricbsp themajorityofgraphalgorithmsare iterativeandtraversethegraphin someway vertexcentricbspgivesanaturalway ofexpressingthesealgorithmsina parallelfashionbythinkinglikea vertex howeveraswellseeinthenext sectiontheobtainedimplementations neednotbethemostefficientones 33 1111 2111 3111 4531 4311 6431 7777 8777 8977 aaafffttteeeirrrnsssituuuippapleeeirrrdssstttseeeppp321 another example connected components connectedcomponentsinvertexbsp forundirectedgraphs initiallyeachnodehasadistinctlabelid ineachsuperstepnodescommunicate theirlabeltoneighbouringnodesand keeptheminimumoftheircurrentlabel andthelabelsinreceivedmessages keepdoingnewsuperstepsuntilnolabel changesanymore uponconvergenceeachnodeina connectedcomponenthasthesamelabel 34 111 111 111 431 311 431 777 777 877 aaafffttteeerrrsssuuupppeeerrrsssttteeeppp321 another example connected components connectedcomponentsinvertexbsp 1 2 forundirectedgraphs initiallyeachnodehasadistinctlabelid 3 ineachsuperstepnodescommunicate theirlabeltoneighbouringnodesand 5 4 keeptheminimumoftheircurrentlabel andthelabelsinreceivedmessages 6 keepdoingnewsuperstepsuntilnolabel changesanymore 7 8 9 uponconvergenceeachnodeina connectedcomponenthasthesamelabel initialids 34 111 211 311 531 411 631 777 877 977 aafftteeirrnssituuippaleeirrdssttseepp32 another example connected components connectedcomponentsinvertexbsp 1 1 forundirectedgraphs initiallyeachnodehasadistinctlabelid 1 ineachsuperstepnodescommunicate theirlabeltoneighbouringnodesand 4 3 keeptheminimumoftheircurrentlabel andthelabelsinreceivedmessages 4 keepdoingnewsuperstepsuntilnolabel changesanymore 7 7 8 uponconvergenceeachnodeina connectedcomponenthasthesamelabel aftersuperstep1 34 111 211 311 451 431 641 777 877 897 aafftteeirrnssituuipapleeirrdssttseepp31 another example connected components connectedcomponentsinvertexbsp 1 1 forundirectedgraphs initiallyeachnodehasadistinctlabelid 1 ineachsuperstepnodescommunicate theirlabeltoneighbouringnodesand 3 1 keeptheminimumoftheircurrentlabel andthelabelsinreceivedmessages 3 keepdoingnewsuperstepsuntilnolabel changesanymore 7 7 7 uponconvergenceeachnodeina connectedcomponenthasthesamelabel aftersuperstep2 34 111 211 311 453 431 643 777 877 897 aafftteeirrnssituuipapleeirrdssttseepp21 another example connected components connectedcomponentsinvertexbsp 1 1 forundirectedgraphs initiallyeachnodehasadistinctlabelid 1 ineachsuperstepnodescommunicate theirlabeltoneighbouringnodesand 1 1 keeptheminimumoftheircurrentlabel andthelabelsinreceivedmessages 1 keepdoingnewsuperstepsuntilnolabel changesanymore 7 7 7 uponconvergenceeachnodeina connectedcomponenthasthesamelabel aftersuperstep3 34 speedup scaleup parallelism example lecture 1 embarassinglyparallel example count number times word belgium appears documents web server multiple cpus read multiple disks parallel server analyze many documents parallel end sum perserver counters done fast parallelism example lecture 1 let us consider maximal aggregate bandwidth speed analyze data parallel assuming ideal data distributionoverserversdisks component max aggrbandwidth 1 hard disk 100 mbsec 1 gbps server 12 hard disks 12 gbsec 12 gbps rack 80 servers 96 gbsec 768gbps clusterdatacenter 30 racks 288 tbsec 23 tbps scanning400tbhencetakes138secs23minutes scanning400tbsequentiallyat100mbsectakes4629days parallelism example lecture 1 let us consider maximal aggregate bandwidth speed analyze data parallel assuming ideal data distributionoverserversdisks component max aggrbandwidth 1 hard disk 100 mbsec 1 gbps server 12 hard disks 12 gbsec 12 gbps rack 80 servers 96 gbsec 768gbps clusterdatacenter 30 racks 288 tbsec 23 tbps scanning400tbhencetakes138secs23minutes scanning400tbsequentiallyat100mbsectakes4629days parallelismhencegivesaspeedupof28800x parallelism example lecture 1 let us consider maximal aggregate bandwidth speed analyze data parallel assuming ideal data distributionoverserversdisks component max aggrbandwidth 1 hard disk 100 mbsec 1 gbps server 12 hard disks 12 gbsec 12 gbps rack 80 servers 96 gbsec 768gbps clusterdatacenter 30 racks 288 tbsec 23 tbps scanning400tbhencetakes138secs23minutes scanning400tbsequentiallyat100mbsectakes4629days parallelismhencegivesaspeedupof28800x itisnotacoincidencethatthenumberofparallelresourcesweareusingis 12harddisksx80serversx30racks28800 speedup definition speedupistheratioofthelatencyoftwosystemsaandbwhenrunonthe sameproblemofthesamesize 36 thisislinearspeedup adding28800timestheresourcesgetsthejobdone 28800timesquicker speedup definition speedupistheratioofthelatencyoftwosystemsaandbwhenrunonthe sameproblemofthesamesize example sequentialscanof400tbwith1harddrivetakes4629days b parallelscanof400tbwith28800harddrivestakes23minutes speedupofbwrt 28800 b 36 speedup definition speedupistheratioofthelatencyoftwosystemsaandbwhenrunonthe sameproblemofthesamesize example sequentialscanof400tbwith1harddrivetakes4629days b parallelscanof400tbwith28800harddrivestakes23minutes speedupofbwrt 28800 b thisislinearspeedup adding28800timestheresourcesgetsthejobdone 28800timesquicker 36 speedup definition speedupistheratioofthelatencyoftwosystemsaandbwhenrunonthe sameproblemofthesamesize example sequentialscanof400tbwith1harddrivetakes4629days b parallelscanof400tbwith28800harddrivestakes23minutes speedupofbwrt 28800 b thisislinearspeedup adding28800timestheresourcesgetsthejobdone 28800timesquicker bottomline moreprocessorshigherspeed linearspeedupidealbutnotalwayspossible 36 speedup definition speedupistheratioofthelatencyoftwosystemsaandbwhenrunonthe sameproblemofthesamesize 20 15 10 5 0 5 10 15 20 processorsp 36 srossecorppfopudeeps linearvsnonlinearspeedup observe 1 lim speedupp p 1α amdahls law amdahlslawisaformulathatgivesthetheoreticalspeedupinlatencyoftheexecutionofa taskatfixedworkloadthatcanbeexpectedofasystemwhoseresourcesareimproved amdahlslaw 1 speedupp 1α α p pdenotesthenumberofparallel processors αdenotesthefractionofthetaskthat benefitsfromparallelism 1αdenotesthefractionofthetaskthat runsinherentlysequential 37 amdahls law amdahlslawisaformulathatgivesthetheoreticalspeedupinlatencyoftheexecutionofa taskatfixedworkloadthatcanbeexpectedofasystemwhoseresourcesareimproved amdahlslaw 1 speedupp 1α α p pdenotesthenumberofparallel processors αdenotesthefractionofthetaskthat benefitsfromparallelism 1αdenotesthefractionofthetaskthat runsinherentlysequential observe 1 lim speedupp p 1α 37 thisissublinearscaleup werequire22timesmoreresourcestoperform2 timestheamountofwork example in10minutes10serversprocess100tbofdata b in10minutes22serversprocess200tbofdata 200 scaleupofbwrt 2 100 bottomline moreprocessorscanprocessmoredatainthesametime linearscaleupidealbutnotalwayspossible scaleup definition scalabilityisthecapacityofasystemtohandleagrowingamountofworkby addingagrowingamountofresources scaleupistheratiobetweentheamountofworkthattwosystemsaandb processwhenrunningforthesameamountoftime 38 thisissublinearscaleup werequire22timesmoreresourcestoperform2 timestheamountofwork bottomline moreprocessorscanprocessmoredatainthesametime linearscaleupidealbutnotalwayspossible scaleup definition scalabilityisthecapacityofasystemtohandleagrowingamountofworkby addingagrowingamountofresources scaleupistheratiobetweentheamountofworkthattwosystemsaandb processwhenrunningforthesameamountoftime example in10minutes10serversprocess100tbofdata b in10minutes22serversprocess200tbofdata 200 scaleupofbwrt 2 100 38 bottomline moreprocessorscanprocessmoredatainthesametime linearscaleupidealbutnotalwayspossible scaleup definition scalabilityisthecapacityofasystemtohandleagrowingamountofworkby addingagrowingamountofresources scaleupistheratiobetweentheamountofworkthattwosystemsaandb processwhenrunningforthesameamountoftime example in10minutes10serversprocess100tbofdata b in10minutes22serversprocess200tbofdata 200 scaleupofbwrt 2 100 thisissublinearscaleup werequire22timesmoreresourcestoperform2 timestheamountofwork 38 scaleup definition scalabilityisthecapacityofasystemtohandleagrowingamountofworkby addingagrowingamountofresources scaleupistheratiobetweentheamountofworkthattwosystemsaandb processwhenrunningforthesameamountoftime example in10minutes10serversprocess100tbofdata b in10minutes22serversprocess200tbofdata 200 scaleupofbwrt 2 100 thisissublinearscaleup werequire22timesmoreresourcestoperform2 timestheamountofwork bottomline moreprocessorscanprocessmoredatainthesametime linearscaleupidealbutnotalwayspossible 38 gustafsonbarsiss law 200 150 100 50 0 0 50 100 150 200 p 39 ppuelacs thegustafsonbarsislawisaformulathatgivesthetheoreticalscaleupthatcanbe expectedofasystemwhoseresourcesareimproved gustafsonbarsislaw gustafsonbarsisslaw 95 scaleupp1ααp 90 75 50 pdenotesthenumberofparallel processors αdenotesthefractionofthetaskthat benefitsfromparallelism 1αdenotesthefractionofthetaskthat runsinherentlysequential inotherwords anembarrasinglyparallelproblemisaproblemwhereα100iethereis littleornoinherentsequentialcomputationinvolvedeg countoccurrencesof belgium inotherwords whenphrasedasabspalgorithmthenumberofsuperstepssisfixeddoes notdependonproblemsizeandthetotalworkwandtotalcommunicationh areinverselyproportionaltothenumberofprocessorsp embarassing parallelism definition parallel computing embarrassinglyparallel workload problem also calledperfectlyparallelorpleasinglyparallelisonewherelittleornoeffortis neededtoseparatetheproblemintoanumberofparalleltasks thisisoftenthecasewherethereislittleornodependencyorneedforcom municationbetweenthoseparalleltasksorforresultsbetweenthem wikipedia 40 inotherwords whenphrasedasabspalgorithmthenumberofsuperstepssisfixeddoes notdependonproblemsizeandthetotalworkwandtotalcommunicationh areinverselyproportionaltothenumberofprocessorsp embarassing parallelism definition parallel computing embarrassinglyparallel workload problem also calledperfectlyparallelorpleasinglyparallelisonewherelittleornoeffortis neededtoseparatetheproblemintoanumberofparalleltasks thisisoftenthecasewherethereislittleornodependencyorneedforcom municationbetweenthoseparalleltasksorforresultsbetweenthem wikipedia inotherwords anembarrasinglyparallelproblemisaproblemwhereα100iethereis littleornoinherentsequentialcomputationinvolvedeg countoccurrencesof belgium 40 embarassing parallelism definition parallel computing embarrassinglyparallel workload problem also calledperfectlyparallelorpleasinglyparallelisonewherelittleornoeffortis neededtoseparatetheproblemintoanumberofparalleltasks thisisoftenthecasewherethereislittleornodependencyorneedforcom municationbetweenthoseparalleltasksorforresultsbetweenthem wikipedia inotherwords anembarrasinglyparallelproblemisaproblemwhereα100iethereis littleornoinherentsequentialcomputationinvolvedeg countoccurrencesof belgium inotherwords whenphrasedasabspalgorithmthenumberofsuperstepssisfixeddoes notdependonproblemsizeandthetotalworkwandtotalcommunicationh areinverselyproportionaltothenumberofprocessorsp 40 problems problemsongraphsaretypicalexamplesofproblemswhereparallelism canhelpbutwhicharenotembarassinglyparallel 41 scalability cost believe could say nobody ever got fired using hadoop cluster norm data analytics run commodityclusterswithprogrammingframeworkssuchas mr spark rowstron et al hotcdp 2012 43 norm data analytics run commodityclusterswithprogrammingframeworkssuchas mr spark believe could say nobody ever got fired using hadoop cluster rowstron et al hotcdp 2012 43 costbenefitanalysisrequired butrunningcomputationonarented clustercostsmoney nobodylikestheirawsazuregcp bill didyoureallyneedthis infrastructureforyourcomputation caseinpoint graphanalyticsnext word caution runninghadoopsparkisregarded asthecoolthingtodo clustersasaservicehasbecome extremelyeasy 44 caseinpoint graphanalyticsnext word caution runninghadoopsparkisregarded asthecoolthingtodo clustersasaservicehasbecome extremelyeasy costbenefitanalysisrequired butrunningcomputationonarented clustercostsmoney nobodylikestheirawsazuregcp bill didyoureallyneedthis infrastructureforyourcomputation 44 word caution runninghadoopsparkisregarded asthecoolthingtodo clustersasaservicehasbecome extremelyeasy costbenefitanalysisrequired butrunningcomputationonarented clustercostsmoney nobodylikestheirawsazuregcp bill didyoureallyneedthis infrastructureforyourcomputation caseinpoint graphanalyticsnext 44 quiz speedupofadataparallelalgorithmbeforeaandafterbachangewasmade tothealgorithm whichsystemwouldyouprefer figuresourcemcsherryetalscalabilitybutatwhatcosthotosworkshop2015 45 bottomline speedupandscaleupdontmeananythingiftheyarebecauseofsystem inefficienciesoverheadsthatareparallelizable speedup isnt everything speedupandtotalruntimeofadataparallelalgorithmbeforeaandafterba changewasmadetothealgorithm whichsystemwouldyouprefer figuresourcemcsherryetalscalabilitybutatwhatcosthotosworkshop2015 46 speedup isnt everything speedupandtotalruntimeofadataparallelalgorithmbeforeaandafterba changewasmadetothealgorithm whichsystemwouldyouprefer bottomline speedupandscaleupdontmeananythingiftheyarebecauseofsystem inefficienciesoverheadsthatareparallelizable figuresourcemcsherryetalscalabilitybutatwhatcosthotosworkshop2015 46 reportedelapsedtimesfor20pagerankiterations reportedelapsedtimesforconnectedcomponents comparedtoanimprovedsinglethreadedimple rbeapsoerdteodnellaabpeslepdrtoipmaegsatfioorn20copmagpearraendktoitearastiniognles mentationtraversingedgesindifferentorder cthormeapdaereddimtopalesminegnlteattihorneandoetduimsinpglelmabeenltaptrioopnaga tion parallelism isnt everything twographdatasets frommcsherryetalscalabilitybutatwhatcosthotosworkshop2015 47 reportedelapsedtimesfor20pagerankiterations reportedelapsedtimesforconnectedcomponents comparedtoanimprovedsinglethreadedimple basedonlabelpropagationcomparedtoasingle mentationtraversingedgesindifferentorder threadedimplementationnotusinglabelpropaga tion parallelism isnt everything twographdatasets reportedelapsedtimesfor20pagerankiterations comparedtoasinglethreadedimplementation frommcsherryetalscalabilitybutatwhatcosthotosworkshop2015 47 reportedelapsedtimesforconnectedcomponents rbeapsoerdteodnellaabpeslepdrtoipmaegsatfioorn20copmagpearraendktoitearastiniognles cthormeapdaereddimtopalesminegnlteattihorneandoetduimsinpglelmabeenltaptrioopnaga tion parallelism isnt everything twographdatasets reportedelapsedtimesfor20pagerankiterations comparedtoanimprovedsinglethreadedimple mentationtraversingedgesindifferentorder frommcsherryetalscalabilitybutatwhatcosthotosworkshop2015 47 reportedelapsedtimesfor20pagerankiterations comparedtoanimprovedsinglethreadedimple reportedelapsedtimesfor20pagerankiterations mentationtraversingedgesindifferentorder comparedtoasinglethreadedimplementation parallelism isnt everything twographdatasets reportedelapsedtimesforconnectedcomponents basedonlabelpropagationcomparedtoasingle threadedimplementationnotusinglabelpropaga tion frommcsherryetalscalabilitybutatwhatcosthotosworkshop2015 47 cost definition definethecostofasystemonagiven workloaddatasettobetheconfiguration atwhichitoutperformsasinglethreaded optimizedalgorithm thecosthencequantifieswhenitbecomesusefultouseadistributedprocessing systemforagivenworkload ifthecostishighasinglethreadedimplementationmayactuallybemore preferablefromaneconomicviewpoint somesystemshaveunboundedcostoncertaingraphrelatedproblems checkoutthefulltalkaboutthecostpaperat httpswwwyoutubecomwatchv6bwbejbmng0 48 conclusion bigdataprogrammingframeworkssuchasmrandsparkcanallowanalysisofhuge datasets theseframeworksintroducetheirownoverheadsavoidnetworkcommunicationif possible thebspmodelisnaturaltheoreticalmodelfortheformulationandanalysisof distributedparallelalgorithms therearelimitstospeedupscaleupismorefavorable bigdataprogrammingframworksaregreatforembarassinglyparallelproblems forproblemsthataremoredifficulttoparallelizeeggraphproblemsitmaynotalways makesensetousedistributedprocessingagoodcentralalgorithmcangoalongway 49 references robhbisseling parallelscientificcomputation astructuredapproachusing bspandmpioxforduniversitypressmarch2004 peterzecevićandmarkobonaći sparkinactionmanning2017 leslievaliant abridgingmodelforparallelcomputationcommunicationsof theacm3381990 malewiczetal pregel asystemforlargescalegraphprocessingsigmod conference2010 frankmcsherrymichaelisardderekgmurray scalability butatwhatcost hotosworkshop2015 50 questions 51 acknowledgements basedoncontentcreatedbystijnvansummeren 52

cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information chapter 1 scaling machine learning introduction ron bekkerman mikhail bilenko john langford distributedandparallelprocessingofverylargedatasetshasbeenemployedfordecades inspecializedhighbudgetsettingssuchasfinancialandpetroleumindustryapplica tionsrecentyearshavebroughtdramaticprogressinusabilitycosteffectivenessand diversityofparallelcomputingplatformswiththeirpopularitygrowingforabroadset ofdataanalysisandmachinelearningtasks current rise interest scaling machine learning applications partiallyattributedtotheevolutionofhardwarearchitecturesandprogrammingframe worksthatmakeiteasytoexploitthetypesofparallelismrealizableinmanylearning algorithms number platforms make convenient implement concurrent pro cessingofdatainstancesortheirfeaturesthisallowsfairlystraightforwardparalleliza tion many learning algorithms view input unordered batch examples andaggregateisolatedcomputationsovereachofthem increasedattentiontolargescalemachinelearningisalsoduetothespreadofvery largedatasetsacrossmanymodernapplicationssuchdatasetsareoftenaccumulated distributed storage platforms motivating development learning algorithms thatcanbedistributedappropriatelyfinallytheproliferationofsensingdevicesthat perform realtime inference based highdimensional complex feature representa tionsdrivesadditionaldemandforutilizingparallelisminlearningcentricapplications examplesofthistrendincludespeechrecognitionandvisualobjectdetectionbecoming commonplaceinautonomousrobotsandmobiledevices theabundanceofdistributedplatformchoicesprovidesanumberofoptionsforim plementingmachinelearningalgorithmstoobtainefficiencygainsorthecapabilityto processverylargedatasetstheseoptionsincludecustomizableintegratedcircuitseg fieldprogrammable gate arrays fpgas custom processing units eg general purposegraphicsprocessingunitsgpusmultiprocessorandmulticoreparallelism highperformance computing hpc clusters connected fast local networks datacenterscalevirtualclustersthatcanberentedfromcommercialcloudcomputing providersasidefromthemultipleplatformoptionsthereexistsavarietyofprogram ming frameworks algorithms implemented framework choices tend 1 web service cambridge university press wwwcambridgeorg cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information 2 1 scalingupmachinelearningintroduction particularly diverse distributed architectures clusters commodity pcs wide range platforms frameworks parallel distributed comput ing presents opportunities challenges machine learning scientists engineers fully exploiting available hardware resources requires adapting algorithms redesigning others enable concurrent execution pre diction model learning algorithm structure dataflow underlying task decomposition must taken account determine suitability particular infrastructurechoice chaptersmakingupthisvolumeformarepresentativesetofstateoftheartsolutions span space modern parallel computing platforms frameworks varietyofmachinelearningalgorithmstasksandapplicationsalthoughitisinfeasible cover every existing approach every platform believe presented set techniques covers commonly used methods including popular top performers eg boosted decision trees support vector machines common baselinesegkmeansclustering becausemostchaptersfocusonasinglechoiceofplatformandorframeworkthe rest introduction provides reader unifying context brief overview ofmachinelearningbasicsandfundamentalconceptsinparallelanddistributedcom puting summary typical task application scenarios require scaling learning thoughts evaluating algorithm performance platform tradeoffs followingtheseareanoverviewofthechaptersandbibliographynotes 11 machinelearningbasics machine learning focuses constructing algorithms making predictions data machine learning task aims identify learn function fx maps input domain x data onto output domain possible predictions function f isselectedfromacertainfunctionclasswhichisdifferentforeachfamily learning algorithms elements x applicationspecific representations ofdataobjectsandpredictionsrespectively twocanonicalmachinelearningsettingsaresupervisedlearningandunsupervised learningsupervisedlearningalgorithmsutilizetrainingdatatoconstructaprediction function fwhichissubsequentlyappliedtotest instancestypicallytrainingdatais provided form labeled examples xyx x data instance andyisthecorrespondinggroundtruthpredictionforx ultimate goal supervised learning identify function f produces accuratepredictionsontestdatamoreformallythegoalistominimizetheprediction errorlossfunctionl rwhichquantifiesthedifferencebetweenany fx andythepredictedoutputofx anditsgroundtruthlabelhoweverthelosscannot minimized directly test instances labels typically unavailable training time instead supervised learning algorithms aim construct predictive functions generalize well previously unseen data opposed performingoptimallyjustonthegiventrainingsetthatisoverfittingthetrainingdata themostcommonsupervisedlearningsettingisinductionwhereitisassumedthat eachtrainingandtestexamplexyissampledfromsomeunknownjointprobability web service cambridge university press wwwcambridgeorg cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information 12 reasonsforscalingupmachinelearning 3 distribution p x objective find f minimizes expected loss e xyp lfxybecausethejointdistributionpisunknownexpectedlosscannot minimized closed form hence learning algorithms approximate based training examples additional supervised learning settings include semisupervised learning input data consists labeled unlabeled instances transferlearningandonlinelearningseesection163 twoclassicsupervisedlearningtasksareclassificationandregressioninclassifica tiontheoutputdomainisafinitediscretesetofcategoriesclassesy c c 1 k whereas regression output domain set real numbers r complex output domains explored within advanced learning frameworks structuredlearningbakiretal2007 simplest classification scenario binary two classes let us consider small example assume task learn function predicts whetheranincomingemailmessageisspamornotacommonwaytorepresenttextual messagesisaslargesparsevectorsinwhicheveryentrycorrespondstoavocabulary wordandnonzeroentriesrepresentwordsthatarepresentinthemessagethelabel represented 1 spam 1 nonspam thiscid2cid3represencid4tation common learn vector weights w optimizing fxsign wx predictthelabel prominent example unsupervised learning data clustering clus tering goal construct function f partitions unlabeled dataset k yclusterswithy beingthesetofclusterindicesdatainstancesassignedtothe cluster presumably similar data instances assigned cluster many ways define similarity data instancesforexampleforvectordatainvertedeuclideandistanceandcosinesimi larityarecommonlyusedclusteringqualityisoftenmeasuredagainstadatasetwith existingclasslabelsthatarewithheldduringclusteringaqualitymeasurepenalizes f ifitassignsinstancesofthesameclasstodifferentclustersandinstancesofdifferent classestothesamecluster wenotethatbothsupervisedandunsupervisedlearningsettingsdistinguishbetween learning inference tasks learning refers process identifying prediction function f inference refers computing fx data instance x many learning algorithms inference component learning process predictions interim candidate fcid3 training data used search optimal f depending application domain scaling may required either learning inference algorithm chapters book present numerousexamplesofspeedingupboth 12 reasonsforscalingupmachinelearning number settings practitioner could find scale chine learning task daunting singlemachine processing consider employing parallelizationsuchsettingsarecharacterizedby 1 largenumberofdatainstancesinmanydomainsthenumberofpotentialtraining examplesisextremelylargemakingsinglemachineprocessinginfeasible web service cambridge university press wwwcambridgeorg cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information 4 1 scalingupmachinelearningintroduction 2 highinputdimensionalityinsomeapplicationsdatainstancesarerepresentedbya verylargenumberoffeaturesmachinelearningalgorithmsmaypartitioncomputation acrossthesetoffeatureswhichallowsscalinguptolengthydatarepresentations 3 model algorithm complexity number highaccuracy learning algorithms eitherrelyoncomplexnonlinearmodelsoremploycomputationallyexpensivesubrou tines cases distributing computation across multiple processing units bethekeyenablerforlearningonlargedatasets 4 inferencetimeconstraintsapplicationsthatinvolvesensingsuchasrobotnavigation orspeechrecognitionrequirepredictionstobemadeinrealtimetightconstraintson inferencespeedinsuchsettingsinviteparallelizationofinferencealgorithms 5 prediction cascades applications require sequential interdependent predictions havehighlycomplexjointoutputspacesandparallelizationcansignificantlyspeedup inferenceinsuchsettings 6 model selection parameter sweeps tuning hyperparameters learning algo rithmsandstatisticalsignificanceevaluationrequiremultipleexecutionsoflearningand inferencefortunatelytheseproceduresbelongtothecategoryofsocalledembarrass inglyparallelizableapplicationsnaturallysuitedforconcurrentexecution thefollowingsectionsdiscusseachofthesescenariosinmoredetail 121 largenumberofdatainstances datasets thataggregatebillions eventsperday havebecomecommon inanumber domains internet finance event potential input learningalgorithmalsomoreandmoredevicesincludesensorscontinuouslylogging observationsthatcanserveastrainingdataeachdatainstancemayhaveforexample thousandsofnonzerofeaturesonaverageresultingindatasetsof1012instancefeature pairs per day even feature takes 1 byte store datasets collected timecaneasilyreachhundredsofterabytes thepreferredwaytoeffectivelyprocesssuchdatasetsistocombinethedistributed storage bandwidth cluster machines several computational frameworks haverecentlyemergedtoeasetheuseoflargequantitiesofdatasuchasmapreduce anddryadlinqusedinseveralchaptersinthisbooksuchframeworkscombinethe ability use highcapacity storage execution platforms programming via simplenaturallyparallelizablelanguageprimitives 122 highinputdimensionality machinelearninganddataminingtasksinvolvingnaturallanguageimagesorvideo easily input dimensionality 106 higher far exceeding comfortable scaleof101000featuresconsideredcommonuntilrecentlyalthoughdatainsome domains sparse always case sparsity also lost parameterspaceofmanyalgorithmsparallelizingthecomputationacrossfeaturescan thusbeanattractivepathwayforscalingupcomputationtoricherrepresentations speeding algorithms naturally iterate features decision trees web service cambridge university press wwwcambridgeorg cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information 12 reasonsforscalingupmachinelearning 5 123 modelandalgorithmcomplexity datainsomedomainshasinherentlynonlinearstructurewithrespecttothebasicfea turesegpixelsorwordsmodelsthatemployhighlynonlinearrepresentationssuch asdecisiontreeensemblesormultilayerdeepnetworkscansignificantlyoutperform simpler algorithms applications although feature engineering yield high accuracieswithcomputationallycheaplinearmodelsinthesedomainsthereisagrow ing interest learning automatically possible base representation commoncharacteristicofalgorithmsthatattemptthisistheirsubstantialcomputational complexityalthoughthetrainingdatamayeasilyfitononemachinethelearningpro cessmaysimplybetooslowforareasonabledevelopmentcyclethisisalsothecase forsomelearningalgorithmsthecomputationalcomplexityofwhichissuperlinearin thenumberoftrainingexamples forproblemsofthisnatureparallelmultinodeormulticoreimplementationsappear viableandhavebeenemployedsuccessfullyallowingtheuseofcomplexalgorithms andmodelsforlargerdatasetsinadditioncoprocessorssuchasgpushavealsobeen employedsuccessfullyforfasttransformationoftheoriginalinputspace 124 inferencetimeconstraints theprimarymeansforreducingthetestingtimeisviaembarrassinglyparallelreplica tionthisapproachworkswellforsettingswherethroughputistheprimaryconcern thenumberofevaluationstobedoneisverylargeconsiderforexampleevaluating 1010emailsperdayinaspamfilterwhichisnotexpectedtooutputresultsinrealtime yetmustnotbecomebacklogged inference latency generally stringent concern compared throughput latency issues arise situation systems waiting prediction overall application performance degrades rapidly latency instance occursforacardrivingrobotmakingpathplanningdecisionsbasedonseveralsensors oranonlinenewsproviderthataimstoimproveuserexperiencebyselectingsuggested storiesusingontheflypersonalization constraints throughput latency entirely compatible example data pipelining trades throughput latency however utilizing highly parallelized hardware architectures gpus fpgas found effective 125 predictioncascades many realworld problems object tracking speech recognition machine translationrequireperformingasequenceofinterdependentpredictionsformingpre diction cascades cascade viewed single inference task large joint output space typically resulting high computational costs due creasedcomputationalcomplexityinterdependenciesbetweenthepredictiontasksare typicallytackledbystagewiseparallelizationofindividualtasksalongwithadaptive taskmanagementasillustratedbytheapproachofchapter21tospeechrecognition web service cambridge university press wwwcambridgeorg cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information 6 1 scalingupmachinelearningintroduction 126 modelselectionandparametersweeps thepracticeofdevelopingtuningandevaluatinglearningalgorithmsreliesonwork flow embarrassingly parallel requires intercommunication tasks independent executions dataset two particular processes nature parameter sweeps statistical significance testing parameter sweeps learning algorithm run multiple times dataset differ entsettingsfollowedbyevaluationonavalidationsetduringstatisticalsignificance testingproceduressuchascrossvalidationorbootstrappingtrainingandtestingisper formedrepeatedlyondifferentdatasetsubsetswithresultsaggregatedforsubsequent measurementofstatisticalsignificanceusefulnessofparallelplatformsisobviousfor thesetasksastheycanbeeasilyperformedconcurrentlywithouttheneedtoparallelize actuallearningandinferencealgorithms 13 keyconceptsinparallelanddistributedcomputing performance gains attainable machine learning applications employing parallel anddistributedsystemsaredrivenbyconcurrentexecutionoftasksthatareotherwise performed serially two major directions concurrency real ized data parallelism task parallelism data parallelism refers simultaneous processing multiple inputs whereas task parallelism achieved algorithm executioncanbepartitionedintosegmentssomeofwhichareindependentandhence canbeexecutedconcurrently 131 dataparallelism dataparallelismreferstoexecutingthesamecomputationonmultipleinputsconcur rently natural fit many machine learning applications algorithms accept input data batch independent samples underlying distribution representation samples via instancebyfeature matrix naturally suggests twoorthogonaldirectionsforachievingdataparallelismoneispartitioningthematrix rowwise subsets instances processed independently eg computing update weights logistic regression splitting columnwiseforalgorithmsthatcandecouplethecomputationacrossfeaturesegfor identifyingthesplitfeatureindecisiontreeconstruction basic example data parallelism encountered embarrassingly par allelalgorithmswherethecomputationissplitintoconcurrentsubtasksrequiringno intercommunicationwhichrunindependentlyonseparatedatasubsetsarelatedsim pleimplementationofdataparallelismoccurswithinthemasterslavecommunication model master process distributes data across slave processes execute samecomputationseeegchapters8and16 less obvious cases data parallelism arise algorithms instances fea turesarenotindependentbutthereexistsawelldefinedrelationalstructurebetween represented graph data parallelism achieved computationcanbepartitionedacrossinstancesbasedonthisstructurethenconcur rentexecutionondifferentpartitionsisinterlacedwithexchangeofinformationacross themapproachespresentedinchapters10and15relyonthisalgorithmicpattern web service cambridge university press wwwcambridgeorg cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information 14 platformchoicesandtradeoffs 7 foregoing examples illustrate coarsegrained data parallelism subsets instancesorfeaturesthatcanbeachievedviaalgorithmdesignfinegraineddataparal lelismincontrastreferstoexploitingthecapabilityofmodernprocessorarchitectures thatallowparallelizingvectorandmatrixcomputationsinhardwarestandardlibraries suchasblasandlapack1 provideroutinesthatabstractouttheexecutionofbasic vectorandmatrixoperationslearningalgorithmsthatcanberepresentedascascades operations leverage hardwaresupported parallelism making correspondingapicallsdramaticallysimplifyingthealgorithmsimplementation 132 taskparallelism unlikedataparallelismdefinedbyperformingthesamecomputationonmultipleinputs simultaneouslytaskparallelismreferstosegmentingtheoverallalgorithmintoparts someofwhichcanbeexecutedconcurrentlyfinegrainedtaskparallelismfornumeri calcomputationscanbeperformedautomaticallybymanymodernarchitectureseg viapipeliningbutcanalsobeimplementedsemimanuallyoncertainplatformssuchas gpuspotentiallyresultinginverysignificantefficiencygainsbutrequiringindepth platform expertise coarsegrained task parallelism requires explicit encapsulation eachtaskinthealgorithmsimplementationaswellasaschedulingservicewhichis typicallyprovidedbyaprogrammingframework thepartitioningofanalgorithmintotaskscanberepresentedbyadirectedacyclic graphwithnodescorrespondingtoindividualtasksandedgesrepresentingintertask dependenciesdataflowbetweentasksoccursnaturallyalongthegraphedgesapromi nentexampleofsuchaplatformismapreduceaprogrammingmodelfordistributed computation introduced dean ghemawat 2004 several chapters book rely see chapter 2 details additional crosstask communica tion supported platforms via pointtopoint broadcast messaging message passing interface mpi introduced gropp et al 1994 example suchmessagingprotocolthatiswidelysupportedacrossmanyplatformsandprogram minglanguagesseveralchaptersinthisbookrelyonitseesection44ofchapter4 details besides wide availability mpis popularity due flexibility supports pointtopoint collective communication synchronous asynchronousmechanisms many algorithms scaling efficiently achieved mixture dataandtaskparallelismcapabilityforhybridparallelismisrealizedbymostmodern platforms example exhibited highly distributed dryadlinq frameworkdescribedinchapter3andbycomputervisionalgorithmsimplementedon gpusandcustomizedhardwareasdescribedinchapters18and19 14 platformchoicesandtradeoffs letusbrieflysummarizethekeydimensionsalongwhichparallelanddistributedplat forms characterized classic taxonomy parallel architectures proposed 1 httpwwwnetliborgblasandhttpwwwnetliborglapack web service cambridge university press wwwcambridgeorg cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information 8 1 scalingupmachinelearningintroduction byflynn1972differentiatesthembyconcurrencyofalgorithmexecutionsinglevs multiple instruction input processing single vs multiple data streams distinctionscanbemadebasedontheconfigurationofsharedmemoryandtheorgani zationofprocessingunitsmodernparallelarchitecturesaretypicallybasedonhybrid topologieswhereprocessingunitsareorganizedhierarchicallywithmultiplelayersof sharedmemoryforexamplegpustypicallyhavedozensofmultiprocessorseachof multiple stream processors organized blocks individual blocks access relatively small locally shared memory much larger globally shared memorywithhigherlatency unlikeparallelarchitecturesdistributedcomputingplatformstypicallyhavelarger physicaldistancesbetweenprocessingunitsresultinginhigherlatenciesandlower bandwidthfurthermoreindividualprocessingunitsmaybeheterogeneousanddirect communicationbetweenthemmaybelimitedornonexistenteitherviasharedmemory orviamessagepassingwiththeextremecasebeingonewherealldataflowislimited totaskboundariesasisthecaseformapreduce overall variety parallel distributed platforms frameworks available machine learning applications may seem overwhelming ever following observations capture key differentiating aspects platforms cid2 parallelismgranularityemployinghardwarespecificsolutionsgpusandfpgas allows finegrained data task parallelism elementary numerical tasks operationsonvectorsmatricesandtensorscanbespreadacrossmultipleprocessing unitswithveryhighthroughputachievedbypipelininghoweverusingthiscapability requires redefining entire algorithm dataflow elementary tasks eliminatingbottlenecksmovinguptoparallelismacrosscoresandprocessorsingeneric cpus constraints defining algorithm sequence finely tuned stages relaxed parallelism longer limited elementary numeric operations withclusteranddatacenterscalesolutionsdefininghighergranularitytasksbecomes imperativebecauseofincreasingcommunicationcosts cid2 degree algorithm customization depending platform choice complex ity algorithm redesign required enabling concurrency may vary simply using thirdparty solution automatic parallelization existing imperative declarativestyle implementation completely recreate algorithm even implement directly hardware generally implementing learning algo rithms hardwarespecific platforms eg gpus requires significant expertise hardwareaware task configuration avoiding certain commonplace software pat ternssuchasbranchingincontrasthigherlevelparallelanddistributedsystemsallow using multiple commonplace programming languages extended apis enable parallelism cid2 abilitytomixprogrammingparadigmsdeclarativeprogramminglanguagesarebe comingincreasinglypopularforlargescaledatamanipulationborrowingfromavariety predecessors functional programming sql make parallel program ming easier expressing algorithms primarily mixture logic dataflow languages often hybridized classic imperative programming pro videmaximumexpressivenessexamplesofthistrendincludemicrosoftsdryadlinq web service cambridge university press wwwcambridgeorg cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information 15 thinkingaboutperformance 9 googles sawzall pregel apache pig hive even applications suchdeclarativestylelanguagesareinsufficientforexpressingthelearningalgorithms theyareoftenusedforcomputingthebasicfirstandsecondorderstatisticsthatproduce highlypredictivefeaturesformanylearningtasks cid2 dataset scaleout applications process datasets large fit memory com monly rely distributed filesystems sharedmemory clusters parallel comput ing frameworks tightly coupled distributed dataset storage allow op timizing task allocation scheduling maximize local dataflows contrast scheduling hardwarespecific parallelism decoupled storage solutions used large datasets hence requires crafting manual solutions maximize throughput cid2 offline vs online execution distributed platforms typically assume user higher tolerance failures latency compared hardwarespecific solutions example algorithm implemented via mapreduce submitted virtual cluster typically guarantees completion time contrast gpubased algo rithmscanassumededicateduseoftheplatformwhichmaybepreferableforrealtime applications finally note growing trend hybridization mul tipleparallelizationlevelsforexampleitisnowpossibletorentclusterscomprising multicore nodes attached gpus commercial cloud computing providers given particular application hand choice platform programming framework guided criteria given identify appropriate solution 15 thinkingaboutperformance term performance deeply ambiguous parallel learning algorithms includes predictive accuracy computational speed measured number metrics variety learning problems addressed chaptersofthisbookmakesthepresentedapproachesgenerallyincomparableinterms ofpredictiveperformancethealgorithmsaredesignedtooptimizedifferentobjectives indifferentsettingseveninthosecaseswherethesameproblemisaddressedsuchas binary classification clustering differences application domains evaluation methodology typically lead incomparability accuracy results consequence possible provide meaningful quantitative summary relative accuracy across chapters book although understood every casethattheauthorsstrovetocreateeffectivealgorithms classicalanalysisofalgorithmscomplexityisbasedononotationoritsbrethren toboundandquantifycomputationalcoststhisapproachmeetsdifficultieswithmany machine learning algorithms often include optimizationbased termination conditions formal analysis exists example typical early stopping algorithmmayterminatewhenpredictiveerrormeasuredonaholdouttestsetbegins torisesomethingthatisdifficulttoanalyzebecausethecorealgorithmdoesnothave accesstothistestsetbydesign web service cambridge university press wwwcambridgeorg cambridge university press 9780521192248 scaling machine learning parallel distributed approaches edited ron bekkerman mikhail bilenko john langford excerpt information 10 1 scalingupmachinelearningintroduction neverthelessindividualsubroutineswithinlearningalgorithmsdooftenhaveclear computationalcomplexitieswhenexaminingalgorithmsandconsideringtheirappli cationtoagivendomainwesuggestaskingthefollowingquestions 1 whatisthecomputationalcomplexityofthealgorithmorofitssubroutineisitlinear ieoinputsizeorsuperlinearingeneralthereisaqualitativedifferencebetween algorithmsscalingasoinputsizeandothersscalingasoinputsizeαforα 2for allpracticalpurposesalgorithmswithcubicandhighercomplexitiesarenotapplicable torealworldtasksofthemodernscale 2 whatisthebandwidthrequirementforthealgorithmthisisparticularlyimportantfor algorithm distributed cluster computers also relevant parallel algorithmsthatusesharedmemoryordiskresourcesthisquestioncomesintwoflavors aggregate bandwidth used maximum bandwidth nodeanswersoftheformoinputsizeoinstancesandoparameterscanallarise naturally depending data organized algorithm proceeds answers substantial impact running time input dataset may besay1014 bytesinsizeyethaveonly1010examplesand108parameters key metrics used analyzing computational performance parallel algorithms arespeedupefficiencyandscalability cid2 speedup ratio solution time sequential algorithms versus parallel counterpart cid2 efficiencymeasurestheratioofspeeduptothenumberofprocessors cid2 scalabilitytracksefficiencyasafunctionofanincreasingnumberofprocessors forreasonsexplainedearlierthesemeasurescanbenontrivialtoevaluateanalytically machine learning algorithms generally considered conjunction withaccuracycomparisonshoweverthesemeasuresarehighlyinformativeinempir icalstudiesfromapracticalstandpointgiventhedifferencesinhardwareemployed forparallelandsequentialimplementationsviewingthesemetricsasfunctionsofcosts hardwareandimplementationisimportantforfaircomparisons empiricalevaluationofcomputationalcostsfordifferentalgorithmsshouldbeide ally performed comparing datasets predictive perfor mance may done work presented subsequent chapters given dramatic differences tasks application domains underlying frameworks implementations different methods however possible consider general feature throughput methods presented different chapters defined runningtime basedontheresultsreportedacrosschapterswelldesignedparallelized inputsize methods capable obtaining high efficiency across different platforms tasks 16 organizationofthebook chaptersinthisbookspanarangeofcomputingplatformslearningalgorithmspre diction problems application domains describing variety parallelization techniques scale machine learning book organized four parts web service cambridge university press wwwcambridgeorg

introduction deep learning yannaël le borgne cecoia mlg ulb 14th april 2025 outline introduction perceptron multilayer perceptron transformers embeddings attention training data llm ecosystem trends challenges 2 lets start small quiz httpsappwooclapcomeventsppumtkvotes 3 2025 deep learning everywhere two nobel prizes 2024 passes turing test 100m users two months 4 deep learning start google trend deep learning 5 deep learning neural networks deep learning systems google trend neural networks red vs deep learning blue neural networks exist since 70s even rebranding 6 deep learning neural networks deep learning systems neural networks exist since 70s even rebranding neural networks started work thanks data better hardware 7 deep learning 8 deep learning httpsintrotodeeplearningcom 9 demo face recognition httpsenvittasciencecomia 10 main components deep learning keras data computing power libraries 11 landmarks 2012 2016 2018 2020 2022 2024 2015 2017 2019 2021 2023 12 2017 transformers gpt bert t5 paper 117 million parameters 2018 gpt1 groundbreaking 15 billion parameters 2019 gpt2 contribution field ai recent 2020 gpt3 175 billion parameters years 175 billion parameters 2022 gpt35 chatgpt 1t parameters 2023 gpt4 13 perceptron 14 basic computing unit neuron example perceptron perceptron model rosenblatt 1957 neuron model integrates activation values previous layer means wx linear separator scalar product f activation function unit step function 15 activation functions activation functions f introduces nonlinearity examples common differentiable activation functions tanh sigmoid rectified linear relu 16 learning define training set loss function training set n examples input x output loss function example squared loss regression 17 training gradient descent forward backward pass repeat forward pass compute output perceptron backward pass gradient update compute gradient squared loss update parameters demo tensorflow playground 18 learning rate multilayer perceptron 19 multilayer perceptron fully connected layers 20 fully connected layers forward backward pass repeat forward pass compute output backward pass compute gradient update parameters rumelhart david e hinton geoffrey e williams ronald j 1986 learning representations backpropagating errors nature 323 6088 533536 21 learning rate fully connected layers act space partitioners output fully connected layer linear partitioner product wa input space demo tensorflow playground 22 programming keras httpskerasiogettingstartedsequentialmodelguide 23 summary fully connected layers space partitioners gradient descent used parametrize weights w learning backpropagation learning rate determines fast descent performed 24 deep networks generalization 25 deep networks dnn input layer x layers hidden computing units neurons parametrised w layer computes output layer overall ydnnwx 26 notes dnn layers may achieve wide range different processing tasks partitioning convolutional pooling memory units attention processing task function denoted h takes parameters w number layers high hundreds new architectures aim mitigating vanishingexploding gradient problems improving latent representations reducing number parameters allowing parallelisation 27 httpsgraphicsstanfordeducoursescs46817springlectureslidesl1020 20intro_to_deep_learningpdf 28 transformers 29 transformer httpsdocscoherecomdocstransformermodels 30 represent words word encoding tokenization onehot encoding animal didnt cross street tired vocabulary size v issues token word token value 1000000000 semantics encoding 1 0100000000 animal 2 large input vectors size v 0010000000 didnt 3 v2 v1 0000000010 httpstiktokenizervercelapp 0000000001 tired v 31 word embeddings king queen woman princess royalty 099 099 002 098 masculinity 099 005 001 002 005 093 099 096 femininity explained conspiracy make ai seem harder gustav söderström 32 word embeddings king queen woman princess royalty 099 099 002 098 masculinity 099 005 001 002 005 093 099 096 femininity 07 06 05 01 age explained conspiracy make ai seem harder gustav söderström 33 word embeddings king man royalty woman queen royalty 099 001 098 002 100 masculinity 099 099 000 001 001 005 005 000 099 099 femininity explained conspiracy make ai seem harder gustav söderström 34 learn word embeddings encoder decoder focus context word word efficient method learning high quality distributed vectors context focus context word httpslilianwenggithubioposts20171015wordembedding 35 httpswwwcscmuedudstwordembeddingdemoindexhtml 36 word embeddings greatly reduce representation space 100s dimensions instead 10000s infer semantics allow arithmetics words eg wqueenwwomanwmanwking 37 transformer z x httpsdocscoherecomdocstransformermodels 38 attention mechanism lets look two sentences sentence 1 bank river sentence 2 money bank httpscoherecomllmuwhatisattentioninlanguagemodels 39 attention mechanism modified sentence 1 bank1 river modified sentence 2 money bank2 httpscoherecomllmuwhatisattentioninlanguagemodels 40 attention mechanism modified sentence 1 bank1 river modified sentence 2 money bank2 httpscoherecomllmuwhatisattentioninlanguagemodels 41 attention mechanism input x output z z rowwise linear combination values word embeddings queries q look eg article needing information noun keys k provide eg noun giving genre value v provide eg genre httpssebastianraschkacomblog2023selfattentionfromscratchhtml 42 attention mechanism input x output z httpssebastianraschkacomblog2023selfattentionfromscratchhtml 43 attention mechanism input x output z z rowwise linear combination values word embeddings queries q look eg article needing information noun keys k provide eg noun giving genre value v provide eg genre 3blue1brown 44 transformer httpsdocscoherecomdocstransformermodels 45 transformer network demo httpspoloclubgithubiotransformerexplainer httpsbbycroftnetllm 46 training data 47 data often lot focus models architecture httpsnonintcom20230610the itinaimodelsisthedataset httpsdocsgooglecompresentationd1ikzesdowdmwvpxielyji8k3ez98_cl6c5zclksyvg training data fineweb 15 trillion tokens httpshuggingfacecospaceshuggingfacefwblogpostfinewebv1 httpsarxivorgpdf230318223 49 base models good chatting httpsapphyperbolicxyzmodelsllama31405bbase 50 training model chatting dataset conversations human 22 assistant 22 4 human instead assistant 22 4 22 human sky blue assistant rayleigh scattering human wow assistant indeed let know help anything else human hack computer assistant im sorry cant help deep dive llms like chatgpt andrej karpathy 51 instruct datasets example alpaca httpshubzenomlcomprojectf192ed0bc88040cf9d0743f9d4cf176calpaca dataset 52 pretraining vs posttraining pretraining post training 3 months vs 3 hours pretraining simulator internet document posttraining imitating conversation obtained humans labelers llms following labeling instructions instructions given company labelers fairly skilled deep dive llms like chatgpt andrej karpathy 53 conversation training 54 llms make content hallucinations 55 llms make content hallucinations deep dive llms like chatgpt andrej karpathy 56 hallucinations mitigation 1 use model interrogation discover models knowledge programmatically augment training dataset knowledgebased refusals cases model doesnt know eg deep dive llms like chatgpt andrej karpathy 57 hallucinations mitigation 2 allow model search deep dive llms like chatgpt andrej karpathy 58 adding tools llms use llms andrej karpathy 59 llm ecosystem 60 httpsinformationisbeautifulnetvisualizationstheriseofgenerativeailargelanguagemodelsllmslikechatgpt 61 httpsderenrichnetareyousmarterthananllmindexhtml 62 benchmarking example leaderboards httpslmarenaaileaderboard httpsartificialanalysisaimodels httpsscalecomleaderboard 63 hugging face home open ml 300k stars github 500k open source models 100k founded 2016 public data sets 1m 170 employees daily downloads 700k daily visitors 30 httpshuggingfaceco libraries 64 deep learning trends 1 multimodality 2 agents 3 smaller faster models 4 opensource 65 deep learning challenges lack theory designingtraining networks network structures mostly empirical trial error interpretability hard interpret data processed neurons bias adversarial examples approximate reasoning computational resources training times cost carbon footprint training large networks require substantial computational resources deep learning need httpsarxivorgabs210603253 66 go online courses mit introduction deep learning 2025 fastai free online course youtube channels andrej karpathy 3blue1brown books dive deep learning 2022 build large language model scratch 2024 67

72 chapter 3 finding similar items afundamentaldataminingproblemistoexaminedataforsimilaritems shalltake upapplications insection31but anexample wouldbe lookingata collectionofwebpagesandfindingnearduplicatepages thesepagescouldbe plagiarisms example could mirrors almost content differ information host mirrors thenaiveapproachtofindingpairsofsimilaritemsrequiresustolookatev erypairofitems whenwearedealingwithalargedatasetlookingatallpairs items may prohibitive even given abundance hardware resources example even million items gives us half trillion pairs examine million items considered small dataset todays standards therefore pleasant surprise learn family techniques called localitysensitive hashingorlshthatallowsustofocusonpairsthatarelikely similar without look pairs thus possible avoid quadratic growth computation time required naive algorithm usually downside localitysensitive hashing due presence false negatives pairs items similar yet included set pairs examine careful tuning reduce fraction false negatives increasing number pairs consider general idea behind lsh hash items using many different hash functions hash functions conventional sort hash functions rather carefully designed property pairs much likely wind bucket hash function items similar similar examine candidate pairs pairs items wind bucket least one hash functions webeginourdiscussionoflshwithanexaminationoftheproblemoffind ing similar documents share lot common text first show convert documents sets section 32 way lets us view textual similarity documents sets large overlap precisely 73 74 chapter 3 finding similar items measure similarity sets jaccard similarity ratio sizes intersection union second key trick need minhashing section 33 way convert large sets much smaller represen tations called signatures still enable us estimate closely jaccard similarity represented sets finally section 34 see apply bucketing idea inherent lsh signatures section 35 begin study apply lsh items sets consider general notion distance measure tells degree items similar section 36 consider general idea localitysensitivehashingandinsection37weseehowtodolshforsomedata typesotherthansets thensection38examinesindetailseveralapplications ofthe lshidea finallyweconsiderinsection39sometechniquesforfinding similar sets efficient lsh degree similarity want high 31 applications set similarity shall focus initially particular notion similarity similarity setsbylookingattherelativesizeoftheirintersection thisnotionofsimilarity called jaccard similarity introduced section 311 examinesomeoftheusesoffindingsimilarsets theseincludefindingtextually similar documents collaborative filtering finding similar customers similarproducts inordertoturntheproblemoftextualsimilarityofdocuments one set intersection use technique called shingling subject section 32 311 jaccard similarity sets jaccard similarity sets ratio size intersection size union shall denote jaccard similarity simst example 31 fig 31 see two sets three elements intheirintersectionandatotalofeightelementsthatappearins ort orboth thus simst38 312 similarity documents important class problems jaccard similarity addresses well finding textually similar documents large corpus web collection news articles understand aspect similarity wearelookingathereischaracterlevelsimilaritynotsimilarmeaningwhich requiresustoexaminethewordsinthedocumentsandtheiruses thatproblem also interesting addressed techniques hinted 31 applications set similarity 75 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 cid0cid1 figure 31 two sets jaccard similarity 38 section 131 however textual similarity also important uses many involve finding duplicates near duplicates first let us observe testing whether two documents exact duplicates easy compare twodocumentscharacterbycharacterandiftheyeverdifferthenthey arenot howeverin many applications documents identical yet share large portions text examples plagiarism finding plagiarized documents tests ability find textual similarity plagiarizer may extract parts document may alter words may alter order sentences original appear yet resulting document may still contain much original simple process comparing documents character character detect sophisticated plagiarism mirror pages commonfor importantor popular web sites duplicated number hosts order share load pages mirror sites quite similar rarely identical instance might contain information associated particular host might links mirror sites related phenomenon reuseofweb pagesfromoneacademicclassto another thesepagesmight includeclassnotesassignmentsandlectureslides similarpagesmightchange name course year make small changes year year important able detect similar pages kinds search engines produce better results avoid showing two pages nearly identical within first page results 76 chapter 3 finding similar items articles source common one reporter write news article gets distributed say associated press many newspapers publish article web sites newspaper changes article somewhat may cut paragraphs even add material likelywillsurroundthearticlebytheirownlogoadsandlinkstootherarticles site however core newspapers page original article news aggregatorssuchas google news try find versionsofsuch article order show one task requires finding two web pages textually similar although identical1 313 collaborative filtering similarsets problem anotherclassofapplicationswheresimilarityofsetsisveryimportantiscalled collaborativefilteringaprocesswherebywerecommendtousersitemsthatwere liked users exhibited similar tastes shall investigate collaborative filtering detail section 93 moment let us see common examples online purchases amazoncomhasmillions customersandsells millions items database recordswhichitemshavebeenboughtbywhichcustomers wecansaytwocus tomersaresimilariftheirsetsofpurchaseditemshaveahighjaccardsimilarity likewise two items sets purchasers high jaccard similarity deemed similar note might expect mirror sites jaccard similarity 90 unlikely two customers jac card similarity high unless purchased one item even jaccardsimilaritylike20mightbeunusualenoughtoidentifycustomerswith similar tastes observation holds items jaccard similarities need high significant collaborative filtering requires several tools addition finding similar customers items discuss chapter 9 example two amazon customers like sciencefiction might buy many sciencefiction books common however combining similarity finding clustering chapter 7 might able discover science fiction books mutually similar put one group getamorepowerfulnotionofcustomersimilaritybyaskingwhethertheymade purchases within many groups 1newsaggregationalsoinvolvesfindingarticlesthatareaboutthesametopiceventhough textually similar problem yield similarity search requires techniques thanjaccardsimilarityofsets 31 applications set similarity 77 movie ratings netflix records movies customers rented also ratings assigned movies customers regard movies similar rented rated highly many customers see customers similar rented rated highly many movies thesameobservationsthatwemadeforamazonaboveapplyinthissituation similarities need high significant clustering movies genre make things easier whenourdata consistsofratingsratherthan binarydecisionsboughtdid buy likeddisliked cannot rely simply sets representations customers items options 1 ignore lowrated customermovie pairs treat events customer never watched movie 2 comparing customers imagine two set elements movie liked hated customer rated movie highly put liked thatmovieinthecustomersset iftheygavealowratingtoamovieput hated movie set look high jaccard similarity among sets use similar trick comparing movies 3 ratings 1to5stars put movie customers set n times rated movie nstars use jaccard similarity bags measuring similarity customers jaccard similarity bags b c defined counting element n times intersection n minimum number times element appears b c union count element sum number times appears b c2 example 32 bagsimilarity bags aaab aabbc 13 intersection counts twice b size 3 size union two bags always sum sizes two bags 9 case since highest possible jaccard similarity bags 12 score 13 indicates two bags quite similar apparent examination contents 2althoughtheunionforbagsisnormallyeginthesqlstandarddefined tohavethe sumofthenumberofcopiesineachofthetwobagsthisdefinitioncausessomeinconsistency withthejaccardsimilarityforsets underthisdefinitionofbagunionthemaximumjaccard similarityis12not1sincetheunionofasetwithitselfhastwiceasmanyelementsasthe intersection set prefer jaccard similarity set withitselfbe1wecanredefinetheunionofbagstohaveeachelementappearthemaximum number times appears either two bags change also gives reasonable measureofbagsimilarity 78 chapter 3 finding similar items 314 exercises section 31 exercise 311 computethejaccardsimilaritiesofeachpairofthefollowing three sets 1234 2357 246 exercise 312 compute jaccard bag similarity pair fol lowing three bags 1112 11223 1234 exercise 313 suppose universal set u n elements choose two subsets random n elements expected value jaccard similarity 32 shingling documents mosteffective wayto representdocuments assets forthe purposeofiden tifying lexically similar documents construct document set short strings appear within documents share pieces short sentences even phrases many common elements sets even sentences appear different orders two docu ments section introduce simplest common approach shingling well interesting variation 321 kshingles document string characters define kshingle document substring length k found within document may associate eachdocumentthe set ofkshinglesthat appearone ormore times within document example 33 suppose document string abcdabd pick k 2 set 2shingles abbccddabd note substring ab appears twice within appears asa shingle avariationofshinglingproducesa bagratherthanaset soeach shinglewouldappearintheresultasmanytimesasitappearsinthedocument however shall use bags shingles several options regarding white space blank tab newline etc istreated itprobablymakessensetoreplaceanysequenceofoneormore whitespacecharactersbyasingleblank thatwaywedistinguishshinglesthat cover two words example 34 use k 9 eliminate whitespace altogether would see lexical similarity sentences plane ready touch quarterback scored touchdown howeverif retain blanks first shingles touch dow ouch second touchdown eliminated blanks would touchdown 32 shingling documents 79 322 choosing shingle size wecanpickk tobeanyconstantwelike howeverifwepickk toosmallthen would expect sequences k charactersto appear documents could documents whose shinglesets high jaccardsimi larity yet documents none sentences even phrases extreme example use k 1 web pages commoncharactersandfewothercharacterssoalmostallwebpageswillhave high similarity howlargek shouldbe depends onhowlongtypicaldocuments areandhow large set typical characters important thing remember k shouldbe picked largeenough probability givenshingle appearing given document low thus corpus documents emails picking k 5 fine see suppose letters general whitespace character ap pear emails although practice printable ascii characters expected appear occasionally would 275 14348907 possible shingles since typical email much smaller 14 millioncharacterslongwewouldexpectk 5toworkwellandindeeditdoes howeverthe calculation bit subtle surely 27 charac ters appear emails howeverall charactersdo appear equal proba bility common letters blanks dominate z letters high pointvalue scrabble rare thus even short emails many 5shingles consisting common letters chances unrelated emails sharing common shingles greater would implied calculation paragraph good rule thumb imagine 20 characters estimate number kshingles 20k large documents researcharticles choice k 9 considered safe 323 hashing shingles instead using substrings directly shingles pick hash function thatmapsstringsoflengthk tosomenumberofbucketsandtreattheresulting bucket number shingle set representing document set integers bucket numbers one kshingles appear document instance could construct set 9shingles document map 9shingles bucket number 32 range 0 2 1 thus shingle represented four bytes instead nine data compacted manipulate hashed shingles singleword machine operations notice differentiate documents better use 9shingles hashthemdowntofourbytesthantouse4shingleseventhoughthespaceused torepresentashingleisthesame thereasonwastoucheduponinsection322 use 4shingles sequences four bytes unlikely impossible 80 chapter 3 finding similar items find typical documents thus effective number different shingles muchlessthan232 1 ifasinsection322weassumeonly20charactersare frequent english text number different 4shingles likely 4 occur 20 160000 howeverif use 9shinglesthere many 232 likely shingles hash four bytes expect almost sequence four bytes possible discussed section 132 324 shingles built words analternativeformofshinglehasprovedeffectivefortheproblemofidentifying similarnewsarticlesmentionedinsection312 theexploitabledistinctionfor thisproblemisthatthenewsarticlesarewritteninaratherdifferentstylethan elements typically appear page article news articles prose lot stop words see section 131 common words many applications want ignore stop words since dont tell us anything useful article topic howeverfor problem finding similar news articles found defining shingle stop wordfollowedby next two wordsregardless whether stop words formed useful set shingles advantageofthis approachis thatthe news articlewouldthencontribute shingles set representing web page would surrounding ele ments recall goal exercise find pages articles regardless surrounding elements biasing set shingles favor article pages article different surrounding material higher jaccard similarity pages surrounding material different article example 35 ad might simple text buy sudzo however news article idea might read something like spokesperson forthe sudzo corporation revealed today studies shown itis good people buy sudzo products italicized likely stop words although set number frequent words considered stop words first three shingles made stop word next two following spokesperson sudzo sudzo corporation nine shingles sentence none ad 325 exercises section 32 exercise 321 first ten 3shingles first sentence sec tion 32 33 similaritypreserving summaries sets 81 exercise 322 use stopwordbased shingles section 324 take stop words words three fewer letters shingles first sentence section 32 exercise 323 largest number kshingles document n bytes may assume size alphabet large enough number possible strings length k least n 33 similaritypreserving summaries sets sets shingles large even hash four bytes space neededtostoreasetisstillroughlyfourtimesthespacetakenbythedocument millions documents may well possible store shinglesets main memory3 goal section replace large sets much smaller represen tations called signatures important property need signatures compare signatures two sets estimate jaccard sim ilarity underlying sets signatures alone possible signatures give exact similarity sets represent esti mates provide close larger signatures accurate estimates example replace 200000byte hashedshingle sets derive 50000byte documents signatures 1000 bytes usually get within percent 331 matrix representation sets explaining possible construct small signatures large sets helpful visualize collection sets characteristic matrix columns matrix correspond sets rows correspond elements universalset elements sets drawn 1 row r column c element row r member set column c otherwise value position rc 0 element s1 s2 s3 s4 1 0 0 1 b 0 0 1 0 c 0 1 0 1 1 0 1 1 e 0 0 1 0 figure 32 matrix representing four sets 3thereisanotherseriousconcern evenifthesetsfitinmainmemorythenumberofpairs may great us evaluate similarityof pair take solution thisprobleminsection34 82 chapter 3 finding similar items example 36 fig 32 example matrix representing sets chosen universal set abcde s1 ad s2 c s3 bde ands4 acd thetoprowandleftmostcolumnsarenotpartofthematrix pr esent nly remind us rows columns represent important remember characteristicmatrix unlikely thewaythedataisstoredbutitisusefulasawaytovisualizethedata forone reason store data matrix matrices almost always sparse many 0s 1s practice saves space represent sparsematrixof0sand1sbythepositionsinwhichthe1sappear foranother reason data usually stored format purposes asanexampleifrowsareproductsandcolumnsarecustomersrepresented set products bought data would really appear database table purchases tuple table would list item purchaserandprobablyotherdetailsaboutthe purchasesuchasthe dateand credit card used 332 minhashing signatures desire construct sets composed results largenumberofcalculationssayseveralhundredeachofwhichisaminhash characteristic matrix section shall learn minhash computed principle later sections shall see good approxi mation minhash computed practice tominhash asetrepresentedby columnofthe characteristicmatrixpick permutation rows minhash value column number first row permuted order column 1 example 37 let us suppose pick order rows beadc matrix fig 32 permutation defines minhash function h maps sets rows let us compute minhash value set s1 according h first column column set s1 0 row b proceed row e second permuted order 0 column s1 proceed row find 1 thus hs1a element s1 s2 s3 s4 b 0 0 1 0 e 0 0 1 0 1 0 0 1 1 0 1 1 c 0 1 0 1 figure 33 permutation rows fig 32 although physically possible permute large characteristic matrices minhash function h implicitly reorders rows matrix 33 similaritypreserving summaries sets 83 fig 32 becomes matrix fig 33 matrix read values h scanning top come 1 thus see hs2c hs3b hs4a 333 minhashing jaccard similarity remarkable connection minhashing jaccard similarity sets minhashed probability minhash function random permutation rows produces value two sets equals jaccard similarity sets see need picture columns two sets restrict ourselvestothecolumnsforsetss1 ands2thenrowscanbedividedintothree classes 1 type x rows 1 columns 2 type rows 1 one columns 0 3 type z rows 0 columns since matrixis sparsemostrowsareoftype z howeveritis ratio numbers type x type rows determine sims1s2 probability hs1 hs2 let x rows type x rows type sims1s2xxy reasonis x size s1 s2 xy size s1 s2 consider probability hs1 hs2 imagine rows permutedrandomlyandweproceedfromthetoptheprobabilitythatweshall meet type x row meet type row xx first row top type z rows type x row surely hs1 hs2 hand first row type z row thatwemeetisatypey rowthenthesetwitha1getsthatrowasitsminhash value however set 0 row surely gets row permuted list thus know hs1hs2 firstmeet type row 6 conclude probability hs1hs2 xxy also jaccard similarity s1 s2 334 minhash signatures againthinkofacollectionofsetsrepresentedbytheircharacteristicmatrixm represent sets pick random number n permutations rowsofm perhaps100permutationsorseveralhundredpermutationswilldo call minhash functions determined permutations h1h2h n fromthecolumnrepresentingsetsconstructtheminhash signatureforsthe vector h1sh2sh ns normally representthis list hashvalues 84 chapter 3 finding similar items column thus form matrix signature matrix ith column replaced minhash signature set ith column note signature matrix number columns n rows even represented explicitly compressed form suitable sparse matrix eg locations 1s normal signature matrix much smaller remarkable thing signature matrices use columns estimate jaccard similarity sets correspond columns signature matrix theoremprovedin section 333 know probability two columns value given row signature matrix equals jaccardsimilarity sets corresponding columns moreoversince permutations minhash values arebasedwerechosenindependently canthink ofeachrowofthe signature matrix independent experiment thus expected number rows two columns agree equals jaccard similarity corresponding sets moreover minhashings use ie rows sig nature matrix smaller expected error estimate jaccard similarity 335 computing minhash signatures practice itisnotfeasibletopermutealargecharacteristicmatrixexplicitly evenpicking random permutation millions billions rows timeconsuming necessary sorting rows would take even time thus permuted matrices like suggested fig 33 conceptually appealing implementable fortunatelyitispossibletosimulatetheeffectofarandompermutationby random hash function maps row numbers many buckets rows hash function maps integers 01k 1 bucket numbers 0throughk 1typicallywillmapsomepairsofintegerstothesamebucketand leave buckets unfilled however difference unimportant long k large many collisions maintain fiction hash function h permutes row r position hr permuted order thusinsteadofpickingnrandompermutationsofrowswepicknrandomly chosen hash functions h1h2h n rows construct signature matrixbyconsideringeachrowintheirgivenorder letsigicbetheelement signature matrix ith hash function column c initially set sigic c handle row r following 1 compute h1rh2rh nr 2 column c following c 0 row r nothing 33 similaritypreserving summaries sets 85 b howeverifchas1inrowr thenforeachi12nsetsigic smaller current value sigic h ir row s1 s2 s3 s4 x1 mod 5 3x1 mod 5 0 1 0 0 1 1 1 1 0 0 1 0 2 4 2 0 1 0 1 3 2 3 1 0 1 1 4 0 4 0 0 1 0 0 3 figure 34 hash functions computed matrix fig 32 example 38 let us reconsider characteristic matrix fig 32 reproduce additional data fig 34 replaced letters naming rowsby integers0 through4 havealsochosentwohash functions h1xx1 mod 5andh2x3x1 mod 5 thevaluesofthese two functions applied row numbers given last two columns fig 34 notice simple hash functions true permutations rowsbut true permutationis possible number rows5 prime general collisions two rows get hash value let us simulate algorithm computing signature matrix initially matrix consists s1 s2 s3 s4 h1 h2 first consider row 0 fig 34 see values h10 h20 1 row numbered 0 1s columns sets s1 s4 columns ofthe signaturematrix canchange 1 less wedo infactchangebothvaluesinthe columnsfors1 ands4 current estimate signature matrix thus s1 s2 s3 s4 h1 1 1 h2 1 1 move row numbered 1 fig 34 row 1 s3andits hashvaluesareh112andh214 thuswesetsig13to2 sig23 4 signature entries remain columns 0 row numbered 1 new signature matrix s1 s2 s3 s4 h1 1 2 1 h2 1 4 1 86 chapter 3 finding similar items row fig 34 numbered 2 1s columns s2 s4 hashvalues areh123 andh222 could changethe values signaturefors4butthevaluesinthiscolumnofthesignaturematrix11are less corresponding hash values 32 however since column s2 still replace 32 resulting s1 s2 s3 s4 h1 1 3 2 1 h2 1 2 4 1 next comes row numbered 3 fig 34 columns s2 1 andthe hashvalues h134 andh230 value 4 h1 exceeds already signature matrix columns shall change values first row signature matrix however value 0forh2 islessthanwhatisalreadypresentsowelowersig21sig23and sig24to0 notethatwecannotlowersig22becausethecolumnfors2 fig 34 0 row currently considering resulting signature matrix s1 s2 s3 s4 h1 1 3 2 1 h2 0 2 0 0 finally consider row fig 34 numbered 4 h140 h243 since row 4 1 column s3 compare current signature column set 20 hash values 03 since 02 change sig13 0 since 3 0 change sig23 final signature matrix s1 s2 s3 s4 h1 1 3 0 1 h2 0 2 0 0 estimate jaccard similarities underlying sets signature matrix notice columns 1 4 identical guess sims1s410 ifwelookatfig34weseethatthetruejaccardsimilarity s1 s4 23 remember fraction rows agree signature matrix estimate true jaccard similarity example much small law large numbers assure estimates close additional examples signature columns s1 s3 agree half rows true similarity 14 signatures s1 s2 estimate 0 jaccard similarity correct value 336 speeding minhashing process minhashing timeconsuming since need examine entire krow matrix minhash function want let us first return 33 similaritypreserving summaries sets 87 model section 332 imagine rows actually permuted compute one minhash function columns shall go way end permutation look first k rows make small compared k reduce work large factor km however downside making small long column hasatleastone1inthe firstmrowsinpermutedordertherowsafterthe mth effect minhash value may well looked whatifsomecolumnsareall0sinthefirstmrows wehavenominhashvalue columns instead use special symbol shall use examine minhash signatures two columns order esti matethejaccardsimilarityoftheirunderlyingsetsasinsection334wehave take account possibility one columns minhash value components signature three cases 1 neither column given row change needed count row example equal values two values example unequal values 2 one column case used rows originalpermuted matrix column would eventuallyhave givensome row number andthat number surely one first rows permuted order column value one first rows thus surelyhaveanexampleofunequalminhashvaluesandwecountthisrow signature matrix example 3 nowsupposebothcolumnshave inrow thenintheoriginalpermuted matrixmthefirstmrowsofbothcolumnswereall0s wethushaveno information jaccard similarity corresponding sets similarityis function lastk rowswhichwe havechosen look therefore count row signature matrix neither example equal values unequal values long third case columns rare get almost many examples averageas rows signature matrix effect reduce accuracy estimates jaccard distance somewhat much since able compute minhash values columns much faster examined rows afford time apply minhash functions get even better accuracy originally faster 337 speedup using hash functions reasons physically permute rows manner assumed section 336 however idea true permutations makes 88 chapter 3 finding similar items sense context section 336 section 332 reason need construct full permutation k elements pick small number k rows pick randompermutation ofthose rows depending onthe value ofm andhowthe matrixm isstoredit might make sense follow algorithm suggested section 336 literally however likely strategy akin section 335 needed rows fixed permuted choose hash function hashes row numbers compute hash values first rows follow algorithm section 335 reach mthrowwhereuponwestopand andforeachcolumns takethe minimum hash value seen far minhash value column since columnmay have0 rowsit possible someof minhash values assuming sufficiently large minhash values rarewe still get good estimate jaccardsimilarity sets comparing columns signature matrix suppose set elements universal set represented first rows matrix let s1 ands2 setsrepresentedby twocolumns ofm thenthe firstm rows represent sets s1 s2 sets empty ie bothcolumnsareall0intheirfirstmrowsthenthisminhashfunctionwillbe columns ignoredwhen estimating jaccardsimilarity columns underlying sets least one sets s1 s2 nonempty proba bility two columns equal values minhash function jaccard similarity two sets s1 s2 s1 s2 long ast chosento randomsubset ofthe universalset expected value fraction jaccard similarity s1 s2 howeverthere randomvariationsince depending ont could findmoreorlessthananaveragenumberoftype x rows1s inbothcolumns andor type rows 1 one column 0 among first rows matrix tomitigatethisvariationwedonotusethesamesett foreachminhashing rather divide rows km groups4 hash function compute one minhash value examining first rows different minhash value examining second rows thus get km minhash values single hash function single pass rows fact km large enough may get rows signature matrix need single hash function applied subsets rows 4in follows assume divides k evenly convenience unimportant longaskmislargeifsomerowsarenotincludedinanygroupbecause k isnotaninteger multipleofm 33 similaritypreserving summaries sets 89 moreoverby using eachofthe rowsofm compute one ofthese minhash valueswetendtobalanceouttheerrorsinestimationofthejaccardsimilarity due one particular subset rows jaccard similarity s1 s2 determines ratio type x type rows type x rowsaredistributedamongthekmsetsofrowsandlikewisethetypey rows thus one set rowsmay one type row average must set rows fewer average type example 39 fig 35 see matrix representing three sets s1 s2 s3 withauniversalsetofeightelementsie k 8 letuspick m4soone pass rows yields two minhash values one based first four rows second four rows s1 s2 s3 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 figure 35 boolean matrix representing three sets first note jaccard similarities three sets sims1s2 12 sims1s3 15 sims2s3 12 look first four rows whatever hash function use minhash value s1 minhash value s2 hash value 4th row minhash value s3 smaller hash values third fourth rows thus minhash values s1 s2 never agree makes sense since set elements representedby first four rows s1 therefore sims1 ts2 0 however secondfourrowsthe jaccardsimilarityofs1 ands2 restrictedtothe elements represented last four rows 23 weconcludethatifwegeneratesignaturesconsistingoftwominhashvalues using hash function one basedon firstfour rowsandthe second based last four rows expected number matches get signatures s1 s2 average 0 23 13 since actual jaccard similarity s1 s2 12 error great error larger examples minhash values based far four rows expected error approachzero similarly see effect splitting rows two pairs columns s1 s3 top half represents sets jaccard 90 chapter 3 finding similar items similarity 0 bottom half represents sets jaccard similarity 13 theexpectednumberofmatchesinthesignaturesofs1ands3istherefore average 16 compares true jaccard similarity sims1s3 15 finally compare s2 s3 note jaccardsimilarity columns first four rows 12 jaccardsimilarityinthebottomfourrows theaverage12alsoagreesexactly sims2s312 338 exercises section 33 exercise 331 verify theoremfrom section 333 relates jac card similarity probability minhashing equal values partic ular case fig 32 computethejaccardsimilarityofeachofthepairsofcolumnsinfig32 b compute pair columns figure fraction 120 permutations rows make two columns hash value exercise 332 using data fig 34 add signatures columns values following hash functions h3x2x4 mod 5 b h4x3x 1 mod 5 element s1 s2 s3 s4 0 0 1 0 1 1 0 1 0 0 2 1 0 0 1 3 0 0 1 0 4 0 0 1 1 5 1 0 0 0 figure 36 matrix exercise 333 exercise 333 fig 36 matrix six rows compute minhash signature column use following three hash functions h1x 2x1 mod 6 h2x 3x2 mod 6 h3x5x2 mod 6 b hash functions true permutations 34 localitysensitive hashing documents 91 c howclosearetheestimatedjaccardsimilaritiesforthesixpairsofcolumns true jaccardsimilarities exercise 334 nowthat knowjaccardsimilarityis relatedto proba bility two sets minhash value reconsider exercise 313 use relationship simplify problem computing expected jaccard similarity randomly chosen sets exercise 335 provethat jaccardsimilarity oftwo columns 0 minhashing always gives correct estimate jaccard similarity exercise 336 one might expect could estimate jaccard simi larityofcolumnswithoutusingallpossiblepermutationsofrows forexample could allow cyclic permutations ie start randomly chosen row r becomes first order followed rows r1 r2 last row continuing first row second row row r 1 n permutations n rows howeverthese permutations notsufficient estimate jaccard similarity correctly give example twocolumn matrix averaging cyclic permutations give jaccard similarity exercise 337 supposewewanttouseamapreduceframeworktocompute minhash signatures matrix stored chunks correspond columns quite easy exploit parallelism map task gets ofthecolumnsandallthehashfunctionsandcomputestheminhashsignatures given columns however suppose matrix chunked rows thatamaptaskisgiventhehashfunctionsandasetofrowstoworkon design map reduce functions exploit mapreduce data form exercise 338 noticed section 336 problems column 0s compute minhash function using entire columns section 332 time get 0s column columnrepresentstheemptyset howshouldwehandletheemptysettomake sure errors jaccardsimilarityestimation introduced exercise 339 example 39 three estimates jaccard sim ilarity obtained either smaller true jaccard similarity possible another pair columns average jaccardsimilaritiesoftheupperandlowerhalveswillexceedtheactualjaccard similarity columns 34 localitysensitive hashing documents even though use minhashing compress large documents small signatures preserve expected similarity pair documents stillmaybe impossibletofindthe pairswithgreatestsimilarityefficiently 92 chapter 3 finding similar items reasonis thatthe number ofpairsofdocumentsmay toolargeevenifthere many documents example 310 suppose million documents use signatures length 250 use 1000 bytes per document signatures entiredatafitsinagigabytelessthanatypicalmainmemoryofalaptop howeverthere 1000000 half trillion pairs documents takes 2 cid0 cid1 microsecond compute similarity two signatures takes almost six days compute similarities laptop goal compute similarity every pair nothing reduce work although parallelism reduce elapsed time however often want similar pairs pairs lower bound similarity need focus attention onpairsthatarelikely tobe similarwithoutinvestigatingeverypair thereis general theory provide focus called localitysensitive hashing lshornearneighbor search inthissectionweshallconsideraspecificform oflshdesignedfortheparticularproblemwehavebeenstudying documents representedby shinglesetsthen minhashed short signatures section 36 present general theory localitysensitive hashing number applications related techniques 341 lsh minhash signatures onegeneralapproachtolshistohashitemsseveraltimesinsuchawaythat similar items likely hashed bucket dissimilar items consider pair hashed bucket hashings candidate pair check candidate pairs similarity hope dissimilar pairs never hash bucket therefore never checked dissimilar pairs hash bucket false positives hope small fraction pairs also hope truly similar pairs hash bucket least one hash functions false negatives hope small fraction truly similar pairs minhash signatures items effective way choose hashings divide signature matrix b bands consisting r rows band hash function takes vectors r integers portion one column within band hashes large number buckets use hash function bands use separatebucketarrayfor eachband columns vector different bands hash bucket example 311 figure37showspartofasignaturematrixof12rowsdivided four bands three rows second fourth explicitly shown columns column vector 021 first band 34 localitysensitive hashing documents 93 1 0 0 0 2 band 1 3 2 1 2 2 0 1 3 1 1 band 2 band 3 band 4 figure 37 dividing signature matrix four bands ofthree rowsper band definitely hash bucket hashing first band thus regardless columns look like three bands pair columns candidate pair possible columns first two shown explicitly also hash bucket according thehashingofthe firstband howeversincetheircolumnvectorsaredifferent 130and021andtherearemanybucketsforeachhashingweexpectthe chances accidental collision small shall normally assume two vectors hash bucket identical twocolumnsthatdonotagreeinband1havethreeotherchancestobecome candidate pair might identical one bands however observe similar two columns likely thattheywillbeidenticalinsomeband thusintuitivelythebandingstrategy makes similar columns much likely candidate pairs dissimilar pairs 342 analysis banding technique suppose use b bands r rows suppose particular pair documents jaccard similarity recall section 333 prob ability minhash signatures documents agree one particular row signature matrix calculate probability documents rather signatures become candidate pair follows 1 probability signatures agree rows one particular band sr 2 probability signatures disagree least one row par ticular band 1 sr 3 probability signatures disagree least one row bands 1 srb 94 chapter 3 finding similar items 4 probability signatures agree rows least one band therefore become candidate pair 1 1 srb probability becoming candidate 0 jaccard similarity 1 documents figure 38 scurve may obvious regardless chosen constants b r function form scurve suggested fig 38 threshold value similarity probability becoming candi date 12 function b r threshold roughly rise steepest large b r find pairs similarity thresholdareverylikelytobecomecandidateswhile thosebelowthe threshold areunlikely become candidates exactly situation want approx imationtothe thresholdis1b1r forexampleifb16andr 4then threshold approximately s12 since 4th root 116 12 example 312 letusconsiderthecaseb20andr 5 thatiswesuppose signatures length 100 divided twenty bands five rows figure 39 tabulates values function 1 1 s520 notice threshold value curve risen halfway slightly 05 also notice curve exactly ideal step function jumps 0 1 threshold slope curve middle significant example rises 06 going s04 s06 slope middle greater 3 example s08 1 085 0672 raise number 20th power get 000035 subtracting fraction 1 yields099965 thatisifweconsidertwodocumentswith80similaritythen inanyonebandthey haveonlyabouta33chanceofagreeinginallfiverows thus becoming candidate pair howeverthere 20 bands thus 20 34 localitysensitive hashing documents 95 1 1 srb 2 006 3 047 4 186 5 470 6 802 7 975 8 9996 figure 39 values scurve b20 r 5 chancestobecomeacandidate onlyroughlyonein3000pairsthatareashigh as80similarwillfailtobecomeacandidatepairandthusbeafalsenegative 343 combining techniques give approach finding set candidate pairs similar documents discovering truly similar documents among must emphasized approach produce false negatives pairs similar documents identified never become candidate pair also false positives candidate pairs evaluated found sufficiently similar 1 pickavalue ofk andconstructfromeachdocument setofkshingles optionally hash kshingles shorter bucket numbers 2 sort documentshingle pairs order shingle 3 pick length n minhash signatures feed sorted list algorithm section 335 compute minhash signatures documents 4 choose threshold defines similar documents order regardedas desired similar pair pick number bands b number rows r br n threshold approximately 1b1r avoidance false negatives important may wish select b r produce threshold lower speed important wish limit false positives select b r produce higher threshold 5 constructcandidatepairsbyapplyingthelshtechniqueofsection341 6 examineeachcandidatepairssignaturesanddeterminewhetherthefrac tion components agree least 96 chapter 3 finding similar items 7 optionally signatures sufficiently similar go documents check truly similar rather documents luck similar signatures 344 exercises section 34 exercise 341 evaluatethe scurve1 1 srb s010209for following values r b r3 b10 r6 b20 r5 b50 exercise 342 rb pairs exercise 341 compute thresholdthatisthevalueofsforwhichthevalueof1 1 srbisexactly12 value compare estimate 1b 1r suggested section 342 exercise 343 use techniques explainedinsection135to approximate scurve 1 1 srb sr small exercise 344 suppose wish implement lsh mapreduce specifi cally assume chunks signature matrix consist columns elements keyvalue pairs key column number value signature ie vector values show produce buckets bands output single mapreduce process hint remember map function produce severalkeyvalue pairs single element b show another mapreduce process convert output list pairs need compared specifically column list columns j needs compared 35 distance measures take short detour study general notion distance measures jaccard similarity measure close sets although really distance measure closer sets higher jaccard similarity rather 1 minus jaccard similarity distance measure shall see called jaccard distance however jaccard distance measure closeness makes sense weshallexamineinthissectionsomeotherdistancemeasuresthathave applications section 36 see distance measures 35 distance measures 97 also lsh technique allows us focus nearby points without comparingallpoints otherapplicationsofdistancemeasureswillappearwhen study clustering chapter 7 351 definition distance measure suppose set points called space distance measure space function dxy takes two points space arguments produces real number satisfies following axioms 1 dxy 0 negative distances 2 dxy 0 x distances positive except distance point 3 dxydyx distance symmetric 4 dxy dxzdzy triangle inequality triangle inequality mostcomplex condition saysintuitively totravelfromxtoywecannotobtainanybenefitifweareforcedtotravelvia particular third point z triangleinequality axiom makes distance measures behave distance describes length shortest path one point another 352 euclidean distances familiar distance measure one normally think dis tance ndimensional euclidean space one points vectors n real numbers conventionaldistance measure space shall refer l2norm defined n dx1x2x n y1y2y nv u uxx i2 ti1 square distance dimension sum squares take positive square root easy verify first three requirements distance measure satisfied euclidean distance two points cannot negative causethe positivesquarerootisintended sinceallsquaresofrealnumbersare nonnegative x forces distance strictly positive 6 onthe otherhand ifx forall ithenthe distanceis clearly0 symmetry follows x 2 x 2 triangle inequality requires good deal algebra verify however well understood property euclideanspace sum ofthe lengths ofany two sides triangle less length third side 98 chapter 3 finding similar items thereareotherdistancemeasuresthathavebeenusedforeuclideanspaces constant r define l norm distance measure r defined n dx1x2x n y1y2y n xx r1r i1 thecaser 2istheusuall2normjustmentioned anothercommondistance measure l1norm manhattan distance distance two points sum magnitudes differences dimension called manhattan distance distance one would travel points one constrained travel along grid lines streets city manhattan another interesting distance measure lnorm limit r approaches infinity l norm r gets larger dimension r largest difference matters formally lnorm defined maximum x dimensions example 313 consider twodimensional euclidean space custom ary plane points 27 64 l2norm gives distance 2 627 42 4232 5 l1norm gives distance p 2 6 7 4 437 lnorm gives distance max2 6 7 4max434 353 jaccard distance mentioned beginning section define jaccard distance sets dxy1 simxy jaccard distance 1 minus ratio sizes intersection union sets x must verify function distance measure 1 dxy nonnegative size intersection cannot exceed size union 2 dxy0 xy x xx xx however xy 6 size ofx strictly less size ofx dxy strictly positive 3 dxydyx union intersection symmetric ie x x x x 4 triangle inequality recall section 333 simxy probability random minhash function maps x value thus jaccard distance dxy probability random min hashfunction send x value therefore 35 distance measures 99 translate condition dxy dxzdzy statement h random minhash function probability hx hy 6 greater sum probability hx hz 6 probability hz hy however statement true 6 whenever hx hy least one hx hy must different 6 hz could hz hx hy would 354 cosine distance cosine distance makes sense spaces dimensions including eu clidean spaces discrete versions euclidean spaces spaces points arevectorswithintegercomponentsorboolean0 or1components suchaspacepointsmaybethoughtofasdirections wedonotdistinguishbe tweena vectoranda multiple ofthatvector thenthe cosinedistancebetween two points angle vectors points make angle range 0 180 degrees regardlessof many dimensions space calculate cosine distance first computing cosine angleand applying arccosinefunction translate anangle 0180degreerange giventwo vectorsx andy cosine ofthe anglebetween dot product xy divided l2norms x ie euclidean distances origin recall dot product vectors x1x2x ny1y2y n pn i1x iy example 314 let two vectors x 12 1 211 dot product xy 1 22 1 1 1 3 l2norm vectors 6 example x l 2norm 1222 12 6 thus cosine p angle x 366 12 angle whose cosine 12 60 degrees cosine distance x must show cosine distance indeed distance measure defined values range 0 180so negative distances arepossible twovectorshaveangle0ifandonlyiftheyarethesamedirection5 symmetry obvious angle x angle x triangle inequality best argued physical reasoning one way rotate x rotate z thence sum two rotations cannot less rotation directly x 5notice satisfy second axiom treat vectors multiples oneanothereg 12and36asthesamedirectionwhichtheyare ifweregardedthese different vectors wouldgivethem distance 0andthus violate thecondition dxxis0 100 chapter 3 finding similar items 355 edit distance distance makes sense points strings distance two strings xx1x2 x n y1y2 smallest number insertions deletions single characters convert x example 315 edit distance strings x abcde acfdeg 3 convert x 1 delete b 2 insert f c 3 insert g e sequenceoffewerthan threeinsertionsandordeletionswill convertxto thus dxy3 another way define calculate edit distance dxy compute longest common subsequence lcs x lcs x string constructed deleting positions x long string constructed way edit distance dxy canbe calculatedasthe lengthofx plus lengthof minus twice length lcs example 316 strings x abcde acfdeg example 315 unique lcs acde sure longest possible becauseitcontainseverysymbolappearinginbothxandy fortunatelythese common symbols appear order strings able use allin lcs note lengthof x 5 length ofy 6 length lcs 4 edit distance thus 56 2 43 agrees direct calculation example 315 another example consider xaba bab edit distance 2 forexamplewecanconvertx toy bydeleting firstaandthen inserting b end two lcss ab ba obtained deleting one symbol string must case multiple lcss pair strings lcss length therefore may compute edit distance 33 2 22 editdistanceisadistancemeasure surelynoeditdistancecanbenegative two identical strings edit distance 0 see edit distance symmetric note sequence insertions deletions reversedwith insertionbecoming deletion vice versa triangle inequality also straightforward one way turn string string turn string u turn u thus number edits made going u plus number edits made going u cannot less smallest number edits turn 35 distance measures 101 noneuclidean spaces notice thatseveralofthe distancemeasuresintroducedinthis sectionare euclidean spaces property euclidean spaces shall find important take clustering chapter 7 average points euclidean space always exists point space however consider space sets defined jaccard dis tance notion average two sets makes sense likewise space strings use edit distance let us take averageof strings vectorspacesforwhichwesuggestedthecosinedistancemayormay euclidean components vectors real num bers space euclidean however restrict components integersthen space euclidean notice instance cannotfindanaverageofthevectors12and31inthespaceofvectors two integer components although treated members thetwodimensionaleuclideanspacethenwecouldsaythattheiraverage 2015 356 hamming distance given space vectorswe define hamming distance two vectors number components differ obvious hamming distance distance measure clearly hamming distance cannot negative zero vectors identical dis tance depend two vectors consider first triangle inequality also evident x z differ components z differ n components x cannot differ mn components commonly hamming distance used vectors booleanthey consistof0sand1sonly howeverinprinciplethe vectorscan components set example 317 hamming distance vectors10101and11110 3 vectors differ second fourth fifth components agree first third components 357 exercises section 35 exercise 351 space nonnegative integers following functions distance measures prove prove fails satisfy one axioms maxxy larger x 102 chapter 3 finding similar items b diffxy x absolute magnitude difference x c sumxyxy exercise 352 findthe l1 andl2 distancesbetweenthepoints567and 824 exercise 353 prove j positive integers j l norm betweenany two points greaterthanthe l normbetween j two points exercise 354 find jaccard distances following pairs sets 1234 2345 b 123 456 exercise 355 compute cosines angles fol lowing pairs vectors6 3 12 231 b 123 246 c 50 4 1 62 011011and 001000 exercise 356 provethatthecosinedistancebetweenanytwovectorsof0s 1s length 90 degrees exercise 357 find edit distances using insertions deletions following pairs strings abcdef bdaefc b abccdabcand acbdcab c abcdef baedfc exercise 358 thereareanumberofothernotionsofeditdistanceavailable forinstancewecanallowinadditiontoinsertionsanddeletionsthefollowing operations 6notethatwhatweareaskingforisnotpreciselythecosinedistancebutfromthecosine angle compute angle perhaps aid table library function 36 theory localitysensitive functions 103 mutation one symbol replaced another symbol note mutationcanalwaysbe performedbyaninsertionfollowedbya deletion butifweallowmutationsthenthischangecountsforonly1not2when computing edit distance ii transposition two adjacentsymbols positions swapped likeamutationwecansimulateatranspositionbyoneinsertionfollowed one deletion count 1 two steps repeat exercise 357 edit distance defined number insertions deletions mutations transpositions needed transform one string another exercise 359 prove edit distance discussed exercise 358 indeed distance measure exercise 3510 find hamming distances pair fol lowing vectors 000000110011010101and 011100 36 theory localitysensitive functions lshtechnique developedinsection34isoneexampleofafamilyoffunc tionstheminhashfunctionsthatcanbecombinedbythebandingtechnique todistinguishstronglybetweenpairsatalowdistancefrompairsatahighdis tance steepness scurve fig 38 reflects effectively avoid false positives false negatives among candidate pairs nowweshallexploreotherfamilies offunctions besides minhashfunc tionsthatcanservetoproducecandidatepairsefficiently thesefunctionscan apply tothe spaceofsetsandthe jaccarddistanceorto anotherspaceandor anotherdistancemeasure therearethreeconditionsthatweneedforafamily functions 1 must likely make close pairs candidate pairs distant pairs make notion precise section 361 2 must statistically independent sense possible estimate probability two morefunctions givea certain response product rule independent events 3 must efficient two ways must able identify candidate pairs time much less time takes look pairs example minhash functions capability since hash sets minhash values time proportional size data rather square number sets data since sets common values colocated bucket implicitly produced 104 chapter 3 finding similar items candidatepairsforasingleminhashfunctionintime muchlessthan number pairs sets b theymustbecombinabletobuildfunctionsthatarebetteratavoid ing false positives negatives combined functions must also take time much less number pairs ex ample banding technique section 341 takes single minhash functions whichsatisfycondition3abutdo notby themselveshave scurve behavior want produces number min hash functions combined function scurve shape first step define localitysensitive functions generally see idea applied several applications finally discuss apply theory arbitrary data either cosine distance euclidean distance measure 361 localitysensitive functions forthepurposesofthissectionweshallconsiderfunctionsthattaketwoitems render decision whether items candidate pair many casesthe function f hashitems decisionwill based whether result equal convenient use notation fx fy mean fxy yes make x candidate pair shall use fxfy shorthand meaning also use fxfy mean make x candidate pair unless 6 function concludes collection functions form called family functions forexamplethefamilyofminhashfunctionseachbasedononeofthepossible permutations rows characteristic matrix form family let d1 d2 two distances according distance measure family f functions said d1d2p1p2sensitive every f f 1 dxy d1 probability fxfy least p1 2 dxy d2 probability fxfy p2 figure 310 illustrates expect probability given function d1d2p1p2sensitive family declare two items didatepair noticethatwesaynothingaboutwhathappenswhenthedistance items strictly d1 d2 make d1 d2 close wish penalty typically p1 p2 closeas well shall see possible drive p1 p2 apart keeping d1 d2 fixed 362 localitysensitive families jaccard distance moment one way find family localitysensitive functions use family minhash functions assume distance 36 theory localitysensitive functions 105 p 1 probabilty declared candidate p 2 1 2 distance figure 310 behavior d1d2p1p2sensitive function measure jaccard distance interpret minhash function h make x candidate pair hxhy thefamilyofminhashfunctionsisad1d21 d11 d2sensitivefamily d1 d2 0 d1 d2 1 reason dxy d1 jaccard distance simxy 1 dxy 1 d1 know jaccard similarity x equal probability minhash function hash x value similar argument applies d2 distance example 318 could let d1 03 d2 06 assert family minhash functions 03060704sensitivefamily jaccard distance x 03 ie simxy 07 atleasta 07 chancethat minhashfunction sendx andy samevalueandifthejaccarddistancebetweenxandy isatleast06ie simxy 04 04 chance x sent value note could make assertion another choice d1 d2 d1 d2 required 363 amplifying localitysensitive family suppose given d1d2p1p2sensitive family f construct newfamilyf bytheandconstructiononfwhichisdefinedasfollows memberoff consistsofr membersoffforsomefixedr iff isinfandf constructed set f1f2f r members f say fx fy f x f 12r notice construction mirrors effect r rows single band band makes x 106 chapter 3 finding similar items candidatepairifeveryoneofthe r rowsinthe bandsaythatxandy areequal therefore candidate pair according row since members f independently chosento make member f canassertthatf d1d2p1rp2r sensitive family cid0 cid1 pifpistheprobabilitythatamemberoffwilldeclarexytobeacandidate pair probability member f declare pr thereisanotherconstructionwhichwecalltheorconstructionthatturns d1d2p1p2sensitive family f d1d21 1 p1b1 1 p2b cid0 cid1 sensitive family f eachmember f f constructed b members f say f1f2f b define fxfy f ixf iy one values orconstruction mirrors effect combining several bands x become candidate pair band makes candidate pair ifpistheprobabilitythatamemberoffwilldeclarexytobeacandidate pairthen1 pistheprobabilityitwillnotsodeclare 1 pbistheprobability none f1f2f b declare xy candidate pair 1 1 pb probability least one f declare xy candidate pair therefore f declare xy candidate pair noticethattheandconstructionlowersallprobabilitiesbutifwechoosef andrjudiciouslywecanmakethesmallprobabilityp2getverycloseto0while higher probability p1 stays significantly away 0 similarly construction makes probabilities rise choosing f b judiciously make larger probability approach 1 smaller probability remainsboundedawayfrom1 cancascadeand andorconstructionsin order make low probability close 0 high probability close 1 course constructions use higher values r b pick larger number functions originalfamily forced use thus better final family functions longer takes apply functions family example 319 supposewestartwithafamilyf weusetheandconstruc tion r 4 produce family f1 apply orconstruction f1 b 4 produce third family f2 note members f2 eacharebuiltfrom16membersoff andthe situationis analogousto starting 16 minhash functions treating four bands four rows 4way andfunction converts probability p p4 follow 4way orconstruction probability converted into1 1 p44 somevaluesofthistransformationareindicatedinfig311 thisfunctionisanscurvestayinglowforawhilethenrisingsteeplyalthough steeply slope never gets much higher 2 leveling high values like scurve fixedpoint value p left unchanged apply function scurve case fixedpoint value p p 1 1 p44 see fixedpointissomewherebetween07and08 belowthatvalueprobabilitiesare decreased increased thus pick high probability 36 theory localitysensitive functions 107 p 1 1 p44 02 00064 03 00320 04 00985 05 02275 06 04260 07 06666 08 08785 09 09860 figure 311 effect 4way andconstruction followed 4way construction fixedpoint low probability shall desired effectthatthelowprobabilityisdecreasedandthehighprobabilityisincreased suppose f minhash functions regarded 02060804sens itive family f2 family constructed 4way followed 4wayorisa02060878500985sensitivefamilyaswecanreadfromthe rows 08 04 fig 311 replacing f f2 reduced falsenegative falsepositive rates cost making application functions take 16 times long p 1 1 p4 4 cid0 cid1 01 00140 02 01215 03 03334 04 05740 05 07725 06 09015 07 09680 08 09936 figure 312 effect 4way orconstruction followed 4way construction example 320 cost apply 4way orconstruction followed 4way andconstruction figure 312 gives transformation onprobabilities implied construction instance suppose f 02060804sensitivefamily constructed family 02060993605740sensitive 108 chapter 3 finding similar items family choice notnecessarilythe best althoughthe higherprobability moved much closer 1 lower probability also raised increasing number false positives example 321 cascade constructions much like exam ple could use construction example 319 family minhash functionsandthenusetheconstructionofexample320ontheresultingfamily theconstructedfamilywouldthenhavefunctions eachbuiltfrom256minhash functions would instance transform 02080802sensitive family 02080999128500000004sensitivefamily 364 exercises section 36 exercise 361 effect probability starting family minhash functions applying 2way construction followed 3way construction b 3way construction followed 2way construction c a2wayandconstructionfollowedbya2wayorconstructionfollowed 2way construction a2wayorconstructionfollowedbya2wayandconstructionfollowed 2way construction followed 2way construction exercise 362 find fixedpoints functions constructed exercise 361 exercise 363 function probability p fig 311 slope given derivative function maximum slope derivative maximum find value p givesa maximumslope scurves given fig 311 fig 312 values maximum slopes exercise 364 generalizeexercise363togiveasafunctionofr andbthe point maximum slope value slope families functions defined minhash functions rway construction followed bway construction b bway construction followed rway construction 37 lsh families distance measures guaranteethata distance measurehas alocalitysensitivefamily hashfunctions sofarwehaveonlyseensuchfamiliesforthejaccarddistance section shall show construct localitysensitive families hamming distance cosine distance normal euclidean distance 37 lsh families distance measures 109 371 lsh families hamming distance quite simple build localitysensitive family functions ham ming distance suppose space ddimensional vectors hxy denotes hamming distance vectors x take one position vectors say ith position define function f x ith bit vector x f x f vectors x agree ith position probability f x f ran domly chosen exactly 1 hxyd ie fraction positions x agree thissituationisalmostexactlyliketheoneweencounteredforminhashing thus family f consisting functions f1f2f d1d21 d1d1 d2dsensitive family hash functions d1 d2 two differences family family minhash functions 1 jaccard distance runs 0 1 hamming distance vector space dimension runs 0 therefore necessary scale distances dividing turn probabilities 2 essentially unlimited supply minhash functions size family f hamming distance first point consequence requires divide appropriate times second point serious relatively small limited number functions composed using constructions thereby limiting steep make scurve 372 random hyperplanes cosine distance recall section 354 cosine distance two vectors angle vectors instance see fig 313 two vectors x make angle θ note vectors may space many dimensions always define plane angle measured plane figure 313 topview plane containing x supposewepickahyperplanethroughtheorigin thishyperplaneintersects plane x line figure 313 suggests two possible hyperplanes one whose intersection dashed line others intersection dotted line pick random hyperplane actually pick normal vector hyperplane say v hyperplane set points whose dot product v 0 firstconsideravectorv thatisnormaltothehyperplanewhoseprojection represented dashed line fig 313 x different 110 chapter 3 finding similar items x θ figure 313 two vectors make angle θ sides hyperplane dot products vx vy different signs assume instance v vector whose projection onto plane x dashed line fig 313 vx positive whilevy isnegative thenormalvectorv insteadmightextendintheopposite directionbelowthedashedline inthatcasevxisnegativeandvy ispositive signs still different hand randomly chosen vector v could normal hyperplane like dotted line fig 313 case vx vy sign projection v extends right dot products positive v extends left negative probability randomly chosen vector normal hyperplane looks like dashed line rather dotted line angles line intersection random hyperplane plane x equally likely thus hyperplane look like dashed line probability θ180 look like dotted line otherwise thus hash function f localitysensitive family f built randomly chosen vector v given two vectors x say fx fy f dot products v x v sign f f f localitysensitive family cosine distance parameters essentially jaccarddistance family described section 362 except scale distances 0180 rather 01 f d1d2180 d1180180 d2180sensitive familyofhashfunctions fromthisbasiswecanamplifythefamilyaswewish minhashbased family 37 lsh families distance measures 111 373 sketches instead chosing randomvector possible vectorsit turns sufficiently random restrict choice vectors whose components 1 1 dot product vector x vector v 1s 1s formed adding components x v 1 subtracting components x v 1 pick collection random vectors say v1v2v n applythemtoanarbitraryvectorxbycomputingv1xv2xv nxandthen replacinganypositive value 1andany negativevalue 1 resultis calledthesketchofx youcanhandle0sarbitrarilyegbychosingaresult1 1 random since tiny probability zero dot product choice essentially effect example 322 suppose space consists 4dimensional vectors pick three random vectors v1 1 111 v2 11 11 v3 11 1 1 vector x3456 sketch 11 1 thatisv1x3 45610 sincetheresultispositivethefirstcomponent ofthesketchis1 similarlyv2x2andv3x 4sothesecondcomponent sketch 1 third component 1 consider vector 4321 similarly compute sketch 1 11 since sketches x agree 13 positions estimate angle 120 degrees randomly chosenhyperplaneistwiceaslikelytolooklikethedashedlineinfig313than like dotted line conclusion turns quite wrong calculate cosine angle x xy 6 15 24 33 440 divided magnitudes two vectors magnitudes 62524232 9274 p 12223242 5477 thus cosine angle x 07875 angle 38 degrees however look 16 different vectors v length 4 1 1 components find four whose dot products x different sign namely v2 v3 complements 1 11 1 1 111 thus picked sixteen vectors form ketc h estimate angle would 180445 degrees 374 lsh families euclidean distance let us turn euclidean distance section 352 see develop localitysensitive family hash functions distance shall startwitha2dimensionaleuclideanspace eachhashfunctionf inourfamily 112 chapter 3 finding similar items fwill associatedwith randomlychosenline inthis space pick constant aanddividethelineintosegmentsoflengthaassuggestedbyfig314where random line oriented horizontal points distance θ bucket width figure314 twopoints atdistanced haveasmallchanceofbeinghashed bucket thesegmentsofthelinearethebucketsintowhichfunctionf hashespoints pointis hashedto bucketin whichits projectionontothe line lies distance two points small compared good chance two points hash bucket thus hash function f willdeclarethetwopointsequal forexampleifda2thenthereisatleast 50 chance two points fall bucket fact angle θ betweenthe randomly chosenline andthe line connecting points large even greater chance two points fall bucket instance θ 90 degrees two points certain fall bucket howeversuppose larger order chance two points falling bucket need dcosθ diagram fig 314 suggests requirement holds note even dcosθ still certain two points fall bucket however guarantee following 2a 13 chance two points fall bucket reason cosθ less 12 need θ range 60 90 degrees θ range 0 60 degrees cosθ 12 since θ smaller angle betweentwo randomly chosenlines plane θ twice likely 0 60 60 90 conclude family f described forms a22a1213 sensitive family hash functions distances a2 proba bilityisatleast12thattwopointsatthatdistancewillfallinthesamebucket whilefordistancesatleast2athe probabilitypointsatthatdistancewillfallin 37 lsh families distance measures 113 bucket 13 amplify family like examples localitysensitive hash functions discussed 375 lsh families euclidean spaces something unsatisfying family hash functions developed section 374 first technique described twodimensional euclidean spaces happens data points space many dimensions second jaccardand cosine distances able develop localitysensitivefamiliesforanypairofdistancesd1 andd2 aslongasd1 d2 section 374 appear need stronger condition d1 4d2 however claim localitysensitive family hash func tions d1 d2 number dimensions familys hash functions still derive random lines space bucket size partitions line still hash points projecting onto line giventhat d1 d2 may know probability p1 two points distance d1 hash bucket certain greater p2 probability two points distance d2 hash bucket reason probability surely grows distance shrinks thus evenif cannotcalculate p1 p2 easilywe know thatthere d1d2p1p2sensitive family hash functions d1 d2 given number dimensions using amplification techniques section 363 adjust twoprobabilitiestosurroundanyparticularvaluewelikeandtobeasfarapart like course apart want probabilities larger number basic hash functions f must use 376 exercises section 37 exercise 371 supposeweconstructthebasicfamilyofsixlocalitysensitive functions vectorsof length six eachpair ofthe vectors 000000110011 010101and 011100which six functions makes candidates exercise 372 let us compute sketches using following four random vectors v1 111 1 v2 11 11 v3 1 111 v4 1111 compute sketches following vectors 2345 b 23 45 c 2 34 5 114 chapter 3 finding similar items pair estimated angle according sketches true angles exercise 373 suppose form sketches using sixteen vectors length 4 whose components 1 1 compute sketches three vectorsinexercise372 howdo estimatesofthe anglesbetween pair compare true angles exercise 374 suppose form sketches using four vectors exer cise 372 constraints b c cause sketch vector abcd 1111 b considertwovectorsabcdandefgh whataretheconditionson abh make sketches two vectors exercise 375 suppose points 3dimensional euclidean space p1 123p2 024andp3 432 considerthethreehashfunctions defined three axes make calculations easy let buckets oflengthawithonebuckettheinterval0aiethesetofpointsxsuchthat 0 xa next a2a previous one a0 foreachof three lines assigneachofthe points bucketsassuming a1 b repeat part assuming a2 c candidate pairs cases a1 a2 pair points values pair candidate pair 38 applications localitysensitive hashing section shallexplore three examples ofhow lsh used practice case techniques learned must modified meet certain constraints problem three subjects cover 1 entityresolution thistermreferstomatchingdatarecordsthatreferto realworld entity eg person principal problem addressed similarity records match exactly eitherthesimilarsetsorsimilarvectorsmodelsofsimilarityonwhichthe theory built 2 matching fingerprints possible represent fingerprints sets howeverweshallexploreadifferentfamilyoflocalitysensitivehashfunc tions one get minhashing 38 applications localitysensitive hashing 115 3 matching newspaper articles consider different notion shingling focuses attention core article online news papers web page ignoring extraneous material ads newspaperspecific material 381 entity resolution itiscommontohaveseveraldatasetsavailableandtoknowthatthey referto entities example several different bibliographic sources provide information many books papers general casewe haverecordsdescribingentities ofsome type suchas people books recordsmay allhave format orthey may different formats different kinds information many reasons information entity may vary even field question supposed example names may expressed differently different records misspellings absence middle initial use nickname many reasons example bob jomes robert jones jr may may person records come different sources fields may differ well one sources records may age field another second source might date birth field may information person born 382 entityresolution example shall examine real example lsh used deal entity resolution problem company engaged company b solicit cus tomers b company b would pay yearly fee long customer maintained subscription later quarreled disagreed many customersahadprovidedto b eachhadabout1000000recordssome described people customers provided b records different data fields unfortunately none fields customer provided b thus problem match records two sets see pair represented person record fields name address phone number person however values fields could differ many reasons misspellings naming differences mentioned section381buttherewereotheropportunitiestodisagreeaswell acustomer might give home phone cell phone b might moveandtellb nota longerhadneed fora relationship area codes phones sometimes change thestrategyforidentifying recordsinvolvedscoringthe differencesinthree fields name address phone create score describing likelihood two records one b described per 116 chapter 3 finding similar items son 100 points assigned three fields records exact matchesinallthreefieldsgotascoreof300 howeverthereweredeductionsfor mismatches three fields first approximation editdistance section 355wasused penalty grew quadraticallywith distance certain publicly available tables used reduce penalty ap propriatesituations forexamplebillandwilliamweretreatedasifthey differed one letter even though editdistance 5 however feasible score one trillion pairs records thus simple lsh used focus likely candidates three hash functions used first sent records bucket identical names second identical addresses third phone numbers practice hashing rather records sorted name records identical names would appear consecutively get scored overall similarity name address phone records sorted address address scored finally records sorted third time phone records identical phones scored approach missed record pair truly represented person none three fields matched exactly since goal prove court law persons unlikely pair would accepted judge sufficiently similar anyway 383 validating record matches whatremainsistodeterminehowhighascoreindicatesthattworecordstruly represent individual example hand easy way make decision technique applied many similar situations itwasdecidedtolook atthe creationdatesforthe recordsathand assume 90 days absolute maximum delay time service bought company registered b thus proposed match two records chosen random subject constraint date brecord 0 90 days date arecord would average delay 45 days itwasfoundthatofthepairswithaperfect300scoretheaveragedelaywas 10days ifyouassumethat300scorepairsaresurelycorrectmatchesthenyou look pool pairs given score compute average delay pairs suppose average delay x fraction true matches among pairs score f x 10f 451 f x45 35f solvingfor f find fractionofthe pairswith score truly matches 45 x35 trick used whenever 1 thereisascoringsystemusedtoevaluatethelikelihoodthattworecords represent entity 38 applications localitysensitive hashing 117 record matches good enough every case different may interest know experiment section 383 turned data section 382 scores 185 value x close 10 ie scores indicated likelihood records representing person essentially 1 note score 185 example represents situationwhere one field wouldhaveto caseor records would never even scored one field completely different andthe thirdfieldhada smalldiscrepancy moreoverforscoresaslow 115thevalueofxwasnoticeablylessthan45meaningthatsomeofthese pairs represent person note score 115 represents case one field slight similarity two fields 2 field used scoring derive measure differs averagefor true pairs false pairs forinstancesupposetherewereaheightfieldrecordedbybothcompanies b running example compute average difference heightforpairsofrandomrecordsandwecancomputetheaveragedifferencein heightforrecordsthathaveaperfectscoreandthussurelyrepresentthe entities foragivenscoreswecanevaluatetheaverageheightdifferenceofthe pairs score estimate probability records representing entity h0 average height difference perfect matches h1 average height difference random pairs h average height difference pairs score fraction good pairs score h1 hh1 h0 384 matching fingerprints fingerprints matched computer usual representation image set locations minutiae located minutia context fingerprint descriptions place something unusual happenssuchastworidgesmergingoraridgeending ifweplaceagridovera fingerprintwe representthe fingerprintby set gridsquares minutiae located ideally overlaying grid fingerprints normalized size orientation took two images finger would find minutiae lying exactly grid squares shall consider best ways normalize images let us assume combination techniquesincludingchoiceofgridsizeandplacingaminutiainseveraladjacent grid squares lies close border squares enables us assume 118 chapter 3 finding similar items grid squares two images significantly higher probability agreeing presence absence minutia images different fingers thus fingerprints represented sets grid squares theirminutiae arelocatedandcomparedlikeanysetsusingthe jaccardsim ilarity distance two versions fingerprint comparison however manyone problem one typically expect fingerprint found ona gun want compare fingerprints large database see one matches themanymanyversionoftheproblemistotaketheentiredatabaseand see pairs represent individual manymany version matches model following finding similar items technology used speed manyone problem 385 lsh family fingerprint matching could minhash sets represent fingerprint use standard lsh technique section 34 however since sets chosen relatively small set grid points perhaps 1000 need minhash succinct signatures clear shall study another form localitysensitivehashingthatworkswellfordataofthetypewearediscussing supposeforanexamplethattheprobabilityoffindingaminutiainarandom gridsquareofarandomfingerprintis20 alsoassumethatiftwofingerprints come finger one minutia given grid square theprobabilitythattheotherdoestoois80 wecandefinealocalitysensitive family hash functions follows function f family f defined three grid squares function f says yes two fingerprints minutiae three grid squares otherwise f says put another way may imagine f sends single bucket fingerprints minutiae three fs grid points sends fingerprint bucketofits inwhatfollowswe shallreferto firstofthese bucketsas bucket f ignore buckets required singletons wantto solve manyoneproblem canuse many functions thefamilyfandprecomputetheirbucketsoffingerprintstowhichtheyanswer yes given new fingerprint want match determine buckets belongs compare fingerprints found buckets solve manymany problem compute bucketsforeachofthefunctions andcompareallfingerprintsineachofthe buckets letusconsiderhowmanyfunctions weneedto getareasonableprobability ofcatchingamatchwithouthavingtocomparethefingerprintonthegunwith millions fingerprints database first probability 38 applications localitysensitive hashing 119 two fingerprints different fingers would bucket function f f 026 0000064 reason go bucket onlyiftheyeachhaveaminutiaineachofthethreegridpointsassociatedwith f probability six independent events 02 consider probability two fingerprints finger wind bucket f probability first fingerprint minutiaeineachofthethreesquaresbelongingtof is023 0008 however probability 083 0512 fingerprint well thus fingerprints finger 0008 05120004096probability bucket f much one 200 however use many functions f many get good probability matching fingerprints finger many false positives fingerprints must considered match example 323 specific example let us suppose use 1024 functions chosen randomly f next shall construct new fam ily f1 performing 1024way f probability f1 put fingerprints finger together least one bucket 1 1 00040961024 0985 hand probability two fingerprints different fingers placed bucket 1 1 00000641024 0063 get 15 false negatives abo ut 63 false positives resultofexample 323is best cando offers onlya 15chancethatweshallfailtoidentifythefingerprintonthegunitdoesforce us look 63 entire database increasing number functions f increase number false positives small benefit reducing number false negatives 15 hand also use construction greatly reduce probability false positive making small increase falsenegative rate instance could take 2048 functions f two groupsof1024 constructthebucketsforeachofthefunctions howevergiven fingerprint p gun 1 find buckets first group p belongs take union buckets 2 second group 3 take intersection two unions 4 compare p fingerprints intersection note still take unions intersections large sets finger prints compare small fraction comparison 120 chapter 3 finding similar items fingerprints takes bulk time steps 1 2 fingerprints represented integer indices database use scheme probability detecting matching fingerprint 2 0985 0970 get 3 false negatives however probability false positive 00632 000397 examine 1250thof database 386 similar news articles last case study concerns problem organizing large repository online news articles grouping together web pages derived basic text common organizations like associated press toproduceanewsitemanddistributeittomanynewspapers eachnewspaper puts story online edition surrounds information special newspaper name address newspaper links related articles links ads addition common newspapertomodify thearticleperhapsbyleavingoffthe lastfew paragraphs even deleting text middle result news article appear quite different web sites different newspapers theproblemlooksverymuchliketheonethatwassuggestedinsection34 find documents whose shingles high jaccard similarity note problemisdifferentfromtheproblemoffindingnewsarticlesthattellaboutthe sameevents thelatterproblemrequiresothertechniquestypicallyexamining set important words documents concept discussed briefly section 131and clustering group together different articles topic howeveraninterestingvariationonthe theme ofshinglingwasfoundto effective data type described problem shingling described section 32 treats parts document equally however wish ignore parts document ads headlines articles newspaper added link part news article turns noticeable difference text appears prose text appears ads headlines prose much greaterfrequencyofstopwordstheveryfrequentwordssuchastheorand total number words considered stop words varies application common use list several hundred frequent words example 324 typical ad might say simply buy sudzo hand prose version thought might appear article recommend buy sudzo laundry latter sentence would normal treat stop words suppose define shingle stop word followed next two words ad buy sudzo example 324 shingles 38 applications localitysensitive hashing 121 would reflected representation web page containing ad hand sentence example 324 would represented five shingles recommend buy buy sudzo laundry laundry x x whatever word follows sentence suppose two web pages consists half news text andhalfadsorothermaterialthathasalowdensityofstopwords ifthenews textisthesamebutthesurroundingmaterialisdifferentthenwewouldexpect large fraction shingles two pages would might jaccard similarity 75 however surrounding material news content different number common shingles would small perhaps 25 use conventional shingling shingles say sequences 10 consecutive characters would expect two documents share half shingles ie jaccard similarity 13 regardless whether news surrounding material shared 387 exercises section 38 exercise 381 suppose trying perform entity resolution among bibliographic references score pairs references based similar ities titles list authors place publication suppose also references include year publication year equally likely anyofthetenmostrecentyears furthersupposethatwediscoverthatamong pairsofreferenceswith perfect scorethere anaveragedifference publication year 017 suppose pairs references certain score found average difference publication dates 2 fraction pairs score truly represent pub lication note make mistake assuming average difference publication date random pairs 5 55 need calculate exactly enough information exercise 382 suppose use family f functions described sec tion 385 20 chance minutia grid square 80 chanceofasecondcopyofafingerprinthavingaminutiainagridsquarewhere thefirstcopydoesandeachfunctioninfbeingformedfromthreegridsquares example 323 constructed family f1 using construction 1024members suppose weinsteadused family f2 2048wayor members f compute rates false positives false negatives f2 b rates compare get organize 2048 functions 2way members f1 discussed end section 385 7wemightexpecttheaveragetobe0butinpracticeerrorsinpublicationyeardooccur 122 chapter 3 finding similar items exercise 383 suppose fingerprints statistics outlined ex ercise 382 use base family functions f defined like f using two randomly chosen grid squares construct another set functions f 1 f taking nway functions f function n false positive false negative rates f 1 exercise 384 suppose use functions f1 example 323 want solve manymany problem two fingerprints finger probability compared ie false negative rate b fraction fingerprints different fingers compared ie false positive rate exercise 385 assume set functions f exercise 382 construct new set functions f3 nway functions f value n sum false positive false negative rates minimized 39 methods high degrees similarity lshbased methods appear effective degree similarity accept relatively low want find sets almost identical thereareothermethodsthatcanbefaster moreoverthesemethodsareexact inthattheyfindeverypairofitemswiththedesireddegreeofsimilarity false negatives lsh 391 finding identical items extreme case finding identical items example web pages identical characterforcharacter straightforward compare two docu ments tell whether identical still must avoid compare every pair documents first thought would hash docu ments based first characters compare documents fell bucket scheme work well unless documents begin characters html header second thought would use hash function examines entire document would work use enough buckets would veryrarethattwodocumentswentintothesamebucketyetwerenotidentical thedownsideofthisapproachisthatwemustexamineeverycharacterofevery document limit examination small number characters never examine document unique falls bucket abetterapproachistopicksomefixedrandompositionsforalldocuments make hash function depend way avoid 39 methods high degrees similarity 123 problem common prefix documents yet need examine entire documents unless fall bucket another document oneproblemwithselectingfixedpositionsisthatifsomedocuments areshortthey maynothavesome ofthe selectedpositions howeverifwe lookingforhighlysimilardocumentsweneverneedtocomparetwodocuments differ significantly length exploit idea section 393 392 representing sets strings nowlet us focus onthe harder problemoffinding largecollectionofsets pairs high jaccardsimilarity say least 09 represent set sorting elements universal set fixed order representing set listing elements order list essentially string characterswhere characters elements universal set strings unusual howeverin 1 character appears string 2 two characters appear two different strings appear order strings example 325 supposetheuniversalsetconsistsofthe26lowercaseletters use normal alphabetical order set dab represented string abd inwhatfollowsweshallassumeallstringsrepresentsetsinthemannerjust described thus shall talk jaccard similarity strings strictly speaking mean similarity sets strings represent also shall talk length string surrogate number elements set string represents note documents discussed section 391 exactly match model even though see documents strings fit model would shingle documents assignan orderto shingles andrepresent document list shingles selected order 393 lengthbased filtering simplest way exploit string representationof section 392 sort stringsbylength eachstrings iscomparedwiththosestringst follow list long suppose lower bound jaccard similarity two strings j string x denote length l x note l l intersection sets represented cannot havemorethan l memberswhile unionhas atleastl members thus jaccard similarity denote simst l sl orderfor require comparisonit must j l l equivalently l l j 124 chapter 3 finding similar items better ordering symbols instead using obvious order elements universal set eg lexicographic order shingles order symbols rarest first determine many times element appears collection sets order count lowest first advantage symbols prefixes tend rare thus cause string placed index buckets relatively members need examine string possible matches shall find strings candidates comparison example 326 suppose string length 9 looking strings least 09 jaccard similarity compare strings following lengthbased sorted order length 909 10 compare strings length 9 follow order strings length 10 need compare string suppose length 8 instead would compared following strings length 809 889 string length 9 wouldbe toolongto haveajaccardsimilarityof09withs soweonly haveto compare strings length 8 follow sortedorder 394 prefix indexing addition length several features strings exploited limit number comparisons must made identify pairs similar strings simplest options create index symbol recall symbol string one elements universal set string select prefix consisting first p symbols large p must depends l j lower bound jaccard similarity add string index first p symbols ineffecttheindexforeachsymbolbecomesabucketofstringsthatmustbe compared must certain string simst j least one symbol prefix also appears prefix suppose rather simst j none first p symbols highest jaccard similarity occurs suffix consisting everything first p symbols jaccard similarity would l pl sure compare must certain j l pl p must least 1 jl 1 course want p small possible index string buckets need thus shall hereafter take p 1 jl 1 length prefix 39 methods high degrees similarity 125 gets indexed example 327 suppose j 09 l 9 p 01 9 1 09 1 1 need index first symbol string first symbol position indexedbythatsymbolwillhavejaccardsimilaritywithsthatislessthan09 suppose bcdefghij indexed b suppose begin b two cases consider 1 begins simst 09 abcdefghij case indexed b reason l 10 indexed symbols prefix length 01 10 12 2 begins c later letter maximum value simst occurs cdefghij simst8909 general j 09 strings length 9 indexed first symbol strings lengths 1019 indexed first two symbols strings length 2029 indexed first three symbols use indexing scheme two ways depending whether trying solve manymany problem manyone problem recall distinction introduced section 384 manyone problem create index entire database query matches new set convert set string call probe string determine length prefix must considered 1 jl 1 symbol appearing one prefix positions look index bucketforthatsymbolandwecompareswithallthestringsappearinginthat bucket want solve manymany problem start empty database ofstringsandindexes foreachsetswetreats asanewsetforthemanyone problem convert string treat probe string manyone problem however examine index bucket also add bucket comparedwith later strings could matches 395 using position information considerthestringssacdefghijkandtbcdefghijkandassumej 09 since strings length 10 indexed first two symbols thus indexed c indexed b c whicheverisaddedlastwillfindthe otherinthe bucketforcandthey willbe compared however since c second symbol know two symbols b case union two sets intersection indeed even thoughs identical c 126 chapter 3 finding similar items end theirintersectionis 9symbolsandtheir unionis11thus simst911 less 09 build index based symbol position symbol within string could avoid comparing let index bucket pair xi containing strings symbol x position prefix given string assuming j minimum desired jaccardsimilarity look prefix positions 1 1 jl 1 symbol position prefix x add index bucket xi nowconsiders asa probestring withwhatbucketsmustitbe compared shall visit symbols prefix left shall take advantage fact need find possible matching string none previousbuckets haveexamined matches held thatis need find candidate match thus find ith symbol x need look bucket xj certain small values j symbols definitely appearing one string j figure 315 strings andt begin 1 andj 1 unique symbols respec tively agree beyond compute upper bound j suppose string none whose first j 1symbolsmatchedanythinginsbutthe ithsymbolofsisthe sameasthe jth symbol highest value simst occurs identical beyond ith jth symbols respectively suggested fig 315 case size intersection l i1 since number symbols could possibly size union least l j 1 surely contributes l symbols union therearealsoatleastj 1symbolsoftthatarenotins theratioofthe sizes intersection union must least j must l i1 j l j 1 isolate j inequality j l 1 j i1j j cid0 cid1 example 328 consider string acdefghijk j 09 discussed beginning section suppose probe string already established need consider first two positions 1 39 methods high degrees similarity 127 2 suppose 1 j 10 01 110909 compare symbol strings bucket aj j 211 thus j 1 2 nothing higher suppose 2 require j 10 01 210909 j 1 weconcludethatwemustlookinthebucketsfora1a2andc1 bucket comparison using buckets section 394 would look buckets c equivalent looking buckets aj cj j 396 using position length indexes considered upper limit j previous section assumed follows positions j fig 315 followed positions strings matched exactly wantto build index involves every symbol strings makes total work excessive however add index summary follows positions indexed expands number buckets beyond reasonable bounds yet enables us eliminate many candidate matches without comparing entire strings idea use index buckets correspondingtoasymbolapositionandthesuffixlengththatisthenumber symbols following position question example 329 string acdefghijkwith j 09 would indexed inthebucketsfora19andc28 thatisthefirstpositionofshassymbol suffix length 9 second position symbol c suffix length 8 figure 315 assumes suffixes position position j length either get smaller upper bound size intersection shorter larger lower bound size union longer suppose suffix length p suffix length q case 1 p q maximum size intersection l i1 p q since l ip write expression intersection size q1 minimum size union l j 1 take suffix length account thus require q1 j l j 1 whenever p q 128 chapter 3 finding similar items case 2 p q maximum size intersection l i1 whensuffixlengthwasnotconsidered howevertheminimumsizeoftheunion l j 1q p use relationship l ip replace l p get formula ij 1q size union jaccard similarity least j l i1 j ij 1q whenever pq example 330 letusagainconsiderthestringsacdefghijkbuttomake example show details let us choose j 08 instead 09 know l 10 since 1 jl 1 3 must consider prefix positions i1 2 3 follows let p suffix length q suffix length first consider case p q additional constraint q j q19j 08 enumerate pairs values j q 1 3 follows i1 p9 q 9 let us consider possible values q q 9 must 109j 08 thus j 1 j 2 j 3 note j 4 101308 q 8 must 99j 08 thus j 1 j 2 j 3 91208 q 7 musthave89j 08 j 1 satisfiesthis inequality q 6 possible values j since 79j 08 every positive integer j holds every smaller value q i2 herep8sowerequireq 8 sincetheconstraintq19j 08 depend i8 ca n use analysis case exclude case q 9 thus possible values j q i2 1 q 8 j 1 2 q 8 j 2 3 q 7 j 1 i3 p7 andthe constraintsareq 7 andq19j 08 option q 7 j 1 next must consider case pq additional constraint 11 08 ijq 1 consider possible value 8notethatidoesinfluencethevalueofpandthroughpputsalimitonq 39 methods high degrees similarity 129 i1 p 9 require q 10 10qj 08 possible values q j 1 q 10 j 1 2 q 10 j 2 3 q 11 j 1 i2 p 8 require q 9 9qj1 08 since j must positive integer solution q 9 j 1 possibility already knew i3 p 7 require q 8 8qj2 08 solutions q j 1 j 2 j 3 7 x 8 x x i1 9 x x x 10 x x 11 x 7 x i2 8 x x 9 x i3 7 x figure 316 buckets must examined find possible matches string acdefghijkwith j 08 marked x accumulate possible combinations j q see set index buckets must look forms pyramid figure 316 shows buckets must search must look buckets xjq ith symbol string x j position associated bucket q suffix length 397 exercises section 39 exercise 391 suppose universal set lowercase letters orderofelementsistakento vowelsinalphabeticorderfollowedby consonants reversealphabetic order represent following sets strings qwerty b asdfghjui 130 chapter 3 finding similar items exercise 392 suppose filter candidate pairs based length section393 ifsisastringoflength20withwhatstringsisscomparedwhen j lowerboundonjaccardsimilarityhas followingvalues j 085 b j 095 c j 098 exercise 393 supposewehaveastringsoflength15andwewishtoindex prefix section 394 many positions prefix j 085 b many positions prefix j 095 c forwhatrangeofvaluesofj willsbeindexedunderitsfirstfoursymbols exercise 394 supposesisastringoflength12 withwhatsymbolposition pairs compared use indexing approachof section 395 j 075 b j 095 exercise 395 suppose use position information index sec tion 395 strings chosen random universal set 100 elements assume j 09 probability compared length 9 b length 10 exercise 396 suppose use indexes based position suffix length section 396 string length 20 symbol positionlength triples compared j 08 b j 09 310 summary chapter 3 jaccard similarity jaccard similarity sets ratio size intersection sets size union measure similarityissuitableformanyapplicationsincludingtextualsimilarityof documents similarity buying habits customers shingling kshingle k characters appear consecutively document represent document set kshingles jaccard similarity shingle sets measures textual similarity documents sometimes useful hash shingles bit strings shorter length use sets hash values represent documents minhashing aminhashfunctiononsetsisbasedonapermutationofthe universalset givenanysuchpermutation minhashvalue setis element set appears first permuted order 310 summary chapter 3 131 minhash signatures may represent sets picking list per mutationsandcomputing foreachsetits minhashsignaturewhichis sequenceofminhashvaluesobtainedbyapplyingeachpermutationonthe listtothatset giventwosetsthe expectedfractionofthe permutations yield minhash value exactly jaccard similarity sets efficient minhashing since really possible generate random permutationsitisnormaltosimulateapermutationbypickingarandom hashfunction andtaking minhash value set leasthash value sets members additional efficiency restricting search smallest minhash value small subset universal set localitysensitive hashing signatures technique allows us avoid computing similarity every pair sets minhash sig natures given signatures sets may divide bandsand measure similarity ofa pair sets identi cal least one band choosing size bands appropriatelywe eliminate consideration pairs meet threshold similarity distance measures adistancemeasureisafunctiononpairsofpointsin aspacethatsatisfycertainaxioms thedistancebetweentwopointsis0if thepointsarethesamebutgreaterthan0ifthepointsaredifferent distance symmetric matter order consider two points distance measure must satisfy triangle inequality distance two points never sum distances points third point euclideandistance themostcommonnotionofdistanceistheeuclidean distance ndimensional space distance sometimes called l2norm square root sum squares differences points dimension another distance suitable eu clidean spaces called manhattan distance l1norm sum magnitudes differences points dimension jaccarddistance oneminusthejaccardsimilarityisadistancemeasure called jaccard distance cosinedistance theanglebetweenvectorsinavectorspaceisthecosine distancemeasure cancomputethe cosineofthatangleby takingthe dot product vectors dividing lengths vectors edit distance distance measure applies space strings number insertions andor deletions needed convert one string edit distance also computed sum 132 chapter 3 finding similar items lengths strings minus twice length longest common subsequence strings hamming distance distance measure applies space vectors thehamming distancebetweentwovectorsis numberofpositionsin vectors differ generalized localitysensitive hashing maystartwithanycollection functions minhash functions render decision whether pair items candidates similarity checking constraint functions provide lower bound probability saying yes distance according somedistance measureis agivenlimit andanupper boundon theprobabilityofsayingyesifthedistanceisaboveanothergivenlimit wecanthenincreasetheprobabilityofsayingyesfornearbyitemsand atthesametimedecreasetheprobabilityofsayingyesfordistantitems great extent wish applying construction construction random hyperplanes lsh cosine distance get set basisfunctionstostartageneralizedlshforthecosinedistancemeasure identifying function list randomly chosen vectors apply function given vector v taking dot product v eachvectoronthelist theresultisasketchconsistingofthesigns1or 1ofthedotproducts thefractionofpositionsinwhichthesketchesof two vectors agree multiplied 180 estimate angle two vectors lsh euclidean distance set basis functions start lsh euclideandistancecanbeobtainedbychoosingrandomlinesandproject ingpointsontothoselines eachlineisbrokenintofixedlengthintervals function answers yes pair points fall interval highsimilaritydetectionbystringcomparison analternativeapproach tofindingsimilaritemswhenthethresholdofjaccardsimilarityiscloseto 1avoidsusingminhashingandlshrathertheuniversalsetisordered sets represented strings consisting elements order thesimplestwaytoavoidcomparingallpairsofsetsortheirstringsisto note highly similar sets strings approximately length sort strings compare string small number immediately following strings character indexes represent sets strings similarity thresholdiscloseto1wecanindexallstringsbytheirfirstfewcharacters prefixwhosecharactersmustbe indexedisapproximatelythe length ofthestringtimesthemaximumjaccarddistance1minustheminimum jaccardsimilarity 311 references chapter 3 133 position indexes index strings characters prefixesbut onthe positionofthat characterwithin prefix reduce number pairs strings must compared two strings share character first position stringsthenweknowthateithertherearesomeprecedingcharactersthat areintheunionbutnottheintersectionorthereisanearliersymbolthat appears strings suffixindexes wecanalsoindexstringsbasednotonlyonthecharacters prefixes positions characters length characters suffix number positions follow string structure reduces number pairs must comparedbecauseacommonsymbolwithdifferentsuffix lengthsimplies additionalcharactersthatmustbeintheunionbutnotintheintersection 311 references chapter 3 technique called shingling attributed 11 use manner discussed 2 minhashing comes 3 improvement avoids looking elements 10 original works localitysensitive hashing 9 7 1 useful summary ideas field 4 introduces idea using randomhyperplanes summarize items way respects cosine distance 8 suggests random hyperplanes pluslshcanbemoreaccurateatdetectingsimilardocumentsthanminhashing plus lsh techniques summarizing points euclidean space covered 6 12 presented shingling technique based stop words length prefixbased indexing schemes highsimilarity matching comes 5 technique involving suffix length 13 1 andoni p indyk nearoptimal hashing algorithms approxi mate nearest neighbor high dimensions comm acm 511 pp 117 122 2008 2 azbroderontheresemblanceandcontainmentofdocumentsproc compression complexity sequences pp 2129 positano italy 1997 3 azbrodermcharikaramfriezeandmmitzenmacherminwise independent permutations acm symposium theory computing pp 3273361998 4 ms charikar similarity estimation techniques rounding algo rithms acm symposium theory computing pp 3803882002 134 chapter 3 finding similar items 5 chaudhuri v ganti r kaushik primitive operator sim ilarity joins data cleaning proc intl conf data engineering 2006 6 mdatarn immorlicapindyk andvs mirroknilocalitysensitive hashing scheme based pstable distributions symposium compu tational geometry pp 2532622004 7 gionis p indyk r motwani similarity search high dimen sions via hashing proc intl conf large databases pp 518 529 1999 8 mhenzingerfindingnearduplicatewebpages alargescaleevaluation algorithms proc 29th sigir conf pp 2842912006 9 p indyk r motwani approximate nearest neighbor towards movingthecurseofdimensionalityacmsymposiumontheoryofcom puting pp 6046131998 10 p li ab owen ch zhang one permutation hashing conf neural information processing systems 2012 pp 31223130 11 u manber finding similar files large file system proc usenix conference pp 110 1994 12 theobald j siddharth paepcke spotsigs robust effi cientnearduplicatedetectioninlargewebcollections31stannualacm sigir conference july 2008 singapore 13 c xiao w wang x lin jx yu efficient similarity joins near duplicate detection proc www conference pp 1311402008

1 survey concept drift adaptation joaogamauniversityofportoportugal indre zˇliobaite aaltouniversityfinland albertbifetyahooresearchbarcelonaspain mykolapechenizkiyeindhovenuniversityoftechnologythenetherlands abdelhamidbouchachiabournemouthuniversityuk conceptdriftprimarilyreferstoanonlinesupervisedlearningscenariowhentherelationbetweentheinput dataandthetargetvariablechangesovertimeassumingageneralknowledgeofsupervisedlearningin paper characterize adaptive learning process categorize existing strategies handling concept driftoverviewthemostrepresentativedistinctandpopulartechniquesandalgorithmsdiscussevaluation methodology adaptive algorithms present set illustrative applications survey covers different facets concept drift integrated way reflect existing scattered stateoftheart thus aims providing comprehensive introduction concept drift adaptation researchers industryanalystsandpractitioners categoriesandsubjectdescriptorsi26artificialintelligencelearning generaltermsdesignalgorithmsperformance additionalkeywordsandphrasesconceptdriftchangedetectionadaptivelearning acmreferenceformat gamajzˇliobaiteibifetapechenizkiymandbouchachiaa2013asurveyonconceptdriftadap tationacmcomputsurv11article1january201335pages doi10114500000000000000httpdoiacmorg10114500000000000000 1 introduction digital universe rapidly growing volume data generated 2012 estimated surpass 28 zetabytes 28 trillion gigabytes reported idc surveygantzandreinsel2012efficientandeffectivetoolsandanalysismethodsfor dealing evergrowing amount data different applications fields paramount need traditionally data mining already collected data processed offline mode instance predictive models trained using historical data givenasasetofpairsinputoutputmodelstrainedinsuchawaycanbeafterwards appliedforpredictingtheoutputfornewunseeninputdata however often data comes form streams accommodating large vol umesofstreamingdatainthemachinesmainmemoryisimpracticalandofteninfea sible hence online processing suitable case predictive models trained either incrementally continuous update retraining using recent batches data efficiency issue supervising learning data steams dynamically changing nonstationary environments data distribution change time yielding phenomenon concept drift schlimmer granger 1986 widmer kubat 1996 real concept drift1 refers changes conditional distribution output ie target variable given input input features distribution input may stay unchanged typical example real concept drift change users interests following online news stream whilst distribution incoming news documents often remains conditional distribution interesting thus interesting news 1thetermrealreferstooneparticulartypeofconceptdriftitdoesntmeanthatothertypesofdriftarenot conceptdrifts acmcomputingsurveysvol1no1article1publicationdatejanuary2013 12 jgamaetal documentsforthatuserchangesadaptivelearningreferstoupdatingpredictivemod elsonlineduringtheiroperationtoreacttoconceptdrifts last decade research related learning concept drift creasinglygrowingandmanydriftawareadaptivelearningalgorithmshavebeende veloped spite popularity research topic comprehensive survey conceptdrifthandlingtechniquesisavailabletothecommunityoneofthereasonsfor problem wide scope spans across different research fields moreover terminology well established thus similar adaptive learning strate gieshavebeendevelopedindependentlyunderdifferentnamesindifferentcontexts takingaccountofthecurrentpictureofresearchonconceptdriftbeingverypopular butalsoscatteredamongvariouscommunitiesthereisastrongneedforacomprehen sivesummaryoftheresearchdonesofartounifytheconceptsandterminologyamong theresearchersandtosurveythestateoftheartmethodologiesandtechniquesinves tigatedoverthepast several reviews related driftaware learning available however ei ther focus exclusively concept drift relate specific topics adaptive learningthusthesereviewsarefragmentedandorareoutdatedcurrentlythemost cited survey concept drift published back 2004 tsymbal 2004 fol lowing overviews related topic concept drift focused ensem ble techniques kuncheva 2004 2008 inductive rule learning algorithms maloof 2010ormainlyonnonincrementallearningtechniqueszliobaite2009thatcanuse computational resources unrestrictedly thus limited scope reviews data streams gaber et al 2005 gama 2010 bifet et al 2011a partially deal datadriftdatastreamsresearchcoversadaptivelearningonlytosomeextentwhile main focus remains making learning algorithms incremental optimizing thebalanceofcomputationalresourcesandthepredictiveaccuracy several reviews limited specific application fields focused position pa pergrisogono2006presentsasetofrequirementsforcomplexadaptivesystemstobe usedfordefencearecentfocusedreviewkadlecetal2011surveysadaptationmech anisms used soft sensors finally recent article morenotorres etal2012focusesondescribingvariouswayshowdatadistributioncanchangeover timeandonlybrieflycoversadaptationtechniquesfromdatasetshiftcommunityper spectivemostlyleavingoutworksonconceptdriftarecentreviewalbergetal2012 focusesondecisiontrees present contribution provides integrated view handling concept drift surveying adaptive learning methods presenting evaluation methodologies dis cussing illustrative applications focuses online supervised learning relationbetweentheinputfeaturesandthetargetvariablechangesovertime paper organized follows section 2 introduce problem con cept drift characterize adaptive learning algorithms present motivating applica tionexamplessection3presentsacomprehensivetaxonomyofmethodsforadaptive learningsection4discussestheexperimentalsettingsandevaluationmethodologies ofadaptivelearningalgorithmssection5concludesthesurvey 2 adaptivelearningalgorithms learningalgorithmsoftenneedtooperateindynamicenvironmentswhicharechang ingunexpectedlyonedesirablepropertyofthesealgorithmsistheirabilityofincorpo ratingnewdataifthedatageneratingprocessisnotstrictlystationaryasappliesto real world applications underlying concept predicting forexampleinterestsofauserreadingnewsmaybechangingovertimetheability adapt concept drift seen natural extension incremen tal learning systems giraudcarrier 2000 learn predictive models example acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 13 exampleadaptivelearningalgorithmscanbeseenasadvancedincrementallearning algorithmsthatareabletoadapttoevolutionofthedatageneratingprocessovertime thissectionintroducesconceptdriftandcharacterizesadaptivelearning 21 settinganddefinitions machine learning supervised learning problem formally defined follows aim predict target variable cid601 regression tasks categorical classification tasks given set input features x cid60p example one pair xyforinstancex isasetofsensorreadingsofachemicalprocessat2pmonthe 2nd ofjanuaryandy goodcid48cid48 isthetruequalityoftheproducedproductatthattime training examples used model building x known new examples predictive model applied x known notknownatthetimeofprediction according bayesian decision theory duda et al 2001 classification described prior probabilities classes py class conditional probability density functions pxy classes 1c c number classes classification decision made according posterior probabilities oftheclasseswhichforclassy canberepresentedas pypxy pyx 1 px wherepxcid80c pypxyhereequalcostsofmisclassificationareassumed y1 thetypeofthetargetvariablespacedependsonthetaskinclassificationthetarget variabletakescategoricalvaluesclasslabelswhileinregressionthetargetvariable takescontinuousvalues distinguish two learning modes offline learning online learning flinelearningthewholetrainingdatamustbeavailableatthetimeofmodeltraining training completed model used predicting contrast onlinealgorithmsprocessdatasequentiallytheyproduceamodelandputitinoper ation without complete training data set available beginning modeliscontinuouslyupdatedduringoperationasmoretrainingdataarrives less restrictive online algorithms incremental algorithms process put examples onebyone batchbybatch update decision model ceivingeachexampleincrementalalgorithmsmayhaverandomaccesstopreviousex amplesorrepresentativeselectedexamplesinsuchacasethesealgorithmsarecalled incremental algorithms partial memory maloof michalski 2004 typically incremental algorithms new presentation data update operation model based previous one streaming algorithms online algorithms processing highspeed continuous flows data streaming examples pro cessed sequentially well examined passes typically onethesealgorithmsuselimitedmemoryandlimitedprocessingtimeperitem setting considering data arrives online often real time form ingastreamwhichispotentiallyinfinitethemachineryisgiveninputdatathathas justarrivedwiththegoalofpredictingthevalueofitstargetvariablesthemachin ery must construct predictive model mapping function input feature space corresponding output target space instance given sensor readingsinachemicalproductionprocessthetaskistopredictthequalityoftheprod uctoutput becausedataisexpectedtoevolveovertimeespeciallyindynamicallychangingen vironments nonstationarity typical underlying distribution change dynamically time general assumption concept drift setting acmcomputingsurveysvol1no1article1publicationdatejanuary2013 14 jgamaetal change happens unexpectedly unpredictable although particular realworld situations change known ahead time correlation occurrence particular environmental events solutions general case drift entail solutions particular cases moreover change may take dif ferentformsietheinputdatacharacteristicsortherelationbetweentheinputdata andthetargetvariablemaychange formallyconceptdriftbetweentimepointt andtimepointt canbedefinedas 0 1 x p xycid54p xy 2 t0 t1 p denotes joint distribution time set input variables t0 0 x target variable changes data characterized changes componentsofthisrelationkellyetal1999gaoetal2007inotherterms thepriorprobabilitiesofclassespymaychange theclassconditionalprobabilitiespxymaychangeand result posterior probabilities classes pyx may change affecting prediction interested know two implications changes whether data distribution pyx changes affects predictive decision ii whether changes visible data distribution without knowing true labels ie pxchangesfromapredictiveperspectiveonlythechangesthataffecttheprediction decisionrequireadaptation wecandistinguishtwotypesofdrifts 1 realconceptdriftreferstochangesinpyxsuchchangescanhappeneitherwith without change px real concept drift referred concept shift insalganicoff1997andconditionalchangeingaoetal2007 2 virtual drift happens distribution incoming data changes ie px changes without affecting pyx delany et al 2005 tsymbal 2004 widmer kubat 1993 however virtual drift different interpretations litera ture originally virtual drift defined widmer kubat 1993 occur duetoincompletedatarepresentationratherthanchangeinconceptsinreality virtualdriftcorrespondstochangeindatadistributionthatleadstochangesin thedecisionboundarytsymbal2004 virtualdriftisadriftthatdoesnotaffectthetargetconceptdelanyetal2005 virtual drift also referred temporary drift lazarescu et al 2004samplingshiftsalganicoff1997andfeaturechangegaoetal2007 inthispapervirtualdriftreferstochangeinthedatadistributionpx2 exampleconsideranonlinenewsstreamofarticlesonrealestatethetaskfora givenuseristoclassifytheincomingnewsintorelevantandnotrelevantsupposethat theuserissearchingforanewapartmentthennewsondwellinghousesarerelevant whereas holiday homes relevant editor news portal changes writing style changes well dwelling houses remain relevant user scenario corresponds virtual drift due crisis articles dwelling houses come less articles holiday homes editor writing styleandtheinterestsoftheuserremainthesamethissituationcorrespondstodrift prior probabilities classes hand user bought house 2thetermpopulationdriftissometimesalsousedwhichreferstochangesinthepopulationfromwhich future samples drawn compared population designtraining sample drawnkellyetal1999itisageneraltermcoveringanytypeofdrift acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 15 starts looking holiday destination dwelling houses become relevant holidayhomesbecomerelevantthisscenariocorrespondstotherealconceptdriftin case writing style prior probabilities stay may happen thatalltypesofdriftsmaytakeplaceatthesametime figure1illustratesthetypesofdriftstheplotshowsthatonlytherealconceptdrift changestheclassboundaryandthepreviousdecisionmodelbecomesobsoleteinprac tice virtualdrift changing priorprobabilities noveltiesmay appear incombination withtherealdriftinthesecasestheclassboundaryisalsoaffected original data real concept drift virtual drift pyx changes px changes pyx fig1 typesofdriftscirclesrepresentinstancesdifferentcolorsrepresentdifferentclasses thissurveyprimarilyfocusesonhandlingtherealconceptdriftwhichisnotvisible input data distribution many cases techniques handle real concept drift also handle drifts manifest input data distributions nottheotherwayaroundtechniquesforhandlingtherealconceptdrifttypicallyrely onfeedbackaboutthepredictiveperformancewhiletechniquesfortrackingchanging prior probabilities techniques handling virtual drift novelty detection typically operate without feedback paper cover drifts detected incoming data distribution px readers interested tracking driftingpriorprobabilitiesarereferredtozhangandzhou2010innoveltydetection arereferredtomarkouandsingh2003masudetal2011inhandlingvirtualdrifts bysemisupervisedlearningtechniquesusingonclusteringarereferredtoaggarwal 2005bouchachiaetal2010bouchachiaandvanaret2013 22 changesindataovertime changesindatadistributionovertimemaymanifestindifferentformsasillustrated figure 2 toy onedimensional data data changes happen data meanadriftmayhappensuddenlyabruptlybyswitchingfromoneconcepttoanother time naem atad suddenabrupt incremental gradual reoccuring concepts outlier concept drift fig2 patternsofchangesovertimeoutlierisnotconceptdrift eg replacement sensor another sensor different calibration chemicalplantorincrementallyconsistingonmanyintermediateconceptsinbetween egasensorslowlywearsoffandbecomeslessaccuratedriftmayhappensuddenly egthetopicsofinterestthatoneissurveyingasacreditanalystmaysuddenlyswitch instance meat prices public transportation gradually eg relevant news topics change dwelling holiday homes user switch abruptly rather keeps going back previous interest time one challenges concept drift handling algorithms mix true drift outlier noise refers onceoff random deviation anomaly see acmcomputingsurveysvol1no1article1publicationdatejanuary2013 16 jgamaetal chandolaetal2009foroutlierdetectionnoadaptivityisneededinthelattercase finally drifts may introduce new concepts seen previously seen concepts may reoccur time eg fashion changes characterizedbyseveritypredictabilityandfrequencyminkuetal2010kosinaetal 2010 adaptive learning techniques implicitly explicitly assume specialize insomesubsetofconceptdriftsmanyofthemassumesuddennonreoccurringdrifts butinrealityoftenmixturesofmanytypescanbeobserved 23 requirementsforpredictivemodelsinchangingenvironments predictive models operate settings need mechanisms detect adapt evolving data time otherwise accuracy degrade time passesthedecisionmodelmayneedtobeupdatedtakingintoaccountthenewdataor getscompletelyreplacedtomeetthechangedsituationpredictivemodelsarerequired 1 detecttoconceptdriftandadaptifneededassoonaspossible 2 distinguishdriftsfromnoiseandbeadaptivetochangesbutrobusttonoise 3 operateinlessthanexamplearrivaltimeandusenotmorethanafixedamountof memoryforanystorage 24 onlineadaptivelearningprocedure theonlineadaptivelearningisformallydefinedasfollowsadecisionmodelisafunc tion l maps input variables target lx learning algorithm specifieshowtobuildamodelfromasetofdatainstances onlineadaptivelearningprocedureisthefollowing 1 predict new example x arrives prediction yˆ made using current modell 2 diagnoseaftersometimewereceivethetruelabely andcanestimatethelossas fyˆy 3 updatewecanusetheexamplex formodelupdatetoobtainl t1 depending computational resources data may need discarded processed using latest version model l t1 trainx l alternatively past data may remain accessible l trainx x l different ways handling data t1 online eg partial memory examples stored used regularly training windowbased data presented chunks instancebased example processed upon arrival details various schemes related data presentationwillfollowinsection3 updating model new example x arrives loop receiving t1 predictingfeedbackmodel update continues infinitely time steps one may choosetopreservethecurrentmodell l t1 figure 3 depicts generic schema online adaptive learning algorithm nutshellthememorymoduledefineshowandwhichdataispresentedtothelearning algorithmlearningmodulethelossestimationmoduletrackstheperformanceofthe learning algorithm sends information change detection module update themodelifnecessarysection3willdiscussthefourmodulesofthesystemmemory learningchangedetectionlossestimationindetail setting variations instance true values target vari ablefeedbackcomewithadelayorarenotavailableatallmoreovernewexamples prediction may arrive get feedback data already acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 17 input system output 1memory 1 data learning prediction 3 prediction part feedback 2 2 3 3 diagnostic delay part loss change alarm estimation 2 detection 2 1 predict mandatory 2 diagnose optional 3 update fig3 agenericschemaforanonlineadaptivelearningalgorithm processed case model update would delayed principles oper ation remain finally settings may need process examples batchesratherthanonebyone 25 illustrativeapplications problem concept drift recognized addressed multiple domains application areas including medicine kukar 2003 tsymbal et al 2006 indus try pechenizkiy et al 2009 education castillo et al 2003 business klinken berg2003applicationsrequiringadaptationcanbegroupedintofourcategories monitoringandcontrol category includes detection anomalous behavior adversaryactivitiesonthewebcomputernetworkstelecommunicationsfinancial transactions application areas abnormal behavior needs signaledanditisoftenformulatedasadetectiontask managementandstrategicplanning category includes predictive analytics tasks evaluation creditworthiness demand prediction food sales bus traveltimepredictioncrimepredictionbyregion personalassistanceandinformation category includes recommender systems categorization organization textual information customer profiling mar ketingpersonalmailcategorizationandspamfiltering ubiquitousenvironmentapplications thiscategoryincludesawidespectrumofmov ing stationary systems interact changing environments stancemovingrobotsmobilevehiclessmarthouseholdappliances next discuss motivating application cases within category illustrate demandofadaptivelearningsystemsthatcanhandleconceptdrift 251 monitoringandcontrol monitoring control applications data often pre sented form time series two typical learning tasks timeseries fore castingregressiontaskoranomalydetectionclassificationtask onlinemassflowpredictioninanindustrialboilerpechenizkiyetal2009isanex ample applicationin themonitoring andcontrol categorymass flow predictionwould helptoimproveoperationandcontroloftheboilerinsteadyoperationcombustionis affectedbythedisturbancesinthefeedrateofthefuelandbytheincompletemixingof thefuelinthebedknowingthemassflowisimportantforboilercontrolthesystem takes fuel continuously mixed inside transferred container boiler scaling sensors located container provide streaming data task istopredictestimatethemassflowinrealtime concept drift happens due following reasons fuel feeding manual non standardized process necessarily smooth short inter acmcomputingsurveysvol1no1article1publicationdatejanuary2013 18 jgamaetal ruptionseachoperatorcanhavedifferenthabitstheprocesscharacteristicsmayde pendonthetypeandthequalityoffuelusedthemainfocusforanadaptivelearning algorithm handle two types changes abrupt change feeding slower butstillabruptswitchtoburningonechallengeforlearningisthatthefeedbackthe ground truth mass flow available approximately esti mated retrospectively inspecting historical data additional challenge dealwithspecificonesidedoutliersthatcanbeeasilymistakenforchanges traditionalapproachessuchasadwinseesection323forexplicitchangedetec tionbasedonthemonitoringoftherawsensorsignalorstreamingerroroftheregres sorsgivereasonableresultstheycanbeimprovedbyconsideringthepeculiaritiesof theapplication 252 managementandstrategicplanning thesmartgridsgisanelectricsystemthat usestwowaydigitalinformationcybersecurecommunicationtechnologiesandcom putational intelligence integrated fashion across heterogeneous distributed electricity generation transmission distribution consumption achieve energy efficiency key novel characteristic sgs intelligent layer analyzes thedataproducedbysmartmetersallowingcompaniestodeveloppowerfulnewcapa bilities terms grid management planning customer services energy effi ciencytheadventofsgshaschangedthewayenergyisproducedpricedandbilled key aspect sgs distributed energy production namely renewable energies penetration renewable energies solar wind etc increasing fast power forecastingbecomesanimportantfactorindefiningtheoperationplanningpoliciesto beadoptedbyatransmissionsystemoperator observing literature wind power prediction monteiro et al 2009 one realizes proposals based offline training mode building static model used produce predictions option rely assump tions stationarity wind electric power model must strongly ques tioned bremnes 2004 bessa et al 2009 using real data three distinct wind parksbessaetal2009presentsthemeritsofonlinetrainingagainstofflinetrain ing neural networks authors point evolving nature data presenceofconceptdriftinwindpatternbehavior 253 personalassistanceandinformation textclassificationhasbeenapopulartopicin machinelearningfordecadeshoweverinterestingapplicationsrelatedtotheproblem concept drift appeared relatively recently examples text stream applications cludeemailclassificationcarmonacejudoetal2010emailspamdetectionlind strometal2010andsentimentclassificationbifetandfrank2010sentimentclas sificationisapopulartaskinsocialmediamonitoringcustomerfeedbackanalysisand otherapplications themainsourceofconceptdriftinemailclassificationandspamfilteringaredueto changingemailcontentandpresentationvirtualdriftaswellasadaptivebehaviour spammers trying overcome spam filters may virtual real besides users maychangetheirattitudetowardsparticularcategoriesofemailsstartingorstopping consider spam real drift sentiment classification vocabulary used express positive negative sentiments may change time since collection documentsis staticvirtual drift noveltiesthe feature space representing currentcollectionisdynamicthatmayrequirespecificupdatesofthemodels variousadaptivelearningstrategieshavebeenusedinthisdomainincludingindi vidual methods like casebased reasoning ensembles either evolving explicitdetectionofchangesbymeansofchangedetectorssection32 availability feedback serious challenge personal assistance informa tion dilemma feedback easily available implies need au acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 19 tomated predictions email classification hope time time willreceivefeedbackfromtheuserincaseofmisclassificationsorcandesignanactive learningsystemegzliobaiteetal2013which fromtimetotimeaskstheuserto providelabelsondemandhoweverwhenpossibleweneedtoaimatautomaticways forobtainingthetruelabels suppose monitoring attitude people towards political party want classify polarity sentiment tweets twitter labelling tweets manually positive negative laborious expensive task however tweets may authorprovidedsentimentindicatorschangingsentimentisimplicitintheuseofvar ious types emoticons hence may use label training data smileys emoticons visual cues associated emotional states con structed using characters available standard keyboard representing facial expression emotion using emoticons authors tweets annotate text withanemotionalstateannotatedtweetscanbeusedtotrainasentimentclassifier buildingacontentbasedfilterforadaptivenewsaccesspresentratherdifferentper spectiveontextclassificationinstreamingsettingsthegoalistolearnincrementally keep date user model news story classification simple yet effective approach proposed billsus pazzani 2000 user adap tivelearningsystemisbuiltconsistingofasimpleensemblewithseparatemodelsfor shorttermandlongterminterestsofusersastablenaivebayesclassifierisusedfor modelling long term interests user nearest neighbour classifier cap tures short term interests user shortterm interests model fixed size window liked news stories maintained andor instances weighed withrespecttotheiragenoexplicitchangedetectionisusedformonitoringeitherof shortterm longterm interests true labels instances come naturally due positive relevance feedback ie user accessing particular news itemprovidesthesignalthattheitemisrelevanttohisorherinterests ontheotherhandrecommendersystemsisabroadapplicationinthepersonalas sistance information category bobadilla et al 2013 adomavicius tuzhilin 2005interestsofthedataminingcommunityinrecommendersystemsdomainhave boosted netflix competition3 one lessons learnt winning teams taking temporal dynamics account substantially contributes wards building accurate models modelling user interests handling concept drift interesting aspects collaborative filtering modelling user inter ests relies primarily availability ratings already provide users realistic application case data highly imbalanced movies popular movies users rate many movies many rate rating matrix highdimensional extremely sparse containing 1 nonzero elements properties make application supervised learning techniques inapplicable motivate development advanced collaborative filtering approaches sources nature change canbediversebothitemsandusersarechangingovertimetheitemsideeffectsin cludefirstofallchangingproductperceptionandpopularitypopularityofsomemovies expected follow seasonal patterns userside effects include changing tastes andpreferencesofuserssomeofwhichmaybeshorttermorcontextualandtherefore likelyreoccurringmoodactivitycompanychangingperceptionofratingscalepossi blechangeofraterwithinhouseholdandalikeproblemsthewinningteamdeveloped ensemble approach including multiple models handling various kinds changes suggested koren 2010 popular windowing instance weighing ap proaches handling concept drift best choice kind changing 3wwwnetflixprizecom acmcomputingsurveysvol1no1article1publicationdatejanuary2013 110 jgamaetal behaviour simply collaborative filtering eg relations ratings isofthemainimportanceforpredictivemodelling 254 ubiquitousenvironmentapplications autonomous vehicle control application case ubiquitous environment category darpa grand challenge prize competition open teams developing fully autonomous driverless vehicles stanley thecarofthewinningteamofthesecondlongdistancecompetition4organizedin2005 highly reliable system board allowed car drive different offroad environments relatively high speeds among intelligent components systems stanley perform identification avoidance obstacles roadfindingthrunetal2006 stanley team combined several ideas automated terrain labelling using streaming laser sensor data short medium range obstacle avoidance streamingdatafromthecolourcameratolearntheconceptofthedrivablesurfaceand use velocity control surface ahead nondrivable car slowdownthatisthevisionmodulehadtomaintainanaccurateclassifierforiden tifying drivable nondrivable regions image stream adaptive learning approach necessary performing task reliably many changing andnoteasilymeasurablefactorssuchassurfacemateriallightingconditionsordust ordirtonthecameraitselfthateffectthetargetconcept classification task model colour drivable terrain stan ley team used adaptive mixture gaussians model adapt slowly changing lighting conditions abruptly changing surface colours eg moving paved unpaved road gradual adaptation internal gaussian adjusted rapid adaptation gaussians replaced new ones requiredspeedofadaptationdependedontheroadconditions realworld application cases present evidence adaptive learning makes itpossibletoaddresscomplexlearningproblemswhichwouldnotbefeasibletotackle stationary settings examples also illustrate important first understandthenatureandsourceofdriftandonlythenengineeraneffectiveadaptive learningstrategy 3 taxonomyofmethodsforconceptdriftadaptation section propose new taxonomy adaptive algorithms learn pre dictivemodelfromevolvingdatawithunknowndynamicsthetwomainabstractcon cepts memory forgetting data andor models discuss set representa tive popular algorithms implement adaptation strategies discussion organizedaroundfourmodulesofadaptivelearningalgorithmsthatwereidentifiedin figure 3 memory change detection learning loss estimation main idea hindpresentingthetaxonomyinfourseparatemodulesratherthanatreeofmethods istoseeadaptivelearningsystemsasconsistingofmodularcomponentswhichcanbe permutedandcombinedwitheachotheranoverviewofthetaxonomyispresentedin figure 4 methods within module detailed figures 5 6 7 and8 31 memory learning concept drift requires update predictive model new information also forget old information consider two types mem ory short term memory represented data long term memory represented generalizationofdatamodelsinthissubsectionweanalyzetheshorttermmemory 4httparchivedarpamilgrandchallenge05 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 111 data management memory forgetting mechanism changedetection detectionmodels lleeaarrnniinnggmmooddee learning aaddaappttiioonnmmeetthhooddss mmooddeellmmaannaaggeemmeenntt lossestimation estimationmethods fig4 fourmodulesofadaptivelearningsystemswithataxonomyofmethods singleexample data management multipleexamples fixedsize windows variablesize fixedsize temporalsequence abruptforgetting forgetting window variablesize mechanism sampling gradualforgetting selection yromem fig5 taxonomyofmemorypropertiesofmethods two dimensions illustrated figure 5 data management specifies dataisusedforlearningandiiforgettingmechanismspecifiesinwhichwayolddata isdiscardedtheissuesoflongtermmemorywillbediscussedinsection33 311 data management assumption behind adaptive learning algo rithmsisthatthemostrecentdataisthemostinformativeforthecurrentprediction hencedatamanagementtypicallyaimsatlearningfromthemostrecentdataeither asingleexampleormultipleexamples single example storing single example memory roots online learning algorithms learn one example time access previous examples later online algorithm process input example exampleintheorderofarrivalwithoutkeepingtheentiretrainingdatasetinmemory online learners maintain single hypothesis originate com plex predictive model model updates errordriven new example x arrives prediction yˆ made current hypothesis true target value received loss computed current hypothesis updated necessary example algorithms winnow littlestone 1987 linear classi fiersystemthatusesamultiplicativeweightupdateschemethekeycharacteristicof winnowisitsrobustnesstoirrelevantfeatures online learning algorithms seen naturally adaptive evolving distribu tions mainly due learning mechanisms continuously update model withthemostrecentexampleshoweveronlinelearningsystemsdonothaveexplicit forgetting mechanisms adaptation happens old concepts diluted due new incoming data systems like winnow littlestone 1987 vfdt domin gos hulten 2000 adapt slow changes time main limitation acmcomputingsurveysvol1no1article1publicationdatejanuary2013 112 jgamaetal slow adaptation abrupt changes depends sensible model update new example set setting parameters requires tradeoff stability sensitivity carpenter et al 1991b representative single stancememorysystemsthatexplicitlydealwithconceptdriftincludestaggerschlim mer granger 1986 dwm kolter maloof 2003 2007 svm syed et al 1999 ifcsbouchachia2011aandgt2fcbouchachiaandvanaret2013 multiple examples another approach data management maintain predictive model consistent set recent examples algorithm family flora widmer kubat 1996 one first supervised incremental learning systems evolving data original flora algorithm uses sliding window fixed length stores recent examples firstinfirstout fifo data structure time step learning algorithm builds new model using theexamplesfromthetrainingwindowthemodelisupdatedfollowingtwoprocesses alearningprocessupdatethemodelbasedonthenewdataandaforgettingprocess discarddatathatismovingoutofthewindowthekeychallengeistoselectanappro priate window size short window reflects current distribution accurately thus assure fast adaptation times concept changes stable periods short window worsens performance system large window givesabetterperformanceinstableperiodsbutitreactstoconceptchangesslower ingeneralthetrainingwindowsizecanbefixedorvariableovertime slidingwindowsofafixedsize store memory fixed number recent examples whenever new example arrives saved memory oldest one discarded simple adaptive learning method often used baseline inevaluationofnewalgorithms slidingwindowsofvariablesize varythenumberofexamplesinawindowovertime typically depending indications change detector straightforward ap proachistoshrinkthewindowwheneverachangeissingledsuchthatthetraining datareflectsthemostrecentconceptandgrowthewindowotherwise one first algorithms using adaptive window size flora2 widmer andkubat1996incomingexamplesareaddedtothewindowandtheoldestonesare deleted addition deletion keeps window predictive model consis tent current concept versions algorithm deal recurring concepts flora3 noisy data flora4 later study klinkenberg joachims 2000presentsatheoreticallysupportedmethodforrecognizingandhandlingconcept changes support vector machines method maintains window training examples appropriate size key idea adjust window sizebasedontheestimateofthegeneralizationerrorateachtimestepthealgorithm buildsanumberofsvmmodelsusingvariouswindowsizesandselectsthethewindow sizethatminimizestheleaveoneouterrorestimateasthetrainingwindow learning windows variable length appear maloof michalski 1995 klinkenberg 2004 gama et al 2004 kuncheva zliobaite 2009 assump tion behind relying windowing recency data associated relevance importance unfortunately assumption may true every circumstanceforinstancewhenthedataisnoisyorconceptsreoccurrecencyofdata mean relevance moreover slow change lasts longer window size windowingmayfailaswell 312 forgetting mechanism common approach deal evolving data generatedfromprocesseswithunknowndynamicsisforgettingtheoutdatedobserva tionsthechoiceofaforgettingmechanismdependsontheexpectationsthatwehave regarding changes data distribution tradeoff reactivity ro acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 113 bustness noise system required abrupt forgetting faster thereactivitybutalsothehighertheriskofcapturingnoiseis abrupt forgetting time set observations defines window information considered learning abrupt forgetting partial memory refers mechanisms given observation either inside outside training windowseveralwindowmodelshavebeenpresentedintheliteraturetwobasictypes sliding windows babcock et al 2002 sequence based size windowischaracterizedbythenumberofobservationsandiitimestampbasedwhere thesizeofawindowisdefinedbydurationtime two models sequence based windows sliding windows size j landmarkwindowsaslidingwindowstoresonlythemostrecentexamplesinthefirst infirstout fifo data structure landmark window stores examples given timsetamp called landmark landmark window window variable size timestamp based window size includes elements arrivedwithinthetimefromtunitsoftimebackuntilnowlearningsystemsthatuse abruptforgettingwherediscussedintheprevioussection oneofthealternativestoovercomewindowingespeciallyfixedwindowingissam pling goal summarize underlying characteristics data stream long periods time every included sample drawn uniformly stream one well known algorithm reservoir sampling vitter 1985 goal ofthereservoirsamplingistoobtainarepresentativesamplefortheobservedstream operates follows ith item arrives stored reservoir probabilityp kiwherek isthesizeofthereservoiranddiscardedwiththeproba bility1pifpositivethenarandomlychosensampleisdiscardedfromthereservoir free space new sample number sampling techniques inves tigated like alkateb et al 2007 efraimidis spirakis 2006 aggarwal 2006 rusuanddobra2009reservoirsamplinghasbeendiscussedinsomestudiesrelated drift change detection alternative windowing ng dash 2008 yaoetal2012zhaoetal2011 gradualforgetting gradualforgettingisafullmemoryapproachwhichmeans thatnoexamplesarecompletelydiscardedfromthememoryexamplesinmemoryare associatedwithweightsthatreflecttheirageexampleweightingisbasedonasimple intuition importance example training set decrease itsagesupposethatattimeithestoredsufficientstatisticsiss andweobservean i1 examplex assuminganaggregationfunctiongxsthenewsufficientstatisticsis computedass gx αs whereα01isthefadingfactorthiswaytheoldest i1 informationbecomestheleastimportant linear decay techniques found koychev 2000 2002 technique exponentialdecayispresentedinklinkenberg2004thelattertechniqueweightsex amplesaccordingtotheirageusinganexponentialageingfunctionw xexpλj λ example x appeared j time steps ago parameter λ controls fast theweightsdecreaseforlargervaluesofλalowerweightisassignedtooldexamples deemed less informative current prediction λ 0 exampleshavethesameweight memorymethodsaresummarizedintablei 32 changedetection thechangedetectioncomponentreferstothetechniquesandmechanismsforexplicit driftandchangedetectionitcharacterizesandquantifiesconceptdriftbyidentifying changepointsorsmalltimeintervalsduringwhichchangesoccurbassevilleandniki forov1993weconsiderthefollowingdimensionsasillustratedinfigure6imethods acmcomputingsurveysvol1no1article1publicationdatejanuary2013 114 jgamaetal tableicategorizationofmemorytechniques datamanagement schlimmerandgranger1986littlestone1987 singleexample domingosandhulten2000 kunchevaandplumpton2008kellyetal1999 bouchachia2011aikonomovskaetal2011 salganicoff1997widmerandkubat1996 fixedsize syedetal1999hultenetal2001lazarescuetal2004 multipleexamples bifetandgavalda20062007gomesetal2011 maloofandmichalski1995klinkenberg2004 variablesize gamaetal2004 zhangetal2008kunchevaandzliobaite2009 forgettingmechanisms salganicoff1997widmerandkubat1996 temporalsequences forman2006klinkenberg2004pechenizkiyetal2009 abruptforgetting nganddash2008yaoetal2012delanyetal2005 sampling zliobaite2011azhaoetal2011salganicoff1993 koychev20002002helmboldandlong1994 gradualforgetting klinkenberg2004koren2010 sequentialanalysis controlcharts detectionmethods monitoring twodistributions contextual noitcetedegnahc fig6 taxonomyofcontrolpropertiesofmethods based sequential analysis ii methods based control charts iii methods based ondifferencesbetweentwodistributionsivheuristicmethods already pointed online learning systems without explicit change detection mechanism adapt evolving data advantage explicit change detectionisprovidinginformationaboutthedynamicsoftheprocessgeneratingdata atypicalsectionstrategymonitorstheevolutionoftheperformanceindicatorswid mer kubat 1996 zeira et al 2004 raw data statistically compares toafixedbaselineaseminalworkinadaptivelearningwithmonitoringperformance indicatorsistheflorafamilyofalgorithmstheflora2algorithmwidmerandkubat 1996includesawindowadjustmentheuristicforarulebasedclassifiertodetectcon cept changes accuracy coverage current model monitored thewindowsizeisadaptedaccordinglyinthecontextofinformationfilteringmonitor ingthevaluesofthreeperformanceindicatorsaccuracyrecallandprecisionhasbeen proposedklinkenbergandrenz1998theposteriorofeachindicatoriscomparedto thestandardsampleerrorsofamovingaveragevalue 321 detectors based sequential analysis sequential probability ratio test sprt wald 1947 basis several change detection algorithms let xn 1 sequenceofexampleswherethesubsetofexamplesxw1 w nisgeneratedfrom 1 unknown distribution p subset xn generated another unknown 0 w distributionp whentheunderlyingdistributionchangesfromp top atpointwthe 1 0 1 probability observing certain subsequences p expected significantly 1 higher p significance means ratio two probabilities 0 smaller threshold assuming observations x independent statisticfortestingthehypothesisthatachangepointoccurredattimew againstthe acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 115 nullhypothesisthattherewasnochangeattimewisgivenby n tn logpx wx np1 cid88 logp1x tn1logp1x n 3 w px x p0 p0x w p0x w n n iw andachangeisdetectediftn lwherelisauserdefinedthreshold w thecumulativesumcusumisasequentialanalysistechniqueduetopage1954 thatusesprinciplesofsprtitisoftenusedforchangedetectionthetestoutputsan alarmwhenthemeanoftheincomingdatasignificantlydeviatesfromzerotheinput test residual predictor instance prediction error kalman filter cusum test given g max0g x δ g 0 t1 0 thedecisionruleisifg λthen signal alarmfollowedbysettingg 0herex standsforthecurrentobservedvalueδ correspondstothemagnitudeofchangesthat areallowedtisthecurrenttimeandλisauserdefinedthresholdsincethisruleonly detectschangesinthepositivedirectionwhennegativechangesneedtobefoundmin used instead max case change detected value negative value threshold cusum test memoryless itsaccuracydependsonthechoiceofparametersδ andλbothparameterscontrolthe tradeoff earlier detecting true changes allowing false alarms low values δ allow faster detection cost increased number false alarms cusum applied stream mining example muthukrishnan et al 2007 pagehinkley page 1954 test ph variant cusum sequential analysis technique typically used change detection signal processing allows efficient detection changes normal behavior process established modelthephtestisasequentialadaptationofthedetectionofanabruptchangein average gaussian signal mouss et al 2004 test variable defined cumulative difference observed values mean current time cid80t x x δ x 1 cid80t x δ specifies t1 t1 tolerable magnitude changes minimum defined minm 1tphtestsforthedifferencebetweenm andm ph whenthis difference greater threshold λ user defined change flagged larger λ willentailfewerfalsealarmsbutmightmisssomechanges similartotheonlineversionsofcusumandphtestsshiryaevrobertsmethod shiryaes bayesian test shiryaev 2009 tartakovsky moustakides 2010 rely onlinethresholdingthatisassoonasthecomputedstatisticexceedsapresetthresh old ε change diagnosed accuracy detection methods often depend onthefalsealarmrateandthemisdetectionratesomeofthesemethodsarefurther explainedbelow 322 detectorsbasedonstatisticalprocesscontrol controlchartsorstatisticalprocess control spc standard statistical techniques monitor control quality ofaproductduringacontinuousmanufacturingspcconsiderslearningasaprocess monitors evolution process drift detection methods based spc ap pearinklinkenbergandrenz1998lanquillon2002gamaetal2004gomesetal 2011bouchachia2011a letpairsx formasequenceofexamplesforeachexamplethemodelpredicts yˆ either true yˆ false yˆ cid54 set examples error random variable bernoulli trials binomial distribution gives generalformoftheprobabilityfortherandomvariablethatrepresentsthenumberof errors set n examples point sequence errorrate cid112 probability p observing false standard deviation σ p 1p acmcomputingsurveysvol1no1article1publicationdatejanuary2013 116 jgamaetal drift detector manages two registers model operation p σ min min timeiaftercastingthepredictionforthecurrentexampleandverifyingtheprediction errorifp σ islowerthanp σ thenp p andσ σ min min min min sufficiently large number observations binomial distribution closely approximatedbythenormaldistributionwiththesamemeanandvarianceconsider ingthattheprobabilitydistributionshouldnotchangeunlessaconceptdrifthappens the1δ2confidenceintervalforp withn30examplesisapproximatelyp ασ parameter α depends desired confidence level commonly used confi dence level warning 95 threshold p σ p 2σ min min outofcontrolis99withthethresholdp σ p 3σ min min suppose time j example x arrives model prediction leads p j j j andσ thesystemisdefinedtobeinoneofthefollowingthreestates j 1 incontrol p σ p 2σ error system stable j j min min examplex isdeemedtocomefromthesamedistributionasthepreviousexamples j 2 outofcontrol whenever p σ p 3σ error increased sig j j min min nificantly compared recent past examples probability 99 recentexamplescomefromadifferentdistributionthanthepreviousexamples 3 warning state two previous states error increasing reached outofcontrol yet decisive state error may beincreasingduetonoisedriftorduetoasmalldeficiencyofthepredictivemodel thisstatesignalsthatmoreexamplesarerequiredforconfirmingadrift spc used measure rate change time warning outofcontrol short times indicate fast changes longer distances indicate slower changes rate change also measured rate errors number examples warning spc relies estimates error variance define theactionboundswhichshrinkastheconfidenceoftheerrorestimatesincreasesspc canbeimplementedinsideincrementallearningalgorithmsorasawrappertobatch algorithmsseealgorithm3intheappendix exponentially weighted moving average ewma algorithm ross et al 2012 advancesonsimilarideastheewmacomputesarecentestimateoftheerrorrateµ byprogressivelydownweightingolderdataz µ andz 1λz λe t0 0 0 t1 e error current example shown independently distribution x mean standard deviation z µ µ cid113 zt σ λ 11λ2tσ whereσ isthestandarddeviationofthestreamassume zt 2λ x x thatbeforethechangepointthatµ µ theewmaestimatorz fluctuatesaround 0 value change occurs value µ changes µ z react c diverging µ towards µ change signaled z µ lσ 0 c 0 zt parameterlthecontrollimitdetermineshowfarfromz mustdivergefromµ 0 anchangealarmisissued 323 monitoring distributions two different timewindows methods typically uses fixed reference window summarizes past information sliding detec tionwindowoverthemostrecentexamplesdistributionsoverthesetwowindowsare comparedusingstatisticaltestswiththenullhypothesisstatingthatthedistributions equal null hypothesis rejected change declared start detectionwindowthewindowscanmonitorunivariateormultivariaterawdatasep arately class evolving model parameters performance indicators dries andruckert2009thetwowindowscanbeofequalorprogressivesizesanddifferent windowpositioningstrategiesmaybeemployedadaeandberthold2013 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 117 comparing distributions two detection windows5 context data streams introduced kifer et al 2004 examples two windows compared statistical tests based chernoff bound decide whether two distri butions different line technique vfdtc gama et al 2006 ability deal concept drift continuously monitoring differences two distribution classes examples distribution node leaf weightedsumofthedistributionsinthechildrenofthatnode vorburger bernstein 2006 present entropybased metric measure distribution inequality two sliding windows including respectively older recent examples distributions equal entropy measure results value 1 absolutely different measure result value 0 entropy measure continuously monitored time concept drift signalledwhentheentropymeasuredecreasesbelowagivenfixeduserdefinedthresh old additional examples include change detection methods proposed dasu et al 2006andsebastiaoandgama2007bothusethekullbackleiblerkldivergence tomeasurethedistancebetweentheprobabilitydistributionsoftwodifferentwindows oldandrecenttodetectpossiblechanges anillustrativeexampleispresentedinbachandmaloof2008itusestwomodels stable reactive stable model predicts based long history whereas reactive model predicts based short recent time window technique uses reactive model indicator concept drift uses stable model make predictions since stable model performs better reactive model ac quiring target concept drift detection method uses differences accuracy betweenthetwomodelstodetermine whentoreplacethecurrentstablemodelsince stable model performs worse reactive model target concept changes nishida yamauchi 2007 also consider two accuracies accuracy es timated stream accuracy estimated sliding window recent examples concept drift signaled whenever significant decrease therecentaccuracyisobserved adaptive sliding window adwin bifet gavalda 2006 2007 another change detector using detection window let x x x x sequence real 1 2 3 valued data adwin requires input sequence bounded x 01 beachievedbyrescalingoftheoriginaldataaslongastheloweandtheupperbound values fixed denote µ expected value x drawn ac cording adwin assume particular data distribution adwin slides fixed detection window w recently read x let µˆ denote known w average examples within w µ unknown average µ w w adwin summarized figure 2 appendix operates follows whenever two large enough subwindows w exhibit distinct enough means algorithm concludes expected values within windows different older subwindowisdroppedlargeenoughanddistinctenougharedefinedbythehoeffd ing bound testing whether average two subwindows larger cid15 cut cid113 computed cid15 1 ln4w w denotes length subwindow cut 2m δ mistheharmonicmeanofw andw iem 2 andδ 01isauser 0 1 1w01w1 definedconfidenceparametertherecommendedvalueforwhichis02 themainlimitationofthedetectionmethodsbasedonmonitoringtwodistributions compared sequential detectors discussed memory requirements 5notethatthedetectionwindowsareconceptuallydifferentfromtrainingwindowstheymaycoincidein sizethedetectionwindowsareusedforestimatingdatadistributionandaretypicallypairedthetraining windowsdeterminetrainingdatasetforthelearningalgorithmwhenproducingamodel acmcomputingsurveysvol1no1article1publicationdatejanuary2013 118 jgamaetal sequential detectors need store incoming data distribution based detectors need store data within two windows main advantage precise localization change point although delay least w samples adwin equipped compression using variant exponential histogramdataretal2002thereforeitdoesnotneedtostorealltheexamplesfrom detection window w stores data ologw memory ologw processingtimeperitemratherthantheow adaptation techniques discussed next originally coupled spe cific change detectors general strategy deploy detector since main informationthatisneededfromadetectoriswhetherachangehasoccurredornot 324 contextual approaches splice system harries et al 1998 presents meta learning technique implements context sensitive batch learning approach splice designed identify intervals stable hidden context induce refine local concepts associated contexts idea use timestamp examples input feature batch classifier first stage examples augmented timestamp feature decision tree learned deci siontreefindssplitsonthetimestampfeaturethepartitionsonthatfeaturesuggest different contexts second phase c45 applied partition find interimtemporalconcepts independentlywhetheratimewindowingisusedveryoftenrobustapproachesthat yield balance incremental learning forgetting needed deal changingenvironmentsthisideawasappliedintheincrementalfuzzyclassification system ifcs algorithm bouchachia 2011a context classification maintainsasetofprototypesthatisaclassisrepresentedcoveredbymanyclusters therethreemechanismscalledstalenesspenalizationandoverallaccuracyareused thefirsttwomeasurestackletheproblemofmodelcomplexityaswellasgradualdrift one two measure values falls small threshold called removal threshold prototype removed last one intended handling gradual abrupt drift staleness tracks activity prototype making decisions aboutthenewinputandsuchanactivityindicatesthattherecentincomingnewinput emanatefromtheinputspacecoveredbytheprototypestaleprototypestendtocover obsolete regions express staleness mechanism following formula used wi ζtat indicate respectively current time index last time prototype winner small values forgetting factor ζ accelerates reduction weight weights associated prototypes valueforstaleprototypesdecaysandforactiveprototypesitreinforcesclearlyifthe staleness long ta large enough w diminishes prototype vanishes second mechanism focuses tracking accuracy decisions made thusobservingtheevolutionofthemodelintermsofconsistencywiththerecentinput patterns aim ensure accuracy deteriorate least signif icantly following formula used z λsi number errors made prototype since created weight decreases exponentially asthenumberoferrorsincreasesthesmallerthevalueofλthemorequicklyarethe forgettingspeedandthemodelupdatethethirdmechanismisthestatisticalprocess controlspccriterionwhichisexplainedinsection322andwhichaimsathandling gradual abrupt changes based number errors produced learning modelduringprediction tableiisummarizespropertiesofthepresentedchangedetectionalgorithmsover allcomparingtwodistributionsforchangedetectionrequiresmorecomputationalre acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 119 tableiicomplexityofchangedetectionalgorithmsw isthesizeofthedetectionwindow approach memoryatt processingatt examplealgorithms sequentialanalysis o1 o1 cusumph statisticalprocesscontrol o1 o1 spc monitoringtwodistributions ologwow ologwow adwin tableiiicategorizationofchangedetectiontechniques detectionmethods wald1947page1954moussetal2004 sequentialanalysis muthukrishnanetal2007ikonomovskaetal2011 lanquillon2002gamaetal2004 controlcharts kuncheva2009gomesetal2011rossetal2012 kiferetal2004bifetandgavalda20062007 monitoringtwodistributions vorburgerandbernstein2006leeuwenandsiebes2008 gamaetal2006nishidaandyamauchi2007bachandmaloof2008 driesandruckert2009kuncheva2013 contextual harriesetal1998klinkenberg2004 incremental retraining learningmode online incremental streaming blind adaptionmethods localorpartial informed replaceupdating globalorcomplete replacement singlemodel reoccurrence modelmanagement activemodelpoolmanagement ensemble modelselection modelweighting gninrael fig7 taxonomyoflearningpropertiesofmethods sources detecting based evolution indicators potentially may give morepreciseinformationaboutthelocationofchange 33 learning learning component refers techniques mechanisms generalizing examples updating predictive models evolving data section organized follows learning mode referring model updating new data points available ii model adaptation analyzes behavior predictive models ontimeevolvingdataiiimodelmanagementreferstothetechniquesformaintainng activepredictivemodels 331 learningmode whenevernewlabeledexamplesareavailablethelearningsys tem might update model consider two different learning modes retraining discards current model builds new model scratch using buffered dataincrementaladaptationupdatesofthemodel retraining approaches need data buffer stored memory retrain ing used emulate incremental learning batch learning algo rithms gama et al 2004 beginning model trained available datanextwhenevernewdataarrivesthepreviousmodelisdiscardedthenewdata ismergedwiththepreviousdataandanewmodelislearnedonthisdatastreetand kim2001zeiraetal2004klinkenbergandjoachims2000 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 120 jgamaetal incremental approaches update current model using recent data cremental algorithms process input examples onebyone update sufficient statisticsstoredinthemodelafterreceivingeachexampletheymighthaveaccessto previous examples summaries examples case cvfdt hulten et al 2001 described next section online learning mode updates current modelwiththemostrecentexampletheyareerrordrivenupdatingthecurrentmodel depending misclassifies current example examples include winnow little stone 1987 mbw carvalho cohen 2006 mbw carvalho cohen 2006 refers wellknown algorithms including perceptron multilayer perceptrons traditionally trained using several passes training data restrictedtoasingletrainingpassoverthedataareparticularlyrelevantformassive streaming data carvalho cohen 2006 origins online learning origin goesbacktothelate60swithperceptronrosenblatt1958andearly2000withthe advent paradigm prediction expert advice cesabianchi lugosi 2006theearlyworkappearedinanumberofseminalpaperslittlestone1987lit tlestone warmuth 1994 herbster warmuth 1998 vovk 1998 model updated current example time goes newly arrived data tend erase away prior patterns models artificial neural networks learning isinevitablyconnectedwithforgettingtheabilitytocontinuouslylearnfromastream ofexampleswhilepreservingpreviouslylearnedknowledgeisknownasthestability plasticitydilemmacarpenteretal1991aitisadilemmabecausethereneedstobe balance stable handle noise able learn new patterns someartificialneuralnetworkscompletelyforgetthepastpatternswhenexposedtoa newsetofpatternsthisphenomenonisknownasthecatastrophicforgettingfrench 1994polikaretal2001 streaming algorithms online algorithms processing highspeed continuous flow data examples processed sequentially examined passestypicallyjustonethesealgorithmsuselimitedmemoryandcontroltheavail able memory example hoeffding trees domingos hulten 2000 variants vfdtcgamaetal2006orfimtddikonomovskaetal2011areabletofreezeleaves whenmemorybecomesscarce 332 adaptationmethods theadaptationstrategiesmanageadaptationofthepredic tivemodeltheyfallintotwomajortypesblindandinformed blind adaptation strategies adapt model without explicit detection changesblindadaptationtypicallyusetechniquesasfixedsizeslidingwindowsthat take window size w parameter periodically retrain model lat estwexamplesandexampleweightingthatconsidertheimportanceoftrainingexam pleswidmerandkubat1996klinkenbergandrenz1998klinkenbergandjoachims 2000lanquillon2002 special case blind adaptation incremental online learning model evolves data without strategy explicitly detect concept drift model adapts recent data paradigmatic example vfdt domingos hulten 2000 vfdt new examples update statistics leaves cur rent model tree grows new leaves generated label leaves reflect recent concepts blind strategies proactive datethemodelbasedonthelossfunctionthemainlimitationoftheblindapproaches isslowreactiontoconceptdriftindatatheblindapproachesforgetoldconceptsata constantspeedindependentlyofwhetherchangesarehappeningornotwhenchanges arehappeningitmaybemorebeneficialtodiscardolddatafasteranddiscardolddata slowerornotdiscardatallattimeswhenchangesarenothappening acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 121 informed strategies reactive actions depend whether trigger flagged bifet gavalda 2006 hulten et al 2001 triggers ei ther change detectors examples described section 32 specific data descriptors see reoccurring concept management techniques widmer ku bat1996changedetectorscanbeindependentfromtheadaptationstrategyegthe adaptive training window technique closely integrated adap tationstrategygamaetal2006ikonomovskaetal2011whichwerefertoasmodel integrateddetectorsgamaetal2006 thereactiontoadriftsignalmightapplytothemodelasawholeormightexplore characteristicsofthelanguageusedtorepresentgeneralizationofexamples globalreplacement informedadaptationinglobalmodelssuchaslinearregres sion discriminant classifiers naive bayes require full reconstruction model thisisthemostradicalreactiontoadriftthefullmodelisdeletedandanewmodel started scratch strategy widely used example gama et al 2004street kim 2001 zeira et al 2004 klinkenberg joachims 2000 localreplacement inmanycaseschangesoccuronlyinsomeregionsofthedata space example spam filtering spammers may start using combinations words facebook support previously associated legitimate e mails change concept spam affect small subset incomingemailscarmonacejudoetal2010 granular models decision rules decision trees adapt parts modelinadecisiontreeordecisionruleseachnodeorrulecoversahyperrectangle data space thus decomposable models need adapt nodes cover region data space affected concept drift cvfdt hulten et al 2001continuouslymonitorsthequalityofpreviousdecisionssplittingfeatureswith respecttoaslidingwindowoffixedsizeoverthemostrecentdatastreamcvfdtmain tains window training examples keeps learned tree uptodate window monitoring quality old decisions data moves thewindowinparticularwheneveranewexampleisreaditisaddedtothestatistics nodes tree passes last example window forgottenfromeverynodewhereithadpreviouslyhadaneffectandthevalidityofall statistical tests checked cvfdt detects change starts growing alternate tree parallel rooted newlyinvalidated node alternate accurate new data original original replaced alternate andfreed node hoeffding tree captures statistics time window stream root node receives oldest examples leaf nodes receive recentexamplesnodesneartherootweregeneratedusingexamplesolderthanthose generated nodes near leaves observation basis sequential regularization gama et al 2006 ikonomovska et al 2011 technique compares thedistributionoftheerrorsatleavestothedistributionoftheerrorsatuppernodes tree recall leaves contain recent information upper nodes contain older information thus two error distributions significantly different interpreted concept drift gama et al 2006 concept change detected system adapts model assuming recent formationcontainstheusefulinformationaboutthecurrentconceptthestatisticsof themostrecentexamplesincorporatedinthetreearestoredintheleavestherefore supposing change concept detected node reaction method pushesupalltheinformationofthedescendingleavestothenodeinamelythesuffi cientstatisticsandtheclassesdistributionsthedecisionnodebecomesaleafandthe acmcomputingsurveysvol1no1article1publicationdatejanuary2013 122 jgamaetal subtreerootedatthedecisiontreeisprunedthisforgettingmechanismremovesthe outdatedinformation 333 model management ensemble learning maintains memory ensemble multiple models make combined prediction adaptive ensembles often mo tivated assumption change data generated mixture distribution seen weighted combination distributions charac terizing target concepts individual model models distribution scholz klinkenberg2007thefinalpredictionistypicallyaweightedaverageoftheindivid ualpredictionswheretheweightreflectstheperformanceoftheindividualmodelson themostrecentdatatheweightschangeovertime ensemble methods dynamically changing data categorized kuncheva 2004 three types dynamic combinatio base learners trained advanceanddynamicallycombinedtorespondtochangesintheenvironmentbymod ifyingthecombinationrulewinnoworweightedmajoritylittlestone1987blum1997 tsymbaletal2006widmerandkubat1996iicontinuousupdateofthelearners learners either retrained batch mode updated online us ing new data breiman 1999 fern givan 2003 oza 2001 combination rule mayormaynotchangeintheprocessiiistructuralupdatewherenewlearnersare added existing ones activated deactivated ineffi cientonesareremovedordeactivatedkolterandmaloof2003streetandkim2001 bouchachia2011bthesethreecategoriesdonotnecessarilyneedtobemutuallyex clusiveitistechnicallypossibletocombinetwoorallthreeofthesestrategies connection data drift online learning application ensemble learn ing subject investigations recent past years stance elwell polikar 2011 batchbased ensemble classifiers called learnnse proposed deal concept drift proposed algorithm aims coping drift regardless rate type drift number concept classespresentatanytimeforeachnewbatchofdataanewclassifieristrainedand combinedusingadynamicallyweightedmajorityvotingstrategy minku et al 2010 diversity learning ensemble investigated presence different types drifts study shows drift occurs ensembles lessdiversityobtainlowertesterrorsbutshortlyafterthedriftoccurshighlydiverse ensemblesarebetterregardlessthetypeofdriftlongerafterthedrifthighdiversity becomeslessimportant sea algorithm street kim 2001 one first techniques handle concept drift classifier ensembles learned streaming data trains sep arate classifier sequential batch training examples trained classifier addedtoafixedsizeensemblewhiletheworstperformingclassifierisdiscardedthe finalpredictionismadeusingasimplemajorityvoting seminal work dynamic weighted majority algorithm dwm kolter maloof20032007thatisanadaptiveensemblebasedontheweightedmajorityalgo rithm littlestone 1987 used online learning algorithm time changingproblemswithunknowndynamicsdwmmaintainsanensembleofpredictive models associated weight models generated learning algorithmondifferentbatchesofdatadwmdynamicallycreatesanddeletesexpertsin responsetochangesinperformancedwmmakespredictionsusingaweightedmajority vote models weights dynamically changing weights models misclassified current example decreased multiplicative constant β overall prediction incorrect new expert added ensem blewithweightequalto1finallyallthemodelsareincrementallyupdatedwiththe currentexampletoavoidcreatinganexcessivenumberofmodelsdwmprunestheen acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 123 semblebyremovingthepoorlyperformingexpertswithaweightlessthanathreshold variant dwm called addexp kolter maloof 2005 extension classi fication regression intelligently prunes previously generated models similar approach using weight schema similar boosting explicit change detection appears chu zaniolo 2004 boostinglike approach trainaclassifierensemblefromevolvingdatastreamsisalsoproposedinscholzand klinkenberg 2007 dynamics classifier weights controlled lift measureinsteadofaccuracywhichmeasuresthecorrelationbetweenpredictionsand theirtruelabel general framework mining conceptdrifting data streams using weighted en semble classifiers proposed wang et al 2003 ensemble predictive models eg c45 ripper naive bayes trained sequential batches data stream weighted voting used make final prediction weights follow expected predictive accuracy model suppose training data set classifier l let c true label example x x proba c bility x belongs class c output l mean square error classi fier mse 1 cid80 1 x2 weight classifier l xd c versely proportional mse weight reflects benefit using individ ual model comparison random classifier mse random classifier mse cid80 pc1pc2wherepcistheprobabilityofobservingclasscthus r c theweightw ofaclassifierl isw mse mse r notable technique ddd minku yao 2011 equips dynamic ensemble diversity control mechanism also uses internal drift detection speed adaptation initially model composed two ensembles low diversity ensemble high diversity ensemble ensembles trained incoming examples low diversity ensemble used predicting weights individual models proportional prequential accuracy described section 4 ddd assumes convergence underlying distributions stable concept drift ddd allows use high diversity ensemble predictions online bagging corresponding sampling replacement used controldiversitylevelsinthetwoensembles reoccurringconceptmanagementflora3widmerandkubat1996isthefirst adaptivelearningtechniqueforthetaskswhereconceptsmayreoccurovertimemore recent works discussing reoccurring concepts appear widmer 1997 yang et al 2006 katakis et al 2010 gama kosina 2011 settings instead dis carding outdated models might useful store learned models sleeping mode following idea work presented gama kosina 2011 discuss genericframeworkthatidentifiescontextusingdriftdetectioncharacterizescontexts usingmetalearningandselectthemostappropriatepredictivemodelfortheincoming datausingunlabeledexamplestheproposedframeworkisbasedonametalearning schemawhichaimstorecognizetheareaofapplicabilityoftheindividualmodel 34 lossestimation supervisedadaptivesystemsrelylossestimationbasedonenvironmentfeedbackthe discussion section close related performance related metrics dis cussedinsection42 model dependent previously referred klinkenberg joachims 2000 recognize handle concept changes using properties support vector machines method maintains window training examples minimizes acmcomputingsurveysvol1no1article1publicationdatejanuary2013 124 jgamaetal tableivcategorizationoflearningtechniques learningmode retraining streetandkim2001zeiraetal2004klinkenbergandjoachims2000 incremental schlimmerandgranger1986littlestone1987 bifetetal2009hultenetal2001polikaretal2001 streaming gamaetal2006ikonomovskaetal2011 adaptationmethods littlestone1987maloofandmichalski2000klinkenbergandrenz1998 blind chuandzaniolo2004bessaetal2009 informed hultenetal2001gamaetal2006ikonomovskaetal2011 modeladaptation modelspecific hultenetal2001gamaetal2006harriesetal1998 modelindependent wald1947gamaetal2004wangetal2003 bifetandgavalda2006kunchevaandzliobaite2009 modelmanagement singlemodel hultenetal2001gamaetal2006ikonomovskaetal2011 recurrent widmer1997gamaandkosina2011 ensemble polikaretal2001streetandkim2001kolterandmaloof2005 gaoetal2007minkuandyao2011elwellandpolikar2011 recurrent yangetal2006katakisetal2010gomesetal2011 modeldependent estimationmethods slidingwindow modelindependent fadingfactor noitamitsessol fig8 taxonomyoflossestimationpropertiesofmethods leaveoneout error estimate key idea get estimate generalization error using socalled ςαestimates efficient method estimating leave oneout error svm leaveoneout estimators accurate estimators usually veryexpensiveonemustrunthelearnersomanytimesasthenumberoftrainingex amplesforsvmςαestimatecanbecomputedforfreeonceasvmhasbeentrained computed percentage training points margins thetheoreticalpropertiesofthisestimatorisdiscussedinjoachims2000analytical loss estimation linear discriminant classifiers found zliobaite kuncheva2009 model independent already referred several works bach maloof 2008 nishidaandyamauchi2007gamaetal2013proposetodetectchangesusingtwo sliding windows short window containing recent information large window used reference containing larger set recent data including data short window rationale behind approach short window reactive large window conservative change occurs statistics computed short window capture event faster using statistics larger window similarly using fading factors smooth forgetting mechanism smaller fading factor detect drifts earlier larger ones based assumption gama et al 2013 propose perform ph test ratio two error estimates long term error estimate using large window fading factor close one short term error estimate using short window fading factor smaller first one drift signaled short term errorestimatorissignificantlygreaterthanthelongtermerrorestimatorthephtest monitorstheevolutionofratioofbothestimatorsandsignalsadriftwhenasignificant increase variable observed authors refer choice α fading acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 125 tablevcategorizationoflossestimationtechniques lossestimation modeldependent klinkenbergandjoachims2000zliobaiteandkuncheva2009 modelindependent slidindwindow maloofandmichalski1995klinkenberg2004bachandmaloof2008 nishidaandyamauchi2007gamaetal2013 fadingfactors koychev2000koychev2002gamaetal2013 factors window size critical experiments show drift detection basedontheratiooffadingestimatesissomewhatfasterthatwithslidingwindows 35 discussion supervised adaptive learning algorithms rely immediate arrival feedback true labels reality labels may become known immediately next time step castingthepredictionegfoodsalespredictionhoweverfeedbackcancomewithan uncontrollable delay unreliable biased costly labels may arrive within fixed orvariabletimelagincreditscoringtypicallythehorizonofbankruptcypredictionis fixed instance one year thus true labels become known one year passedalternativelythesettingmayallowtoobtainlabelsondemandeginemail spamcategorizationwecanasktheuserthetruestatusofagivenmessage 4 evaluation inordertoperformanexperimentalevaluationofanymachinelearningtechniquewe needtoconsiderfirstofalliperformanceevaluationmetricschosenaccordingtothe goalofalearningtaskandtheoperationalsettingsandiiamethodologyallowingto computethecorrespondingestimatesinthestreamingsettingsbesideswemayneed tofindajustifywhetheronemodelortechniqueissuperiortotheotherornot inthissectionweconsidertypicalchoicespeculiarforevaluatinganadaptivelearn ingtechniquecapabletohandleconceptdriftfirstwediscusstheperformanceeval uation metrics present possible experiment designs conclude pointers totheperformancebenchmarking 41 performanceevaluationmetrics performance evaluation metrics may selected traditional accuracy mea sures precision recall weighed average retrieval tasks sensi tivity specificity weighed average medical diagnostics mean absolute scaled errors regression timeseries prediction tasks root mean square error recommendersystems important consider appropriate reference points baseline approaches particular settings example one common baseline time series prediction movingaveragepredictioninitssimplestformknownastomorrowwilllookthesame like today prediction baseline gives reference points allowing judge much improvement supposedly intelligent adaptive technique achieve naiveapproach addition taking account practical considerations streaming settings wemayconsiderthefollowingmeasures ameasureforthecomputationalcostoftheminingprocess ramhours bifet et al 2010b onedimensional measure computational resources used streamingalgorithmsbasedonrentalcostoptionsofcloudcomputingservicesev erygboframdeployedfor1hourequalsoneramhour astatisticforclasstakingintoaccountclassimbalance kappastatistic com puted 1 aa rr accuracy rate intelligent classifier r accuracy rate random classifier randomly permutes predictions acmcomputingsurveysvol1no1article1publicationdatejanuary2013 126 jgamaetal intelligent classifier kappastatistic takes values 0 1 0 means achieved accuracy random statistics convenient computeonlineinastreamascompareforinstancewithanalternativeofroc besides evaluating performance learning strategy may like assess accuracy change detection separately strategies employ explicit driftdetectionaspartoftheconceptdrifthandlingstrategythefollowingcriteriaare relevantforevaluatingchangedetectionmethods probabilityoftruechangedetection characterizes capacity learning sys tem detect drifts occur measure computed synthetic datawheredriftsareknown probabilityoffalsealarms itcharacterizestheresilienceofthedetectionmethodin stead reporting commonly used false positive rate change detections streaming settings convenient use inverse time de tectionortheaveragerunlengthwhichistheexpectedtimebetweenfalsepositive detectionsthismeasurecanbecomputedeitheronsyntheticdatawherethedrifts known real data drifts case detections countedasfalsealarms delayofdetection gives estimate many new instances required detect change actual occurrence change much time would elapse change detected typically average time detection useddriftsneedtobeknownsyntheticdataissuitableforassessingthisaspect know well employed change detection methods perform alsoquantifytheeffectofparticularerrorordelayinchangedetectionontheoverall performanceoftheadaptivemodel 42 experimentaldesign common procedure estimating performance supervised learning techniquesinthetraditionalsettingswithstaticdataiscrossvalidationintraditional batch learning problem limited data overcome analyzing averaging multiple models produced different random arrangements training test data however crossvalidation directly applicable streaming settings withevolvingdatabecauseitwouldmixthetemporalorderofdata inthestreamsettingtheproblemofeffectivelyunlimiteddataposesdifferentchal lengesonesolutioninvolvestakingsnapshotsatdifferenttimesduringtheinduction ofamodeltoseehowmuchthemodelimprovestheevaluationprocedureofalearning algorithmdetermineswhichinstancesareusedfortrainingthealgorithmandwhich areusedtotestthemodeloutputbythealgorithmwhenconsideringwhatprocedure use data stream setting one main concerns build picture accuracy time one classifier may well first half stream badlyontheseconditisimportanttonoticethatstreamingevaluationmeasuresmay beevolvingovertime 421 evaluation timeordered data discuss two common procedures peculiar evaluation adaptive supervised learning techniques holdout prequential evaluationandpointtoarecentideaofthecontrolledpermutations holdoutwhentraditionalbatchlearningreachesascalewherecrossvalidationis time consuming often accepted instead measure performance single holdoutsetthisismostusefulwhenthedivisionbetweentrainandtestsetshasbeen predefined results different studies directly compared testing model time holdout set represents exactly context timetthelossestimatedintheholdoutisanunbiasedestimatorunfortunatelyitis acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 127 always possible use holdout always possible know sure whatexamplesbelongtotheconceptthatisactiveattimet interleaved testthentrain prequential individual instance used test model used training accuracy incrementally updated intentionally performed order model alwaysbeingtestedoninstancesithasnotseenbeforethisschemehastheadvantage holdout set needed testing making maximum use available data also ensures smooth plot accuracy time individual instance become increasingly less significant overall average prequential error computedbasedonanaccumulatedsumofalossfunctionbetweenthepredictionand observedvaluess cid80n fyˆy t1 three prequential evaluations using landmark window interleaved testthentrain sliding window forgetting mechanism holdout evaluation gives good estimation accuracy model recent data however quires recent test data difficult obtain real datasets case forgettingmechanismforestimatingholdoutaccuracygamaetal2013canbeused based prequential accuracy sliding window size w mostrecentobservationsorfadingfactorsthatweighobservationsusingadecayfac torαthesemechanismsgiveanestimationoftheaccuracythatisapproximatetothe accuracyestimationobtaineddoingaholdoutevaluation controlledpermutationsaveragingtheaccuracyovertimehasapotentialprob lem may mask adaptation properties adaptive learning algorithms ex ample one algorithm well first half data badly second another algorithm would show average consistent performance taking average accuracies would mask even prequential evaluation may produce biased results towards fixed order data sequence runs onlyonetestinafixedorderofdatatoreducethisriskcontrolledpermutationseval uationzliobaite2011brunsmultipletestswithrandomizedcopiesofadatastream theoretical way restricted different distributions original data areapproximatelypreservedinpermutationsrandomizationaimsatkeepingthein stances originally nearby time close together technique suits best todatastreamswithsuddendriftsbutisalsoapplicabletostreamswithotherdrifts controlledpermutationsallowtogeneratemultipletestsetsfortestingadaptivetech niques enable assessing volatility robustness models optimize pa rametersandthiswayreducetheriskofoverfittingtheorderofdatainasequence 422 crossvalidationwithalignedseriesofdata inmanycasesweuseanadaptivelearn ingtechniquetolearnnotasingleadaptivemodelforthedatastreamassociatedwith anindividualobjectegpredictingantibioticresistancewithinaparticularhospital learn multiple models one adaptive model per object eg want predict foodsalesforeachoftheproductinthestockorpopularityofeachmovinginamovie rentingstoreeachobjectcorrespondstoadatasteamonwhichweapplyanadaptive model data stream analogue dataset compare perfor mance different classification techniques multiple datasets set tings different data streams would still alike ie sharing feature space nature cases logical estimate performance theadaptivelearningstrategyacrossmultipleobjects method rather generic many real problems need ex act evaluation setting justifies use large computational resources exam ple eg sales recommendations advertising sentiment classification across topics sources figure 9 illustrates estimation generalization performance techniques across multiple individual related data streams cross validating acmcomputingsurveysvol1no1article1publicationdatejanuary2013 128 jgamaetal training part testing part series 1 trainingvalidation series 2 fold 1 series model selection parametrization fold 2 final test series estimating fold 3 generalization error series n fig9 estimatingthegeneralizationperformanceofthetechniquesacrossmultipleindividualbutre lateddatastreamsbycrossvalidatingthedatastreams data streams every individual data stream use one described prequential holdout controlled permutations evaluation procedures subset data streams used run learning strategy different parameter izations identify reasonable parameter settings particular domain andthenverifythemontheremainingtestingdatastreamsthevaluesoftheperfor mancemetricscanbeaveragedoverdatastreaminthetestbasketandaveragedover themultiplecrossvalidationruns 423 statisticalsignificance evaluating classifiers often concerned thestatisticalsignificanceoftheresultsforoneclassifierwemaycomputeconfidence intervalsontheerrorratethesamewayasinstationaryofflineclassificationhowever confidence intervals particularly relevant training sample size small whileindatastreamsitistypicallylargethereforeestimationofconfidenceintervals isnotdeemedtobeofprimaryimportance whencomparingtwoclassifierswemaybeinterestedinstatisticallyvalidatingthe differences achieved error rates mcnemar test mcnemar 1947 non parametrictestusedinthestreamminingforassessingthedifferencesinperformance two classifiers test needs store update two variables number instancesmisclassifiedbythefirstclassifierandcorrectlyclassifierbythesecondone aandthenumberofinstancesotherwayaroundbthemcnemarstatisticmisgiven asm signabab2abthetestfollowstheχ2 distribution whencomparingmorethantwoclassifiersthenemenyitestdemsar2006isused computing significance appropriate test comparing classifiers classifiersovermultipledatasetsbeingbasedontheaverageranksofthealgorithms across datasets nemenyi recommended use rejection null hypothesis friedman nemenyi test following two classifiers per forming differently corresponding average ranks differ least critical cid112 differencecd q kk16n wherek isthenumberoflearnersn isthenumber α ofdatasetsandcriticalvaluesq arebasedonthestudentizedrangestatisticdivided α 2othertestscanberecommendedwhenallclassifiersarecomparedtoacontrol classifierdemsar2006forexamplethedunnetttestcanbeuseddunnett1955 43 performancebenchmarking perform benchmarking comparing several methods need use large datasets software implementations algorithms two types dataset artificial real datasets artificial datasets useful give thegroundtruthofthedataforexampleallthepointswhenthechangesoccurhow ever real datasets interesting correspond realworld applications wherethealgorithmsusabilityistestedsomeauthorsusethetermrealworlddataset torefertodatasetsusingrealworlddatawithforceddriftsthatcannotbeconsidered asrealexamplesofdriftsandwouldratherbecalledrealdatasetswithsyntheticdrifts acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 129 appendixbpresentspopularartificialandrealdatasetsthatarepubliclyavailable online appendix c shows example evaluating classification concept drift usingmoamoaisanopensourcesoftwaretorundatastreamingexperimentsitis written java based experience weka contains methods classificationregressionclusteringandfrequentpatternmining 5 conclusions wepresentedtheconceptualcategorizationofmanyexistingadaptivelearningstrate gies capable handling concept drift along concrete stateoftheart tech niqueswehighlightedthepeculiaritiesoftheevaluationmethodologyforexperiment ingwithadaptivelearningtechniquesinthedatastreamsettings problem concept drift recognized different application domains interest adaptive learning boosted outcomes several centlyorganizedcontestsorchallengesinaicontrollingdriverlesscarsatthedarpa challenge data mining knowledge discovery credit risk assessment competition atpakdd09andrecommendersystemsnetflixmovierecommendationfieldswin ningteamsineachofthesecompetitionsemphasizethatoneofkeyfactorsintheirsuc cesswasduetoaddressingtemporaldynamicsandvarioushiddencontextsaffecting concepts interest therefore also referred conceptual categoriza tionoftypicalapplicationsettingsinwhichadaptivelearningsystemshavetooperate discussed lessons learnt real world cases handling concept drift hope besides serving introduction research area adaptive learning concept drift article help position new adaptive learning techniqueandapplicationsettingstowhichtheyapply work concept drift assumes changes happen hidden con text observable adaptive learning system hence concept drift consideredtobeunpredictableanditsdetectionandhandlingismostlyreactivehow ever various application settings concept drift expected reap pearalongthetimelineandacrossdifferentobjectsinthemodeleddomainseasonal effects vague periodicity certain subgroup object would common eg food demand prediction zliobaite et al 2012a availability external contextual information extraction hidden contexts predictive features may help betterhandlerecurrentconceptdriftegwithuseofametalearningapproach gama andkosina2011moreovertransferlearningbetweenthedifferentcontextsseemsa potentialresearchline temporalrelationshipsminingcanbeusedtoidentifyrelateddriftseginthedis tributed peertopeer settings concept drift one peer may precede drift related peers ang et al 2012 settings accurate moreproactiveandmoretransparentchangedetectionmaybecomepossible vast majority work concept drift detection summarized survey address problem representation bias common adaptive systems enforce suggest particular type behavior whenever reinforcement feedback closedloop control learning mechanism cannotevaluateandcomparetheperformanceofconceptdrifthandlingtechniquesby replaying historical data therefore speculate studies try embed concept drift handling technique real operational settings proper validation majority work handling concept drift focused onsupervisedsettingswithimmediateavailabilityoflabelstheactualproblemspace much wider unsupervised learning evolving data case delayed ondemand labeling supervised learning validation change detection adaptationmechanismsonlystarttobeinvestigated acmcomputingsurveysvol1no1article1publicationdatejanuary2013 130 jgamaetal research concept drift goes beyond areas machine learning data mining pattern recognition term originally coined studied thus process mining6 van der aalst 2012 2011 area research dealing different kinds analysis business processes extracting information event logs recorded information system handling concept drift recognizedasanimportantproblem carmonaandgavalda 2012boseetal2013 thenextchallengesforconceptdriftresearchincludeimprovingscalabilityrobust nessandreliabilitymovingfromsocalledblackboxadaptationtomoreinterpretable explainable adaptation reducing dependence timely accurate feed backtruelabelsandmovingfromadaptivealgorithmstowardsadaptivesystemsthat wouldautomatefullknowledgediscoveryprocessinadditiontoautomatingadaptation ofthedecisionmodelssomeofthesechallengeshavebeendiscussedinzliobaiteetal 2012b studying integrate expert knowledge concept drift handling interact domain experts brings new challenges well relying non inter pretable blackbox models popular among domain experts may need totrustthategacontrolsystemisreallygoingtoreacttochangeswhentheyhappen andtounderstandhowthesechangesaredetectedandwhatadaptationwouldhappen furthermore experts may valuable knowledge improve concept drift handling mechanism validate system continuous incremental learn ing nature adaptive systems makes challenging come strategy incorporate expert knowledge adaptive learning process communicate withexpertsastheprocessevolvesstudyinghowtoperformlocalizationandexplana tion changes eg diagnosing chances would helpful improving usability trustinadaptivelearningsystems references adaeiandbertholdm2013eveaframeworkforeventdetectionevolvingsystems46170 adomaviciusgandtuzhilina2005towardthenextgenerationofrecommendersystemsasurvey ofthestateoftheartandpossibleextensionsieeetransknowldataeng176734749 aggarwal c 2005 change diagnosis evolving data streams ieee trans knowl data eng17587600 aggarwalc2006onbiasedreservoirsamplinginthepresenceofstreamevolutioninprocofthe32nd intconfonverylargedatabasesvldb607618 agrawalrghoshspimielinskitiyerbrandswamian1992anintervalclassifierfor databaseminingapplicationsinprocofthe18thintconfonverylargedatabasesvldb560573 agrawal rimielinski tand swami a1993databaseminingaperformanceperspectiveieee transonknowlanddataeng56914925 alkatebmbyungsuklandwangx2007adaptivesizereservoirsamplingoverdatastreams inprocofintconfonscientificandstatisticaldatabasemanagementssbdm22 albergdlastmandkandela2012knowledgediscoveryindatastreamswithregressiontree methodswileyinterdisciplinaryreviewsdataminingandknowledgediscovery26978 ang hgopalkrishnan vzliobaite ipechenizkiy mand hoi s2012predictivehandling ofasynchronousconceptdriftsindistributedenvironmentsieeetransonknowlanddataeng babcockbbabusdatarmmotwanirandwidomj2002modelsandissuesindatastream systemsinprocofthe21stsigmodsigactsigartsymponprinciofdatabasesystpods116 bachshandmaloofma2008pairedlearnersforconceptdriftinprocofthe8thieeeintconf ondataminingicdm2332 bachekandlichmanm2013ucimachinelearningrepositorytechrep bassevillemandnikiforovi1993detectionofabruptchangestheoryandapplicationonline bessarjmirandavandgamaj2009entropyandcorrentropyagainstminimumsquareerrorin offlineandonline3dayaheadwindpowerforecastingieeetransonpowersyst24416571666 6httpwwwprocessminingorg acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 131 bifetaandfranke2010sentimentknowledgediscoveryintwitterstreamingdatainprocofthe 13thintconfondiscoveryscienceds115 bifet gavalda r2006kalmanfiltersandadaptivewindowsforlearningindatastreamsin procofthe9thintconfondiscoveryscienceds2940 bifetaandgavaldar2007learningfromtimechangingdatawithadaptivewindowinginprocof siamintconfondataminingsdm443448 bifet aholmes gkirkby rand pfahringer b2011adatastreamminingapractical approachtechnicalreportuniversityofwaikato bifet holmes g pfahringer b 2010a leveraging bagging evolving data streams procoftheeurconfonmachlearnandknowledgediscoveryindatabasesecmlpkdd135150 bifet holmes g pfahringer b frank e 2010bfastperceptrondecisiontreelearning fromevolvingdatastreamsinprocofthe14thpaconfonknowldiscovanddatamining299310 bifetaholmesgpfahringerbkirkbyrandgavaldar2009newensemblemethodsfor evolvingdatastreamsinprocoftheintconfonknowldiscovanddatamining139148 bifet aholmes gpfahringer bread jkranen pkremer hjansen tand seidl t2011bmoaarealtimeanalyticsopensourceframeworkinproceurconfonmachlearnand principlesandpracticeofknowledgediscoveryindatabasesecmlpkdd617620 billsusdandpazzanimj2000usermodelingforadaptivenewsaccessusermodelinganduser adaptedinteraction10147180 blum 1997 empirical support winnow weightedmajorityalgorithms results calendar schedulingdomainmachlearn261523 bobadilla j ortega f hernando gutierrez 2013 recommender systems survey knowbasedsyst46109132 bose r p j c van der aalst w p zliobaite pechenizkiy 2013dealingwith conceptdriftinprocessminingacceptedtoieeetransonneurnetandlearsyst bouchachiaa2011afuzzyclassificationindynamicenvironmentssoftcomput15510091022 bouchachiaa2011bincrementallearningwithmultileveladaptationneurocomp741117851799 bouchachia aprossegger mand duman h2010semisupervisedincrementallearninginin procoftheieeeintconfonfuzzysystemsfuzzieee16 bouchachia vanaret c2013gt2fcanonlinegrowingintervaltype2selflearningfuzzy classifierieeetransactionsonfuzzysystemsinpress breiman l 1999 pasting small votes classification large databases online machine learn ing3685103 breimanletal1984classificationandregressiontreeschapmanhallnewyork bremnesj2004probabilisticwindpowerforecastsusinglocalquantileregressionwinden714754 carmonajandgavaldar2012onlinetechniquesfordealingwithconceptdriftinprocessmining inadvancesinintelligentdataanalysisxi11thintsymplecturenotesincomputerscienceseries vol7619springer90102 carmonacejudo j baenagarcia del campoavila j bueno r bifet 2010 gnusmailopenframeworkforonlineemailclassificationinprocofthe19theurconfonartintell ecai11411142 carpenterggrossbergsandreynoldsj1991aartmapsupervisedrealtimelearningand classificationofnonstationarydatabyaselforganizingneuralnetworkneuralnetworks4565588 carpenterggrossbergsandrosend1991bfuzzyartfaststablelearningandcategoriza tionofanalogpatternsbyanadaptiveresonancesystemneuralnetworks46759771 carvalhovandcohenw2006singlepassonlinelearningperformancevotingschemesandonline featureselectioninprocofthe12thacmsigkddintconfonkndiscanddataminkdd548553 castilloggamajandbredaa2003adaptivebayesforastudentmodelingpredictiontaskbased onlearningstylesinprocofthe9thintconfonusermodelingum328332 cesabianchinandlugosig2006predictionlearningandgamescambridgeuniversitypress chandola vbanerjee aand kumar v2009anomalydetectionasurveyacmcomputersur veys411511558 chufandzanioloc2004fastandlightboostingforadaptiveminingofdatastreamsinprocofthe 5thpacasiaconfonknowledgediscoveryanddataminingpakdd282292 dasutkrishnansvenkatasubramaniansandyik2006aninformationtheoreticapproach todetectingchangesinmultidimensionaldatastreamsinprocofthe38thsympontheinterfaceof statisticscomputingscienceandapplications acmcomputingsurveysvol1no1article1publicationdatejanuary2013 132 jgamaetal datar gionis indyk p motwani r 2002 maintaining stream statistics sliding windowssiamjcomput3117941813 delany cunningham p tsymbal coyle l 2005acasebasedtechniquefortracking conceptdriftinspamfilteringknowledgebasedsystems1845187195 demsarj2006statisticalcomparisonsofclassifiersovermultipledatasetsjmachlearnres7130 domingospandhulteng2000mininghighspeeddatastreamsinprocofthe6thacmsigkdd intconfonknowledgediscoveryanddataminingkdd7180 driesaandruckertu2009adaptiveconceptdriftdetectionstatanaldatamin256311327 dudarhartpandstorkd2001patternclassificationwiley dunnettcw1955amultiplecomparisonprocedureforcomparingseveraltreatmentswithacontrol journaloftheamericanstatisticalassociation10961121 efraimidispandspirakisp2006weightedrandomsamplingwithareservoirinfproclett975 181185 elwellrandpolikarr2011incrementallearningofconceptdriftinnonstationaryenvironments ieeetransonneuralnetworks221015171531 fernaandgivanr2003onlineensemblelearninganempiricalstudymachlearn531271109 formang2006tacklingconceptdriftbytemporalinductivetransferinprocofthe29thintacmsigir confonresearchanddevelopmentininfretrievalsigir252259 french r 1994catastrophicforgettinginconnectionistnetworkscausesconsequencesandsolu tionsintrendsincognitivesciences128135 gaber mzaslavsky aand krishnaswamy s2005miningdatastreamsareviewsigmod rec341826 gamaj2010knowledgediscoveryfromdatastreamschapmanhallcrc gamajfernandesrandrochar2006decisiontreesforminingdatastreamsintelligentdata analysis1012346 gama j kosina p 2011 learning learning process proc 10th int conf advancesinintelligentdataanalysisidaspringer162172 gama jmedas pcastillo gand rodrigues p2004learningwithdriftdetectioninprocof the17thbraziliansymponartifintellsbia286295 gamajsebastiaorandrodriguesp2013onevaluatingstreamlearningalgorithmsmachine learning903317346 gantzjandreinseld2012idcthedigitaluniversein2020bigdatabiggerdigitalshadowsand biggestgrowthinthefareastinternationaldatacorporationsponsoredbyemccorporation gao j fan w han j yu p 2007 general framework mining conceptdrifting data streamswithskeweddistributionsinprocofthe7thsiamintconfondataminingsdm gehrke j ramakrishnan r ganti v 1998 rainforest framework fast decision tree constructionoflargedatasetsinprocofthe24rdintconfonverylargedatabasesvldb416427 giraudcarrierc2000anoteontheutilityofincrementallearningaicommun134215223 gomesjbruizemandsousapac2011learningrecurringconceptsfromdatastreamswith acontextawareensembleinprocoftheacmsymponapplcompsac994999 grisogono am2006theimplicationsofcomplexadaptivesystemstheoryforc2instateoftheart stateofthepracticevolccrts hall frank e holmes g pfahringer b reutemann p witten h 2009 wekadataminingsoftwareanupdatesigkddexplornewsl1111018 harriesm1999splice2comparativeevaluationelectricitypricingtechrepsouthwalesuniv harriesmsammutcandhornk1998extractinghiddencontextmachinelearning32101126 helmbolddpandlongpm1994trackingdriftingconceptsbyminimizingdisagreementsmach learn1412745 herbstermandwarmuthm1998trackingthebestexpertmachinelearning322151178 hulten gspencer land domingos p2001miningtimechangingdatastreamsinprocofthe 7thacmsigkddintconfonknowledgediscoveryanddataminingkdd97106 ikonomovskaegamajanddzeroskis2011learningmodeltreesfromevolvingdatastreams dataminingknowledgediscovery231128168 joachims 2000estimatingthegeneralizationperformanceofansvmefficientlyinprocofthe17th intconfonmachlearnicml431438 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 133 kadlecpgrbicrandgabrysb2011reviewofadaptationmechanismsfordatadrivensoftsen sorscomputerschemicalengineering351124 katakisitsoumakasgandvlahavasi2010trackingrecurringcontextsusingensembleclassi fiersanapplicationtoemailfilteringknowledgeandinformationsystems223371391 kelly g hand j adams n 1999 impact changing populations classifier performanceinprocofthe5thacmsigkddintconfonknowldiscanddatminkdd367371 kifer dbendavid sand gehrke j2004detectingchangeindatastreamsinprocofthe13th intconfonverylargedatabasesvldb180191 klinkenberg r 2003 predicting phases business cycles concept drift proc ann workshonmachinelearningofthenationalgermancomputersciencesocietyllwa310 klinkenberg r2004learningdriftingconceptsexampleselectionvsexampleweightingintelligent dataanalysis83281300 klinkenbergrandjoachimst2000detectingconceptdriftwithsupportvectormachinesinproc ofthe17thintconfonmachinelearningicml487494 klinkenbergrandrenzi1998adaptiveinformationfilteringlearninginthepresenceofconcept driftsinworkshopnotesoftheicmlaaai98workshoponlearningfortextcategorization3340 kolter j maloof 2003 dynamic weighted majority new ensemble method tracking conceptdriftinprocofthe3rdieeeintconfondataminingicdm123130 kolterjandmaloofm2005usingadditiveexpertensemblestocopewithconceptdriftinprocof the22thintconfonmachinelearningicml449456 kolterjandmaloofm2007dynamicweightedmajorityanensemblemethodfordriftingconcepts journalofmachinelearningresearch827552790 koreny2010collaborativefilteringwithtemporaldynamicscommunacm5348997 kosina p gama j sebastiao r 2010driftseveritymetricinprocofthe19theurconfon artificialintelligenceecai11191120 koychevi2000gradualforgettingforadaptationtoconceptdriftinprocofecaiworkshoponcurrent issuesinspatiotemporalreasoning101106 koychevi2002trackingchanginguserintereststhroughpriorlearningofcontextinprocofthe2nd intconfonadaptivehypermediaandadaptivewebbasedsystems223232 kukarm2003driftingconceptsashiddenfactorsinclinicalstudiesinprocofaime20039thconfer enceonartificialintelligenceinmedicineineurope355364 kuncheva l 2008classifierensemblesfordetectingconceptchangeinstreamingdataoverviewand perspectivesinprocofthe2ndworkshopsuema2008 kuncheva l zliobaite i2009onthewindowsizeforclassificationinchangingenvironments intelligentdataanalysis136861872 kunchevali2004classifierensemblesforchangingenvironmentsinprocofthe5thintworkshon multipleclassifiersystemsmcs115 kuncheva l 2009 using control charts detecting concept change streaming data tech rep bcstr0012009schoolofcomputersciencebangoruniversityuk kunchevali2013changedetectioninstreamingmultivariatedatausinglikelihooddetectorsieee transactionsonknowledgeanddataengineering25 kunchevaliandplumptonco2008adaptivelearningrateforonlinelineardiscriminantclassi fiersinprocofintworkshonstructuralandsyntacticpatternrecognitionsspr510519 lanquillon c 2002 enhancing text classification improve information filtering kunstliche intelli genz1623738 lazarescummvenkateshsandbuihh2004usingmultiplewindowstotrackconceptdrift intelligentdataanalysis812959 leeuwenmandsiebesa2008streamkrimpdetectingchangeindatastreamsinprocoftheeur confonmachlearnandknowledgediscoveryindatabasesecmlpkdd672687 lindstrom p delany j namee b 2010 handling concept drift text data stream constrainedbyhighlabellingcostinprocofthe23rdintfloridaartintellresearchsocietyconf littlestonen1987learningquicklywhenirrelevantattributesaboundanewlinearthresholdalgo rithmmachinelearning24285318 littlestonenandwarmuthm1994theweightedmajorityalgorithminfcomput1082212261 maloof michalski r2000selectingexamplesforpartialmemorylearningmachinelearn ing412752 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 134 jgamaetal maloof michalski r2004incrementallearningwithpartialinstancememoryartificialin telligence15495126 maloofma2010theaqmethodsforconceptdriftinadvancesinmachinelearningidedicatedto thememoryofprofessorryszardsmichalski2347 maloof michalski r s1995amethodforpartialmemoryincrementallearningandits applicationtocomputerintrusiondetectionininprocofthe7thieeeintconfontoolswithartif intell392397 markoumandsinghs2003noveltydetectionareviewpart1statisticalapproachessignalpro cessing8324812497 masud gao j khan l han j thuraisingham b 2011classificationandnovelclass detectioninconceptdriftingdatastreamsundertimeconstraintsieeetkde236859874 mcnemar q 1947noteonthesamplingerrorofthedifferencebetweencorrelatedproportionsorper centagespsychometrika122153157 mehta agrawal r rissanen j 1996 sliq fast scalable classifier data mining procofthe5thintconfonextendingdatabasetechnoladvancesindatabasetechnoledbt1832 minku l white yao x 2010 impact diversity online ensemble learning presenceofconceptdriftieeetransactionsonknowledgeanddataengineering22730742 minkulandyaox2011dddanewensembleapproachfordealingwithconceptdriftieeetrans actionsonknowledgeanddataengineering244619633 monteiro c bessa r miranda v botterud wang j conzelmann g 2009wind powerforecastingstateoftheart2009techrepanldis101argonnenationallaboratory morenotorresjgraedertalaizrodriguezrchawlanvandherreraf2012a unifyingviewondatasetshiftinclassificationpatternrecognition451521530 mouss hmouss dmouss nand sefouhi l2004testofpagehinkleyanapproachforfault detectioninanagroalimentaryproductionsysteminprocoftheasiancontrolconference815818 muthukrishnansvandenbergeandwuy2007sequentialchangedetectionondatastreams inworkshopprocofthe7thieeeintconfondataminingicdmw551550 ngwanddashm2008atestparadigmfordetectingchangesintransactionaldatastreamsinproc ofthe13thintconfondatabasesystemsforadvancedapplicationsdasfaa204219 nishidakandyamauchik2007detectingconceptdriftusingstatisticaltestinginproceedingsofthe 10thinternationalconferenceondiscoveryscienceds07springerverlagberlinheidelberg264269 ozan2001onlineensemblelearningphdthesisuniversityofcaliforniaberkeley pagees1954continuousinspectionschemesbiometrika4112100115 pechenizkiy mbakker jzliobaite iivannikov aand karkkainen t2009onlinemass flowpredictionincfbboilerswithexplicitdetectionofsuddenconceptdriftsigkddexplor112 109116 polikar r udpa l udpa member member honavar v 2001learnanin crementallearningalgorithmforsupervisedneuralnetworksieeetronsysmanandcyberc31 497508 rosenblatt f 1958theperceptronaprobabilisticmodelforinformationstorageandorganizationin thebrainpsychologicalreview656386408 ross g j adams n tasoulis k hand j 2012 exponentially weighted moving averagechartsfordetectingconceptdriftpatternrecognlett33191198 rusu f dobra a2009sketchingsampleddatastreamsinprocofthe2009ieeeintconfon dataengicde381392 salganicoffm1993densityadaptivelearningandforgettinginprocoftheintconfonmachlearn icml276283 salganicoffm1997toleratingconceptandsamplingshiftinlazylearningusingpredictionerrorcon textswitchingartificialintelligencereview1115133155 schlimmerjandgrangerr1986incrementallearningfromnoisydatamachlearn13317354 scholzmandklinkenbergr2007boostingclassifiersfordriftingconceptsintelldataana111 328 sebastiaorandgamaj2007changedetectioninlearninghistogramsfromdatastreamsinprogress inartificialintelligenceprocoftheportugueseconfonartintell112123 shaferjcagrawalrandmehtam1996sprintascalableparallelclassifierfordatamining inprocofthe22thintconfonverylargedatabasesvldb544555 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation 135 shiryaeva2009onstochasticmodelsandoptimalmethodsinthequickestdetectionproblemstheory probabappl533385401 streetwandkimy2001astreamingensemblealgorithmseaforlargescaleclassificationinproc 7thacmsigkddintconfonknowledgediscoveryanddataminingkdd377382 syednliuhandsungk1999handlingconceptdriftsinincrementallearningwithsupportvector machinesinprocofthe5thacmsigkddintconfonknowldiscanddataminingkdd317321 tartakovsky moustakides g 2010 stateoftheart bayesian changepoint detection se quentialanalysis29125145 thrun montemerlo h dahlkamp stavens aron j diebel p fong j gale mhalpennyghoffmannklaucoakleympalatuccivprattpstangsstrohband c dupont l jendrossek et al 2006 stanley robot darpa challenge j field robot239661692 tsymbala2004theproblemofconceptdriftdefinitionsandrelatedworktechnicalreportdepartment ofcomputersciencetrinitycollegedublin tsymbalapechenizkiymcunninghampandpuuronens2006handlinglocalconceptdrift withdynamicintegrationofclassifiersdomainofantibioticresistanceinnosocomialinfectionsinproc of19thieeeintsymponcomputerbasedmedicalsystcbms679684 van der aalst w p 2011processminingdiscoveryconformanceandenhancementofbusiness processesspringer vanderaalstwmp2012processminingcommunacm5587683 vitterj1985randomsamplingwithareservoiracmtransmathsoftw1113757 vorburger p bernstein 2006entropybasedconceptshiftdetectioninprocofthe6thint confondataminingicdm11131118 vovkv1998agameofpredictionwithexpertadvicejcomputsystsci562153173 walda1947sequentialanalysisjohnwileyandsonsinc wanghfanwyupandhanj2003miningconceptdriftingdatastreamsusingensembleclas sifiersinprocofthe9thacmsigkddintconfonknowldiscanddataminingkdd226235 widmerg1997trackingcontextchangesthroughmetalearningmachlearn273259286 widmergandkubatm1993effectivelearningindynamicenvironmentsbyexplicitcontexttracking inprocoftheeurconfonmachlearnecml227243 widmer g kubat 1996learninginthepresenceofconceptdriftandhiddencontextsmach learn23169101 yangywuxandzhux2006mininginanticipationforconceptchangeproactivereactivepredic tionindatastreamsdataminingandknowledgediscovery133261289 yao rshi qshen czhang yand van den hengel a2012robusttrackingwithweighted onlinestructuredlearninginprocofthe12theurconfoncomputervisioneccv158172 zeiragmaimonolastmandrokachl2004changedetectioninclassificationmodelsinduced fromtimeseriesdataindataminingintimeseriesdatabasesvol57101125 zhangpzhuxandshiy2008categorizingandminingconceptdriftingdatastreamsinprocof the14thacmsigkddintconfonknowldiscanddataminingkdd812820 zhang z zhou j 2010transferestimationofevolvingclasspriorsindatastreamclassification patternrecogn43931513161 zhaophoisjinrandyangt2011onlineaucmaximizationinprocofthe28thintconfon machinelearningicml233240 zliobaitei2009learningunderconceptdriftanoverviewtechnicalreportvilniusuniversity zliobaitei2011acombiningsimilarityintimeandspacefortrainingsetformationunderconceptdrift intelligentdataanalysis154589611 zliobaitei2011bcontrolledpermutationsfortestingadaptiveclassifiersinprocofthe14thintconf ondiscoveryscienceds365379 zliobaite bakker j pechenizkiy 2012a beating baseline prediction food sales howintelligentanintelligentpredictorisexpertsystappl391806815 zliobaite ibifet agaber mgabrys bgama jminku l land musial k2012b nextchallengesforadaptivelearningsystemssigkddexplorations1414855 zliobaiteibifetapfahringerbandholmesg2013activelearningwithdriftingstreaming dataieeetransonneuralnetworksandlearningsystems zliobaiteiandkuncheval2009determiningthetrainingwindowforsmallsamplesizeclassifica tionwithconceptdriftinprocofieeeintconfondataminingworkshopsicdmw447452 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 online appendix survey concept drift adaptation joaogamauniversityofportoportugal indre zˇliobaite aaltouniversityfinland albertbifetyahooresearchbarcelonaspain mykolapechenizkiyeindhovenuniversityoftechnologythenetherlands abdelhamidbouchachiabournemouthuniversityuk pseudocodeofconceptdriftalgorithms algorithm1pagehinkleyalgorithm input admissiblechangeδdriftthresholdλlossatexamplete output drifttruefalse initializetheerrorestimators sr00m 00m 1 pagehinkleytest letsrtsrt1rt letm tm t1rt srt δ letm minm ifm tm λthen drifttrue else driftfalse end algorithm2theadwin changedetectionalgorithm begin initializewindoww foreach t0do w w x ieaddx totheheadofw repeat dropelementsfromw untilµˆ µˆ cid15 holdsforeverysplitofw intow w w w0 w1 cut 0 1 end outputµˆ w end cid13c 2013acm03600300201301art11000 doi10114500000000000000 httpdoiacmorg10114500000000000000 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 app2 jgamaetal algorithm3thespcchangedetectionalgorithm input currentdecisionmodellxyttrainingsetwithclassyy αandβparameters begin letx bethecurrentinstanceandyˆ lx betheprediction leterror eyˆy computemeanp andvarianceσ ofwitherror ifp σ p σ min min p p andσ min min end ifp σ p βσ min min incontrolwarningfalseupdatelwiththeinstancex else ifp σ p ασ min min ifnotwarningthen warningwarningtruestartbufferx else warningbufferbufferx j j end else outcontroltrainanewdecisionmodelusingtheinstancesinthebuffer warningfalserestartp andσ min min end end end algorithm5algorithmfordynamicweightedmajoritydwm input xy1trainingdatafeaturevectorandclasslabelcnnumberofclassesc2β n factorfordecreasingweights0β 1θthresholdfordeletingexpertspperiodbetween expertremovalcreationandweightupdateew1 setofexpertsandtheirweights λλ1cglobalandlocalpredictions σ rcm sumofweightedpredictionsforeach class begin m1 e createnewexpert w 1 foreachi1ndo σ 0 foreachi1mdo λclassifye x j ifλcid54y mod p0then w βw j j end σ σ w λ λ j end λargmax σ j j ifi mod p0then w normalizeweightsw ewremoveexpertsewθ ifλcid54y mm1 e createnewexpert w 1 end end foreachj1mdo e traine x j j acmcomputingsurveysvol1no1article1publicationdatejanuary2013 end returnλ end end asurveyonconceptdriftadaptation app3 algorithm4thecvfdtalgorithm intput isasequenceofexamples x isasetofsymbolicattributes gisasplitevaluationfunction δ isoneminusthedesiredprobabilityof choosingthecorrectattributeatanygivennode τ isausersuppliedtiethreshold w isthesizeofthewindow n istheexamplesbetweenchecksforgrowth min f istheexamplesbetweenchecksfordrift output ht isadecisiontree procedure cvfdtsxgδτwn min initialize letht beatreewithasingleleafl theroot 1 letaltl beaninitiallyemptysetofalternatetreeforl 1 1 letw bethewindowofexamplesinitiallyempty inicializesufficientstatisticstocomputeg processtheexamples foreachexamplexyins sortxyintoasetofleaveslusinght andalltreesinalt ofanynodexypasses letidbethemaximumidoftheleavesinl addxyidtothebeginningofw ifwwthen letx id bethelastelementofw w w w forgetexampleshtnx id w w w letw w withx id removed w w w end cvfdtgrowhtngxyδn τ min iftherehavebeenf examplessincethelastcheckingofalternatetreesthen checksplitvalidityhtnδ end end returnht b datasetsforconceptdrift b1 synthetic synthetic data several benefits easy reproduce bear low cost storage transmission importantly synthetic data provides advan tage knowing ground truth instance know exactly concept drift happens type drift best classification accura cies achievable concept main limitation synthetic data uncertainty weathercorrespondingdriftshappeninreality next present seven popular models generating synthetic data concept drifttheircharacteristicsaresummarizedintablevi seaconceptsgenerator street kim 2001 models abrupt real concept drifts three independent real valued attributes 010 first two tributes relevant prediction original data model produce four dif ferentconceptstheclassdecisionboundaryisdefinedasx x θwherex 1 2 1 x arethefirsttwoattributesandθ isathresholdvaluedifferentforeachconcept 2 1θ 92θ 83θ 7and4θ 95 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 app4 jgamaetal algorithm6theaddexpalgorithmfordiscreteclasses inputxyt atrainingsetwithclassyy β 01factorfordecreasingweightsτ 01lossrequiredtoaddanewexpert begin settheinitialnumberofexpertsn 1 1 settheinitialexpertweightw 1 11 fort1tot getexpertpredictionscid15 cid15 t1 tnt computepredictionyˆ argmax cid80nt w ccid15 cy i1 ti ti updateexpertsweightsw t1i w tiβytcid54cid15ti ifyˆ cid54y addanewexpert n n 1 t1 w γcid80nt w t1nt1 i1 ti end traineachexpertoninstancex end end algorithm7theaddexpalgorithmforcontinuousclasses inputxyt atrainingsetwithclassy01β 01factorfordecreasingweights γ 01factorfornewexpertweightτ 01lossrequiredtoaddanewexpert begin settheinitialnumberofexpertsn 1 1 settheinitialexpertweightw 1 11 fort1tot getexpertpredictionscid15 cid15 01 t1 tnt computepredictionyˆ cid80n 1wticid15ti cid80n 1wti sufferlosscid107yˆ cid107 updateexpertsweightsw t1i w tiβcid107cid15tiytcid107 ifcid107yˆ cid107τ addanewexpert n n 1 t1 w γcid80nt w cid107cid15 cid107 t1nt1 i1 ti ti end traineachexpertoninstancex end end tablevimodelsforsyntheticdatageneration name ofconcepts task typeofdrift realcd virtualcd priors sea 4 classification2 cid88 cid88 stagger 3 classification2 cid88 cid88 rotatinghyperplane classification2 cid88 rbfgenerator classificationany cid88 cid88 cid88 functiongenerator 10 classification2 cid88 cid88 ledgenerator classification10 cid88 waveformgenerator classification3 cid88 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation app5 staggerconceptsgenerator schlimmer granger 1986 models abrupt real concept drifts three independent categorical attributes size small medium large color red green blue shape square circular triangular binaryclassificationtaskisdefinedbyadisjunctofconjunctstherearethreecon cepts 1 positive class size small color red 2 color green shape circular3sizemediumorsizelarge rotatinghyperplane first used test cvfdt vfdt hulten et al 2001 generic model follows data generated uniformly hyperplane ddimensional real space decision boundary defined cid80d w x w i1 0 cid80d w x ith attribute examples cid80d w x w i1 i1 0 labeled positive examples cid80d w x w labeled negative i1 0 concept changes introduced modifying weights w need sat isfy constraint w cid80d w keep prior probabilities fixed hyperplanes 0 i1 flexible used simulating various kinds concept drifts eg abrupt gradual reoccurring changing orientation position hyperplaneviaweightsw noisecanbeaddedbyrandomlyswappingclasslabels randomrbfgenerator bifetetal2009wasdevisedtoofferanalternatecomplex concepttypethatisnotstraightforwardtoapproximatewithadecisiontreemodel afixednumberofrandomcentroidsaregeneratedinddimensionalrealspaceeach centerischaracterizedbyarandompositionasinglestandarddeviationclasslabel prior probability weight new examples generated picking center randomwithagivenpriorprobabilitythedirectionofanoffsetischosenatrandom centroid following normal distribution zero mean given standarddeviationtheexampleislabelledthesameasthecentroidconceptdrift isintroducedbymovingthecentroidsaroundovertime functiongenerator agrawal et al 1992 used popular data model early work scaling decision tree learners agrawal et al 1993 mehta et al 1996 shaferetal1996gehrkeetal1998thegeneratorproducesastreamcontaining nine attributes six numeric three categorical although explicitly stated authors sensible conclusion attributes describe hypothetical loanapplicationstherearetenfunctionsdefinedforgeneratingbinaryclasslabels attributes presumably meaning approval os loan original data modelhasnodriftconceptdriftmaybeintroducedbyswitchingbetweenlabeling functionsovertime ledgenerator originates cart book breiman et al 1984 imple mentationincisavailablefromtheucirepositorybacheandlichman2013the goal predict digit displayed sevensegment led display attributehasa10chanceofbeinginvertedtheoriginaldatamodelhasnodrift driftmaybeintroducedbyswappingthepositionsofattributes waveformgenerator originates cart book breiman et al 1984 available uci repository bache lichman 2013 classification task distinguish three classes waveform gener ated combination two three base waves two versions problemthefirstonehas21numericattributesallofwhichincludenoisethesec ond one 21 attributes addition 19 irrelevant attributes originaldatamodelhasnodriftdriftmaybeintroducedbyswappingthepositions ofattributes implementationsofthesedatamodelsareavailableinmoabifetetal2011b acmcomputingsurveysvol1no1article1publicationdatejanuary2013 app6 jgamaetal b2 realworlddata nowadays easier past find large realworld datasets pub lic benchmarking concept change uci machine learning repository bache andlichman2013containssomerealworldbenchmarkdataforevaluatingmachine learningtechniqueshowevertheyarenotlargedatasets main real data domains large datasets mentioned concept drift pageofwikipedia7 arethefollowingones textmining documents text contains words combinations words featurestousetoprocessthedataacollectionoftextminingdatasetswithconcept drift maintained katakis8 data recollected twitter may also consideredfortextmining electricity widely used dataset electricity market dataset introduced harries 1999 time series based data collected australian new south wales electricity market available j gama9 market pricesarenotfixedandareaffectedbydemandandsupplyofthemarkettheprices market set every five minutes elec2 dataset contains 45312 stanceseachexampleofthedatasetreferstoaperiodof30minutesiethereare 48instancesforeachtimeperiodofonedaytheclasslabelidentifiesthechangeof thepricerelatedtoamovingaverageofthelast24hourstheclasslevelonlyreflect deviationsofthepriceonaonedayaverageandremovestheimpactoflongerterm pricetrends emailspam datasets email messages used predict unsolicited mes sagesornotforexampleecuespamdatasetsaretwodatasetseachconsistingof morethan10000emailscollectedoveraperiodofapproximately2yearsreferring toaoneusercompiledbysjdelany10 businessoriented datasetsthatcontainsdatausedindecisionmanagementsystems incompaniespakdd09competitiondataset11 isusedforacreditevaluationtask collected five year period unfortunately true labels released onlyfor thefirstpartof thedataanotherdataset calledairlinecontainsapproxi mately 116 million flight arrival departure records cleaned sorted iscompiledbyeikonomovska12 games datasets obtained online nononline games example dataset chesscomonlinegamescompiledbyizˇliobaite13 c evaluationexample give example evaluation data stream classification using moa soft ware framework bifet et al 2011b moa opensource framework dealing massive evolving data streams moa related weka hall et al 2009 waikato environment knowledge analysis awardwinning open sourceworkbenchcontainingimplementationsofawiderangeofbatchmachinelearn ingmethods 7httpenwikipediaorgwikiconcept_driftretrieved12112012 8httpmlkdcsdauthgrconcept_drifthtml 9httpwwwliaadupptkdusproductsdatasetsforconceptdrift 10httpwwwcompditieaigrouppage_id729 11httpsedeneurotechcombr443pakdd2009arquivodomethodload 12httpktijssielena_ikonomovskadatahtml 13httpssitesgooglecomsitezliobaiteresources1 acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation app7 moa enables evaluation data stream classification algorithms large streams intheorderoftensofmillionsofinstancesunderexplicitmemorylimitsanylessthan thisdoesnotactuallytestdatastreamalgorithmsinarealisticallychallengingsetting moaiswritteninjavathemainbenefitsofjavaareportabilitywhereapplications canberunonanyplatformwithanappropriatejavavirtualmachineandthestrong andwelldevelopedsupportlibrariesuseofthelanguageiswidespreadandfeatures suchasautomaticgarbagecollectionhelptoreduceprogrammerburdenanderror moa contains stream generators classifiers evaluation methods figure 10 showsthemoagraphicaluserinterfaceacommandlineinterfaceisalsoavailable fig10 moagraphicaluserinterface considering data streams data generated pure distributions moa models aconceptdriftasaweightedcombinationoftwopuredistributionsthatcharacterizes target concepts drift within framework possible definetheprobabilitythatinstancesofthestreambelongtothenewconceptafterthe driftusingthesigmoidfunctionasanelegantandpracticalsolutionbifetetal2009 moa contains popular data generators described appendix b moa streams built using generators reading arff files joining several streams filter ing streams allow simulation potentially infinite sequence data following generators currently available random tree generator sea con cepts generator stagger concepts generator rotating hyperplane random rbf generatorledgeneratorwaveformgeneratorandfunctiongenerator moacontainsseveralclassifiermethodssuchasnaivebayesdecisionstumpho effdingtreehoeffdingoptiontreeadaptivehoeffdingtreebaggingboostingbag gingusingadwinbifetetal2009andleveragingbaggingbifetetal2010a acmcomputingsurveysvol1no1article1publicationdatejanuary2013 app8 jgamaetal fig11 evaluationforastreamofamillionofinstancescomparingholdoutprequentialwithlandmark windowandprequentialwithslidingwindow figure 11 shows comparison holdout evaluation prequential evalu ationusingalandmarkwindowinterleavedtestthentrainandaprequentialevalu ationusingaslidingwindowofsize1000weobservethattheprequentialevaluation usingaslidingwindowisagoodapproximationtotheholdoutevaluation run also moa following experiment simulating concept drift scenario aprequentialevaluationusingaslidingwindowofsize1000forastreamofamillion instances generated random rbf generator following learners hoeffdingtreeadaptivehoeffdingtreeandadwinbaggingandleveragingbagging stream evolving centroids moving constant speed 104 speed defined distance moved new instance arrives initialized byadriftparameter figure 12 shows accuracy kappa statistic ramhours experiment observe hoeffding tree method lower capacity adaption en semble methods perform better single classifiers higher cost ramhours leveraging bagging method higher accuracy kappa statisticbutthenumberofresourcesthatitneedsisconsiderablylargerdatastream evaluationisatwodimensionalprocesswithatradeoffbetweenaccuracyresultsand resourcecosts acmcomputingsurveysvol1no1article1publicationdatejanuary2013 asurveyonconceptdriftadaptation app9 fig12 prequentialevaluationforarbfstreamofamillionofinstanceswherethecentersaremoving withaspeedof104 acmcomputingsurveysvol1no1article1publicationdatejanuary2013

infoh515 big data analytics introduction big data analytics gianlucabontempi machinelearninggroup boulevarddetriomphecp212 httpmlgulbacbe infoh515 second part secondpartofthecoursebigdata distributeddatamanagementand scalableanalytics required firstpartofthecoursethroughputlatencydistributedcomputing mapreducelambdaarchitecturessparksparkstreamingscalability foundationsofmachinelearninginfof422 programmingskillsinpythonspark fromdatamanagementperspectivetoanalyticsperspective exam 1 someoralquestionsintheuvexam 2 projectonllm 252 introduction ml big data analytics whatshouldwekeepinmindabouttheinfof422coursestatistical foundationsofmachinelearning isanadditionalcoursereallynecessary isbigdatajustanhype whatisnew whatisnotnew 452 big big pragmaticmovingtargetdefinition wheneitherthedataistoolarge tofitonasinglemachineoritwouldsimplytaketoolongtoperform thatcomputationonasinglemachine dataisbigwhenconventionalsinglemachinecomputingisnomore feasibleoreffective conventionalcomuputinglimitation dataprocessingalgorithmswithwidedatadependenciessufferbecause networktransferratesareordersofmagnitudeslowerthanmemoryaccesses asthenumberofmachinesworkingonaproblemincreasestheprobability ofafailureincreases areallanalyticsproblemsbigdataproblems ofcoursenot therearestillmanycompaniesbusinessdisciplinesclaimingbigdata problemsthoughthosearesimpleconventionalanalyticsproblems 552 big big librarieslikepandasarenotequippedtodealwithdatanotfittinginthe ram 652 new bigdataanalyticsremainsaninstanceofastatisticalmodelingproblem whichaimsatinferringestimatorspredictorsfromobservations usefulnotionstokeepinmind estimatorasfunctionofdata supervisedunsupervised datapreparation dimensionalityreduction generalizationbiasvarianceunderfittingoverfitting modelselectionandassessment iterativerefinements 752 new specialfocusonbigdataissues volume intermsofvariablesandorsamples itcallsforscalable storageandadistributedapproachtolearning velocity itistherateatwhichdataflowsinto streamingdatatasks onlinelearning variety beyondconventionalnumericmeasurementsheterogenous sourcesblogpoststweetssocialnetworkinteractionsphotoslogs beyondaccuracy performanceeg throughputlatency distributionofthecomputation scalability newprogramminglanguagesparadigmsandtools 852 new manyofthebasicassumptionsmadeinsinglenodesystemsarenomore valid partitioningofdataacrossmanynodes algorithmswithdata dependenciessufferfromthefactthatnetworktransferratesare ordersofmagnitudeslowerthanmemoryaccesses nosharedmemory asthenumberofmachinesworkingonaproblemincreasesthe probabilityofafailureincreases programmingparadigmadaptedtounderlyingsystemwhichmakes easytowritehighlyparallelcode mostliteratureisadhocspecificlearningmachineandprovidesno generalapproachfordistributingmachinelearning 952 ml crash course supervised vs unsupervised tasks supervised regression classification timeseriesforecasting unsupervised clustering outlierdetection densityestimation forenextendedintroductiontothetopicreferto 1152 supervised learning setting input output prediction unknown error dependency training dataset model prediction 1252 machine learning pipeline phenomenon problem formulation experimental design raw data preprocessing model parametric generation identification data model validation model selection model 1352 preprocessing thisstepismoreandmoreimportantandtimeconsumingasdatasize grows datacollection datacleaning missingdatamanagement dataformatting 1452 tabular data formats weassumethattrainingdatasetd canbeformattedas n x x x 1 11 12 1n x x x 2 21 22 2n x x x x n n1 n2 nn wherenisthenumberofobservationsandnisthenumberoffeatures 1552 simple regression task 1652 underfittingoverfitting 1752 underfittingoverfitting 1852 underfittingoverfitting 1952 learning procedure alearningprocedureaimsat 1 choosingaparametricfamilyofhypothesishxαwhichcontainsor givesgoodapproximationoftheunknownfunctionfstructural identification 2 withinthefamilyhxαestimatingonthebasisofd theparameter n α whichminimisesthetrainingerrorparametricidentification n twonestedloops 1 anexternalstructuralidentificationloopthroughdifferentmodel structures 2 aninnerparametricidentificationloopsearchingforthebest parametervectorwithinthefamilystructure 2052 parametric identification empiricalriskminimizationerm cid91 α αd argminmise α n n emp αλ itminimizestheempiricalriskortrainingerror mcid91 ise α cid80n i1y ihx iα2 emp n onthebasisofd n trainingerrorisabiasedie optimisticestimationofthereal generalizationerrormise 2152 validation techniques howtomeasuremiseinareliablewayonafinitedataset testing testingsequenceindependentofd andwiththesame n distribution holdout partitionofd intotwomutuallyexclusivesubsets training n setd andtestsetd tr nts kfoldcrossvalidation israndomlydividedintokmutually n exclusivetestpartitionsofapproximatelyequalsize thecasesnot foundineachtestpartitionareusedfortrainingthehypothesiswhich isthentestedonthepartitionitself 2252 model selection finalchoiceofthemodelstructureaftermodelgenerationand validation twoapproaches 1 winnertakesallapproach choosethemodelstructurethatminimizean accurateestimateofthegeneralizationerroreg crossvalidation 2 combinationofestimatorsapproach combinationeg averagingof differentmodelseg differentordersdifferentfamily 2352 model combination thewinnertakesallapproachisintuitivelytheapproachwhichshould workthebest accuracyofthefinalmodelcanbeimprovednotbychoosingthemodel structurewhichisexpectedtopredictthebestbutbycreatingamodel whoseoutputisthecombinationoftheoutputofdifferentmodels rationale anychosenhypothesishα isonlyanestimateofthereal n targetandlikeanyestimateisaffectedbyabiasandavarianceterm seeintheoreticalresultsonthecombinationofestimators baggingisawellknownexampleofmodelcombination 2452 bagging bootstrap aggregating consideradatasetd andalearningproceduretobuildanhypothesis n α fromd n n asetofbrepeatedbootstrapsamplesdb b1baretakenfrom n n amodelαb isbuiltforeachdb n n afinalpredictorisbuiltbyaggregatingthebmodelsαb n intheregressioncasethebaggingpredictoris b h x 1 cid88 hxαb bag b n b1 intheclassificationcaseamajorityvoteisused 2552 feature selection therearemanypotentialbenefitsoffeatureselection facilitatingdatavisualizationanddataunderstanding reducingthemeasurementandstoragerequirements reducingtrainingandutilizationtimesofthefinalmodel defyingthecurseofdimensionalitytoimprovepredictionperformance 2652 feature selection strategies 1 filtermethods preprocessingmethods theyattempttoassessthe meritsoffeaturesfromthedataignoringtheeffectsoftheselected featuresubsetontheperformanceofthelearningalgorithm examples aremethodsthatselectvariablesbyrankingthemthrough compressiontechniqueslikepcaorclusteringorbycomputing correlationwiththeoutput 2 wrappermethods thesemethodsassesssubsetsofvariables accordingtotheirusefulnesstoagivenpredictor themethodconducts asearchforagoodsubsetusingthelearningalgorithmitselfaspartof theevaluationfunction 3 embeddedmethods selectionaspartofthelearningprocedureand areusuallyspecifictogivenlearningmachines examplesare classificationtreesrandomforestsandmethodsbasedon regularizationtechniqueseg lasso 2752 issues unbalancedness nonstationarityconceptdrift semisupervisedlearningactivelearning 2852 big data analytics applications big data analytics applications someexamples frauddetection buildamodeltodetectcreditcardfraudusing thousandsoffeaturesandbillionsoftransactions bioinformatics easilymanipulategenomicdatafromthousandsof peopletodetectgeneticassociationswithdisease finance estimatefinancialriskthroughsimulationsofportfoliosthat includemillionsofinstruments monitoringofiotapplications assessagriculturallanduseandcrop yieldforimprovedpolicymakingbyperiodicallyprocessingmillionsof satelliteimages recommendersystems intelligentlyrecommendmillionsofproducts tomillionsofusers webanalyticssocialmediaanalytics 3052 scalable machine learning scalability abilityofasystemtomaintainperformanceunderincreasedloadby addingmoreresources load amountofexistingdatarateofincomingdataandrequired qualityofserviceeg numberofqueriespersecond linearlyscalable performanceismaintainedbyaddingresourcesin proportiontotheincreasedload anonlinearlyscalablesystemistypicallynotveryuseful nbascalablesystemdoesnotnecessarilyaddresslatencyand throughputatthesametime kindofparallelismdeterminedbygranularityorgrainsizeie amountofworkperprocess thisismeasuredbytheratio computationaltimeovercommunicationtime 3252 throughput thenumberoftasksoroperationsasystemcanhandle perunitoftimeasresourcesareadded latency thetimetakentoprocessasingletaskorquery 3352 fine grained parallelism taskparallelism segmentationintolargenumberofsmalltaskswhich maybeexecutedconcurrently smallamountofcomputationalworkbetweencommunicationevents algorithmasadataflowofelementarytasksfunctionaldecomposition significantexpertiserequired highnumberofprocessorsbutamountofworkpertaskislow useofsupercomputerspecialpurposeparallelcomputerswithmany processorssharedmemoryandspecializedhardwaregpusandfpgas adhoccompilersconventionaloperatingsystem scaleupvertically scalabilityisobtainedbyaddingmoreresources eg fasterpusormorememory ittypicallyreduceslatency 3452 fine grained parallelism instructionlevelorlooplevelparallelism 3552 coarsegrained parallelism aprogramissplitintosmallnumberoflargehighgranularitytasks largeamountofcomputationalworkbetweencommunicationand synchronization dataparallelism concurrentcalculationsoverportionsofdata adequateforhighvolumetasks movecodewheredatais useofclusterslargecollectionofcommodityhardwareincluding conventionalprocessorscomputenodesconnectedbyethernet cablesorinexpensiveswitches distributedfilesystemdfsprovidingreplicationofdataor redundancyagainstfailuregooglefilesystemgfshadoop distributedfilesystemhdfsandcloudstore useofconventionallanguagesextendedbyapi scaleouthorizontally scalabilityisobtainedbyaddingmore commoditynodes increasedoverheadandcommunicationcostsassociatedwithhaving moremachinesriskofimbalance ittypicallyincreasesthroughput 3652 coarse grained parallelism 3752 big data vs hpc hpc bigdata largecomputationalload largecomplexdatasets highspeed massivestorage finegrainedgpu coarsegrained lowlatency highthroughput datainlocalmemory distributeddata datainmemory dataindisk simulationmodeling analyticsmachinelearningai nofaulttolerant faulttolerant mpi hadoopspark scaleup scaleout 3852 scalable machine learning multiplereasonstoparallelizelearningalgorithms largevolumeofdatainstancesanddataresidentondistributedfile systemsverticalbigdata highinputdimensionalityhorizontalbigdata complexityofmodelparametricidentification modelselectionandhyperparameterscalibration resamplingandaveragingapproaches performancerequirementseg latencyandthroughput 3952 embarrassingly parallel tasks nodependenciesforinstancethereisnoneedtocommunicatedata amongtheminamessagepassingsetting notasktotaskcommunicationtocompleteitspartsofthe computationsotheircomputingcommunicationratioisinfinity example baggingormontecarlosimulationtasksnosynchronization withtheirpeertasksfromstarttofinish highcomputationcommunicationratiotimeataskspendsactually computingdividedbythetimeitspendscommunicating 4052 high volumes ifdatastoredinarectangularmatrixwheretherowsareinstancesand thecolumnsarethefeatures manyinstances dataparallelismbypartitioningthematrixrowwise intosubsetsofinstancesthatarethenprocessedindependentlyeg parametersupdate manyfeatures dataparallelismbysplittingitcolumnwisefor algorithmsthatcandecouplethecomputationacrossfeatureseg decisiontrees drawbacks assumptionsofiidsamplesindependentfeatures 4152 volume vertical big data domainswherestreamsofdataaremeasuredandcollectedsequentially internet financeeventisatransaction businesstransactionseg creditcardtransaction sensorsinternetofthingsrobotscameras 4252 volume horizontal big data domainswhereinstancesaredescribedbyaverylargenumberoffeature orattributes naturallanguageprocessing afeatureperword bioinformatics afeaturepergeneticgenomicfeatureeg variant geneexpressionmethylation images afeatureperpixel dataaresometimessparseyetnotalways 4352 scalable parametric identification parametricidentificationisanexampleofmultivariateoptimization problem innonlinearmodelsuseoflongandtimeintensivetrainingprocedures eg multilayerdeepneuralnetworks finegrainedparallelimplementationofparametricidentification procedureseg backpropagationinneuralnetworksinmultiprocessor orcustomprocessorsolutionseg gpu coarsegraineddecompositionatthesampleleveleg ingradient descentalgorithms adhocsolutionsfortaskslikeimageorvideoprocessing 4452 scalable model assessment validation examplesofembarrassingparalleltasksinmachinelearningare gridsearchparametersweepingonlargehyperparameterspaces validationprocedureslikecrossvalidationleaveonoutbootstraping baggingaveraging univariatefeatureranking modelracingandselection subsamplingormontecarlostrategies multiplestatisticalsignificancetestingeg parallelpvaluecomputing estimationoflearnersvariance 4552 scalable tasks similaritysearchhashing datastreamprocessingonlinelearning searchengine graphmining 4652 metrics parallelization performance classicalanalysisofalgorithmscomplexityisbasedononotationto quantifycomputationalcosts thisishardlyusableinmachinelearningsincetheexecutiontimeeg relatedtoterminationconditionsisoftendependentondata distributionwhichisnotknownapriori keymetricsusedforanalyzingcomputationalperformanceofparallel algorithmsare 1 speedup 2 scaleup 3 sizeup dontforgetthatthesemeasuresshouldbealsobeassessed inpairedsettingseg samedataset inassociationwithgeneralizationaccuracy 4752 speedup lettnptheexecutiontimeofanalgorithmwherenisthesizeofthe inputdataandpisthenumberofprocessors speedupreferstohowmuchaparallelalgorithmisfasterthana correspondingsequentialalgorithm speedupanalysisholdstheinputsizeandequaltonandgrowsthe numberpofworkers itisdefinedbythefollowingformula tn1 speedupp tnp wheretnpistheexecutiontimewithpworkers linearspeeduporidealspeedupisobtainedwhenthespeedupisa linearfunctionofpeg bydoublingthenumberofprocessorswe doublethespeed linearspeedupisdifficulttoachievebecauseofcommunicationcosts 4852 speedup 4952 sizeup sizeupanalysisholdsthenumberofworkersinthesystemconstant andequaltopandgrowsthesizenofthedatasetsbythefactorm sizeupmeasureshowmuchlongerittakesonagivensystemwhen thedatasetsizeismtimeslargerthantheoriginaldataset itisdefinedbythefollowingformula tmnp sizeupm tnp wheretmnpistheexecutiontimeofthealgorithmforprocessinga datasetofsizemn 5052 scaleup scaleupevaluatestheabilityofthealgorithmtodealwiththegrowth ofboththesystemandthedatasetsize scaleupisdefinedastheabilityofamtimeslargersystemtoperform amtimeslargerjobinthesameruntimeastheoriginalsystem itisdefinedbythefollowingformula tnp scaleupm tmnmp scaleupexperimentsareperformedbyincreasingthesizeofthe datasetsindirectproportiontothenumberofcoresinthesystem theidealscaleupcurveisconstant 5152 scaleup 5252 rajaappuswamychristosgkantsidisdushyanthnarayananorion hodsonandantrowstron nobodyevergotfiredforbuyingacluster technicalreportjanuary2013 bbaesens analyticsinabigdataworld theessentialguidetodatascienceand itsapplications wiley2014 gbontempi statisticalfoundationsofmachinelearning thehandbook 2012 handbookofinfof422course 5352

mining highspeed data streams pedro domingos geoff hulten dept ofcomputerscienceengineering dept ofcomputerscienceengineering universityofwashington universityofwashington box352350 box352350 seattlewa981952350usa seattlewa981952350usa pedrodcswashingtonedu ghultencswashingtonedu abstract presentdaydata mining applications bottleneck many organizations today large data time andmemory notexamples latter typically bases databases grow without limit oversupply sense impossible cur rate several million records per day mining con rent kdd systems make use within tinuous data streams brings unique opportunities also available computational resources result new challenges paper describes andevaluates vfdt available examples go unused undercid12tting may result ananytimesystemthatbuildsdecisiontreesusingconstant enoughdatatomodelverycomplexphenomenaisavailable memory constant time per example vfdt butinappropriatelysimplemodelsareproducedbecausewe corporate tens thousands examples per second using unable take full advantage data thus de ocid11theshelf hardware uses hoecid11ding bounds guar velopmentof highlyecid14cient algorithms becomes priority antee output asymptotically nearly identical conventional learner study vfdts proper currentlythemostecid14cientalgorithmsavailable eg17 ties demonstrate utility extensive set concentrateonmakingitpossibletominedatabasesthatdo experiments synthetic data apply vfdt mining notcid12tinmainmemorybyonlyrequiringsequentialscansof continuous stream web access data whole disk even algorithms tested universityof washington main campus million examples many applications less days worth data example every day categoriesandsubjectdescriptors retailchainsrecordmillionsoftransactionstelecommunica tions companies connect millions calls large banks pro h28databasemanagement databaseapplications cess millions atm credit card operations pop data mining i26 articid12cial intelligence learning ular web sites log millions hits expansion conceptlearningi52patternrecognition designme internetcontinues andubiquitouscomputingbecomes thodologyclassicid12er design evaluation ality expect data volumes become rule rather exception current data mining generalterms systems equipped cope new decisiontreeshoecid11dingboundsincrementallearningdisk examples arrive higher rate mined based algorithms subsampling quantity unused data grows without bounds time progresses even simply preserving examples future 1 introduction use bea problemwhentheyneedto besenttotertiary knowledgediscoverysystemsareconstrainedbythreemain storage easily lost corrupted become unusable limited resources time memory sample size tradi whentherelevantcontextualinformationisnolongeravail tionalapplicationsofmachinelearningandstatisticssample able source examples openended data sizetendstobethedominantlimitation thecomputational stream notion mining database cid12xed size resourcesforamassivesearchareavailablebutcarryingout becomes questionable suchasearchoverthesmallsamplesavailabletypicallyless 10000 examples often leads overcid12tting data ideally would like kdd systems operate dredging eg 22 16 thus overcid12tting avoidance continuouslyandindecid12nitelyincorporatingexamplesasthey comesthemainconcernandonlyafractionoftheavailable arrive never losing potentially valuable information computationalpowerisused3 incontrastinmanyifnot desiderata fulcid12lled incremental learning meth ods also known online successive sequential meth ods substantial literature exists however available algorithms typeeg 20 havesignicid12cant shortcomingsfromthekddpointofview somearereason ably ecid14cient guarantee model learned similar oneobtained bylearning onthe data batch mode highly sensitive example ordering potentially never recovering unfavorable set early examples others produce model batch version high cost ecid14ciency often pensive learning complex trees ie trees many pointof slower thanthe batchalgorithm levels fails datasets large cid12t available diskspace thispaperproposeshoecid11dingtreesadecisiontreelearning method overcomes tradeocid11 hoecid11ding trees goal design decision tree learner extremely learned constant time per example precisely large potentially incid12nitedatasets learner time worstcase proportional number quireeachexampletobereadatmostonceandonlyasmall attributes nearly identical trees con constanttimetoprocessit thiswillmakeitpossibletodi ventionalbatchlearnerwouldproducegivenenoughexam rectlymineonlinedatasourcesiewithouteverstoringthe ples probability hoecid11ding conventional examplesandtobuildpotentiallyverycomplextreeswith treelearnerswillchoosedicid11erenttestsatanygivennodede acceptable computational cost achieve noting creasesexponentiallywiththenumberofexamples wealso catlett 2 others order cid12nd best describeandevaluatevfdtadecisiontreelearningsystem attributetotestatagivennodeitmaybesucid14cienttocon based hoecid11ding trees vfdt io boundin sense sideronlyasmall subsetof thetrainingexamples thatpass mines examples less time takes input node thus given stream examples disk store examples parts cid12rstoneswill beusedtochoosetheroot testoncetheroot thereofinmainmemoryrequiringonlyspaceproportional attribute chosen succeeding examples passed size tree andassociated sucid14cient statistics downtothecorrespondingleavesandusedtochoosetheap learn seeing example therefore propriate attributes recursively1 wesolve require examples online stream ever dicid14cult problem deciding exactly many exam stored anytime algorithm sense ples necessary node using statistical result readytouse model available time cid12rst knownasthehoecid11ding boundoradditivechernocid11bound examples seen quality increases smoothly 79 considerarealvaluedrandomvariablerwhoserange time r eg probability range one formation gain range logc c number thenextsectionintroduceshoecid11dingtreesandstudiestheir classes supposewehavemadenindependentobservations properties wethendescribethevfdtsystemanditsem ofthisvariableandcomputedtheirmeanr thehoecid11ding piricalevaluation thepaperconcludeswithadiscussionof boundstates probability 1cid0cid14 true mean related andfuturework variable least rcid0cid15 2 hoeffdingtrees theclassicid12cation problemisgenerallydecid12nedasfollows cid15 setofn trainingexamplesoftheformxyisgivenwhere discrete class label x vector attributes may symbolic numeric goal producefromtheseexamplesamodelyfxthatwillpre dict classes future examples x high accuracy example x could description clients recent purchases decision send customer cat alog x could record cellulartelephone callandy thedecisionwhetheritisfraudulentornot one themostecid11ectiveandwidelyusedclassicid12cation methods decision tree learning 1 15 learners type duce models form decision trees node contains test attribute branch node correspondstoapossibleoutcomeof thetestandeachleaf containsaclass prediction thelabel ydtxforanex ample xis obtained bypassing example roottoaleaftestingtheappropriateattributeateachnode following branch corresponding attributes value example decision tree learned recur sively replacing leaves test nodes starting root attribute test node chosen comparing available attributes choosing best one accord ingtosomeheuristicmeasure classicdecisiontreelearners likeid3c45andcartassumethatalltrainingexamples canbestoredsimultaneouslyinmainmemoryandarethus severely limited number examples learn diskbased decision tree learners like sliq 10 sprint 17 assume examples stored disk learnbyrepeatedlyreadingtheminsequentiallyecid11ectively per level tree greatly increases size usable training sets become prohibitively ex cid0 r2ln1cid14 1 2n thehoecid11dingboundhastheveryattractivepropertythatit isindependentoftheprobabilitydistributiongeneratingthe observations price thisgenerality thatthe bound conservative thandistributiondependentones ie take observations reach cid14 cid15 let gxi heuristic measure used choose test tributes eg measure could information gain c45 gini indexas cartour goal ensure thatwithhighprobabilitytheattributechosenusingnex amples n small possible would chosen using incid12nite examples assume g bemaximizedandletxa betheattributewithhighestob servedgafterseeingnexamplesandx bethesecondbest b attribute let cid1ggxacid0gx bcid210 dicid11erence observed heuristic values given de sired cid14the hoecid11dingboundguarantees thatxa thecor rect choice probability 1cid0cid14 n examples seen node cid1g cid152 words ob 1we assume examples generated stationary stochastic process ie distribution change time examples read disk assume random order case theyshouldberandomized examplebycreating random indexandsorting onit 2in paper assume thirdbest lower tributeshavesucid14cientlysmallergainsthattheirprobability true best choice negligible plan lift assumption futurework attributes given node pessimistically assumed independent simply involves bonferroni correction cid14 11 served cid1g cid15 hoecid11ding bound guarantees true cid1g cid21 cid1gcid0cid15 0 probability 1cid0cid14 therefore xa indeed best attribute proba bility 1cid0cid14 valid long g value node viewed average g values examples thatnodeas case measures typicallyused thusanodeneedstoaccumulateexamplesfromthestream untilcid15becomessmaller thancid1g noticethatcid15isamono tonically decreasing function n point node canbesplitusingthecurrentbestattributeandsucceeding exampleswillbepassedtothenewleaves thisleadstothe hoecid11ding tree algorithm shown pseudocodein table 1 counts n sucid14cient statistics needed com ijk pute heuristic measures quantities quired similarly maintained prepruning carried considering node null attribute x thatconsists notsplittingthe node thusa splitwill onlybemadeifwithconcid12dence1cid0cid14thebestsplitfoundis better according g splitting pseudocode shown discrete attributes extension numeric ones immediate following usual method allowing tests form xi xij computing g allowed threshold xij sequence examples maybeincid12niteinwhichcase procedurenevertermi natesandatanypointintimeaparallelprocedurecanuse current tree ht make class predictions number attributes v maximum number values per attribute c numberof classes hoecid11ding tree algorithm requires odvc memory store neces sary counts leaf l number leaves tree total memory required oldvc inde pendent number examples seen size treedependsonlyonthetrueconceptandisindependent size training set although common assumption analysis decisiontree related al gorithms often fails practice section 3 describes recid12nementto algorithm cope key property hoecid11ding tree algorithm possible guarantee realistic assumptions trees produces asymptotically arbitrarily close ones produced batch learner ie learner uses examples choose test node words incremental nature hoecid11ding tree algo rithmdoesnotsignicid12cantlyacid11ectthe qualityof treesit produces order makethis statement precise need decid12ne notion disagreement two decision trees letpxbetheprobabilitythattheattributevector loosely example x observed let indicator function returns 1 argument true 0 otherwise definition 1 theextensionaldisagreementcid1e two decision trees dt1 dt2 probability produce dicid11erent class predictions example cid1edt1dt2 cid0 table 1 hoecid11ding tree algorithm inputs sequenceof examples x set discrete attributes g split evaluation function cid14 one minus desired probability choosing correct attributeat given node output ht decision tree procedure hoecid11dingtree sxgcid14 let ht tree single leaf l1 root let x1xfxg let g1xbe theg obtainedbypredicting frequent class ins class k valuexij attributexi 2x letn ijkl10 example xy ins k sort xyintoa leaf l using ht xij xsuchthatxi 2x l incrementn l ijk label l withthe majority class among examples seen far atl examples seen far l class compute g lxifor eachattribute xi 2x lcid0fxg using countsn l ijk letxa attribute highestg l letx attributewith secondhighest g b l compute cid15using equation 1 g lxacid0g lx bcid15 andxa 6x replace l byaninternal node thatsplits onxa branchof split adda new leaf lmand let xm xcid0fxag let gmx bethe gobtained bypredicting frequent class atlm foreach class k andeach valuexij attribute xi 2xmcid0fxg let n ijklm0 returnht leaf consider also two pathsthrough trees dicid11erentif theydicid11er lengthor least one node definition 2 intensional disagreement cid1i two decision trees dt1 dt2 probability path example dt1 dicid11er path dt2 cid1idt1dt2 pxidt1x6dt2x x consider two internal nodes dicid11erent con tain dicid11erent tests two leaves dicid11erent contain dicid11erentclass predictions andan internalnode dicid11erent cid0 pxipath1x6path2x x pathix path example x tree dti two decision trees agree intensionally example icid11 indistinguishable example example passed exactly sequence nodes receives identical class prediction intensional disagree mentisastrongernotionthanextensional disagreement sense that8dt1dt2 cid1idt1dt2cid21cid1edt1dt2 letp betheprobabilitythatanexamplethatreacheslevel l l decision tree falls leaf level sim plify assume probability constant ie 8 p p p termed leaf probability l l isarealistic assumptioninthesensethatitistypicallyap proximately true decision trees generated practice let ht tree produced hoecid11d cid14 ing tree algorithm desired probability cid14 given cid12nite sequence examples dtcid3 asymptotic batchdecisiontreeinducedbychoosingateachnodetheat tributewithtruegreatestgiebyusingincid12niteexamples node let ecid1iht cid14dtcid3 expected value cid1iht cid14dtcid3 taken possible incid12nite training sequences wecan thenstate following result theorem 1 ifht isthetreeproducedbythehoecid11ding cid14 tree algorithmwithdesired probability cid14 given incid12niteexam ples table 1 dtcid3 asymptotic batch tree p leaf probability ecid1iht cid14dtcid3cid20cid14p proof forbrevitywewillrefertointensionaldisagreement simply disagreement consider example x falls leaf level l ht leaf level l h cid14 dtcid3 letlminfl hl dg letpathhxn 1hxn 2hx nhx xs path ht level l l cid14 nhx node x goes level ht cid14 andsimilarlyforpathdxxspaththroughdtcid3 ifll h nhx leaf class prediction simi l larly n ldx l l let ii represent proposi tion pathhx pathdx including level i0 true notice pl h 6 l included pn lhx 6 n ldxji lcid01 two paths dicid11erent lengths one tree must leaf otherhasaninternalnode thenomittingthedependency nodes xfor brevity ppathhx6pathdx pnh 6nd_nh 6nd__nh 6nd 1 1 2 2 l l pn 1h 6n 1dji0pn 2h 6n 2dji1 pn lh 6n ldji lcid01 l cid0 l pn ih 6n idjiicid01 cid20 i1 cid0 cid14 cid14l 2 i1 let ht hoecid11ding tree generated training cid14 sequence ecid1iht cid14dtcid3 average incid12nitetrainingsequencessoftheprobabilitythatanexam plespaththroughht swill dicid11erfromitspaththrough cid14 dtcid3 ecid1iht cid14dtcid3 cid0 ps cid0 pxipathhx6pathdx x cid0 pxppathhx6pathdx x 1 cid0 i1 cid0 anexamples paththroughht swill dicid11erfrom itspath cid14 throughdtcid3 giventhatthelatter lengthi atmost cid14i since icid21l thus 1 ecid1iht cid14dtcid3 cid20 pxppathhx6pathdx 3 x2li li set examples fall leaf dtcid3 level according equation 2 probability cid0 i1 cid0 pxcid14i x2li 1 cid0 cid14i i1 cid0 px 4 x2li sum cid0 px probability example x x2li fall leaf dtcid3 level equal 1cid0 picid01p pis leaf probability therefore ecid1iht cid14dtcid3 1 cid20 cid0 1 cid14i1cid0picid01p cid14p i1 cid0 i1cid0picid01 i1 cid14p cid1 1 cid0 1 1cid0picid01 i1 cid0 1cid0picid01cid1cid1cid1 i2 1 cid0 1cid0picid01cid1cid1cid1 ik cid2 cid14p cid3 1 1cid0p 1cid0pkcid01 cid1cid1cid1 cid1cid1cid1 p p p cid4 cid14 cid5 11cid0pcid1cid1cid11cid0pkcid01cid1cid1cid1 cid6 1 cid14 cid0 cid14 1cid0pi 5 p i0 completes demonstration theorem 1 cid7 immediate corollary theorem 1 expected extensionaldisagreementbetweenht cid14anddtcid3isalsoasym ptotically cid14p although case bound muchlooser anothercorollary whose proof omit intheinterestsofspaceisthatthereexistsasubtreeofthe asymptotic batchtree expected disagreement betweenitandthehoecid11dingtreelearnedoncid12nitedataisat mostcid14p inotherwordsifcid14pissmallthenthehoecid11ding treelearnedoncid12nitedataisverysimilartoasubtreeofthe asymptoticbatchtree ausefulapplicationoftheorem1is thatinsteadofcid14userscannowspecifyasinputtotheho ecid11dingtreealgorithm themaximumexpecteddisagreement willing accept given enough examples treetosettle thelatterismuchmoremeaningfulandcan beintuitivelyspecicid12edwithoutunderstandingthe workings ofthealgorithmorthehoecid11dingbound thealgorithmwill alsoneedanestimateofpwhichcaneasilybeobtainedfor examplebyrunningaconventionaldecisiontreelearneron manageable subset data practical bounds suppose best secondbest attribute dicid11er 10 ie cid15r 01 according equa tion 1 ensuring cid14 01 requires 380 examples en suring cid14 00001 requires 345 additional examples anexponentialimprovementincid14andthereforeinexpected disagreement obtained linear increase numberof examples thus even small leaf prob abilities ie large trees good agreements beobtainedwitharelativelysmall numberof examplesper node forexample p001 anexpecteddisagreement thisisaccomplishedbyatregularintervals scanning 1 guaranteed 725 examples per throughall active inactive leaves andreplac node p1 number examples guarantees ing least promising active leaves inactive disagreement 001 ones thatdominate 3 thevfdtsystem poor attributes memoryusageisalsominimizedbydrop pingearlyonattributesthatdonotlookpromising wehaveimplementedadecisiontreelearningsystembased soonasthedicid11erencebetweenanattributesgandthe onthehoecid11dingtreealgorithmwhichwecallvfdtvery bestonesbecomesgreaterthancid15theattributecanbe fast decision treelearner vfdtallows use either dropped consideration memory used information gain gini index attribute evalu store corresponding counts freed ation measure includes number recid12nements algorithm table 1 initialization vfdtcan beinitialized thetreepro ducedbyaconventionalrambasedlearneronasmall subsetofthedata thistreecaneitherbeinputasis ties two attributes similar gs oroverprunedtocontainonlythosenodesthatvfdt potentially many examples required decide wouldhaveacceptedgiventhenumberof examplesat high concid12dence presum thiscangivevfdtaheadstartthatwillal ably wasteful case makes little dif lowittoreachthesameaccuraciesatsmallernumbers ferencewhichattributeischosen thusvfdtcanop examples throughout learning curve tionally decide ecid11ectively tie split current best attribute cid1gcid15cid28 cid28 rescans vfdtcanrescanpreviouslyseenexamples userspecicid12ed threshold optioncanbeactivatedifeitherthedataarrivesslowly enoughthat thereis time dataset cid12 g computation themostsignicid12cantpartofthetimecost niteandsmallenoughthatitisfeasibletoscanitmul per example recomputing g inecid14cient tipletimes thismeansthatvfdtneednevergrowa compute g every new example un smallerandpotentiallyless accuratetreethanother likely decision split made algorithms using exampleonly specicid12cpoint thusvfdtallowstheusertospecifya minimumnumberofnewexamplesnmin thatmustbe accumulatedataleafbeforegisrecomputed thisef next section describes empirical study vfdt fectivelyreducestheglobaltimespentongcomputa utility recid12nementsis evaluated tions bya factor nminandcan makelearning vfdt nearly fast simply classifying train 4 empiricalstudy ing examples notice however ecid11ect implementing smaller cid14 thanthe one speci 41 synthetic data cid12edbytheuserbecauseexampleswillbeaccumulated system like vfdt useful able learn beyondthestrictminimumrequiredtochoosethecor moreaccuratetreesthanaconventionalsystemgivensimi rect attribute concid12dence 1cid0cid14 increases larcomputationalresources inparticularitshouldbeable thetimerequiredtobuildanodebutourexperiments use advantage examples beyond con show net ecid11ect still large speedup ventionalsystemsabilitytoprocess inthissection wetest cause cid14 shrinks exponentially fast numberof empirically comparing vfdt c45 release 8 examples dicid11erence could large cid14 15 series synthetic datasets using allows us put vfdt correspondingly larger freely vary relevant parameters learning pro target cess inordertoensureafair comparison werestrictedthe memory aslongasvfdtprocessesexamplesfasterthan two systems using amount ram theyarrive whichwill bethe case inall butthe done setting vfdts available memory parameter demanding applications sole obstacle learning 40mb giving c45 maximum number examples arbitrarilycomplexmodelswillbethecid12niteramavail thatwouldcid12tinthesamememory100kexamples3 vfdt able vfdts memory use dominated bythe mem usedinformationgainasthegfunction fourteenconcepts ory required keep counts growing leaves used comparison two classes 100 bi themaximumavailablememoryiseverreachedvfdt nary attributes concepts created randomly deactivatestheleastpromisingleavesinordertomake generating decision trees follows level roomfornewones ifp istheprobabilitythatanarbi cid12rstthreeafraction f nodes wasreplacedbyleaves l traryexamplewillfallintoleaflande istheobserved rest became splits random attributethathad l errorrate atthatleaf thenpe isanupperboundon used yet path root node l l theerrorreductionachievablebyrecid12ningtheleaf p e considered decision tree reached depthof 18 l l foranewleafisestimatedusingthecountsatthepar remaining growing nodes replaced leaves ent corresponding attribute value least eachleafwasrandomlyassignedaclass thesize ofthere promisingleavesareconsideredtobetheoneswiththe sulting concepts ranged 22k leaves 61k leaves lowest values pe leaf deactivated median 126k astream training examples l l memoryisfreedexceptforasinglenumberrequiredto 3vfdt occasionally grew slightly beyond 40mb keep trackof p le l aleaf thenbe reactivated thelimitwasonlyenforcedonheapallocatedmemory c45 becomes promising currently active leaves always exceeded40mb bythe size unprunedtree generated sampling uniformly instance space andassigningclassesaccordingtothetargettree weadded 90 variouslevelsofclassandattributenoisetothetrainingex amples 0 304 noise level n means 85 classattribute value probability n reassigned random equal probability values 80 including original one run 50k separate ex 75 amples used testing c45 runwith default settings ran experiments two pentium 6200 70 mhz one pentium ii400 mhz one pentium iii500 mhz machine runninglinux 65 figure 1 shows accuracy learners averaged 60 runs vfdt run cid14 10cid07 cid28 5 nmin 200 leaf reactivation rescans vfdt 55 boot vfdt bootstrapped overpruned version 100 1000 10000 100000 1e006 1e007 1e008 tree produced c45 c45 accurate vfdt 25k examples accuracies two systemsare similar intherangefrom 25kto100kexamples point c45 unable consider exam ples mostsignicid12cantlyvfdtisabletotakeadvantageof theexamplesafter100ktogreatlyimproveaccuracy887 vfdt 888 vfdtboot vs 765 c45 c45s early advantage comes fact reuses exam ples make decisions multiple levels tree inducingwhilevfdtuseseachexampleonlyonce asex pectedvfdtbootsinitializationletsitachievehighaccu racy quickly without however vfdtboots performanceissurprisinginthatitsaccuracyismuchhigher c45s 100k examples vfdtboot seen examples c45 explanation many experiments reported figure 1 containednoiseandascatlett2showedoverpruningcan veryecid11ectiveat reducing overcid12ttingin noisydomains figure 2 shows average number nodes trees induced learners notice vfdt vfdtbootinducetreeswithsimilarnumbersofnodesand thatbothachievegreateraccuracywithfarfewernodesthan c45 suggests using vfdt substantially creasethecomprehensibilityofthetreesinducedrelativeto c45 italsosuggeststhatvfdtislesspronethanc45to overcid12tting noisydata figure3showshowthealgorithmsrespondtonoise itcom pares four runs concept 126k leaves butwithincreasing levelsof noiseaddedtothetraining ex amples c45s accuracy reports training sets 100k examples vfdtand vfdtboots train ing sets 20 million examples vfdts advantage com pared c45 increases noise level fur therevidencethatuseofthehoecid11dingboundisanecid11ective pruningmethod 4the exact concepts used form f noise level nodes leaves 015 010 74449 37225 015 010 13389 6695 017 010 78891 39446 017 010 9339146696025000252091260502502025209 126050250302520912605025000159177959 025 010 31223 15612 025 015 16781 8391 025 020 4483 2242 028 010 122391 61196 028 010 6611 3306 025 010 25209 12605 last set pa rameters also used basis lesion studies reported ycarucca c45 vfdt vfdtboot examples figure 1 accuracy function number training examples 25000 20000 15000 10000 5000 0 100 1000 10000 1000001e0061e0071e008 sedon c45 vfdt vfdtboot examples figure 2 tree size function number training examples 95 90 85 80 75 70 65 60 55 0 5 10 15 20 25 30 ycarucca c45 vfdt vfdtboot noise figure 3 accuracy function noise level figure4showshowthealgorithmscompareonsixconcepts ofvaryingsize5 allthetrainingsetshad10noise asbe 100 fore c45s resultsareforlearningon100kexamples vfdt vfdtboots 20 million versions 95 vfdt better c45 every concept size con sidered however contrary would expect 90 concept size increases relative benecid12t seems remain 85 approximately constant vfdt vfdtboot look ing deeper cid12nd 20 million examples vfdt 80 vfdtboot induce trees approximately 9k nodes regardless size underlying concept sug 75 gests would take good advantage even training examples 70 wecarried outall runswithouteverwritingvfdtstrain 65 ing examples disk ie generating cid13y 0 10000 20000 30000 40000 50000 60000 70000 passing themdirectlytovfdtfortime comparison pur poses however measured time takes vfdt read examples 025 010 25209 12605 data set disk pentium iii500 mhz machine vfdt takes5752secondstoreadthe20millionexamplesand625 seconds process words learning time order magnitude less input time runs c45 takes 36 seconds read process 100k examples andvfdt takes47 seconds finally generated 160 million examples 025 010 25209 12605 concept figure 5compares vfdtand c45onthisdataset vfdtmakesprogressovertheentire datasetbutbeginstoasymptoteafter10millionexamples thecid12nal150millionexamplescontribute058toaccuracy vfdttook9501secondstoprocesstheexamplesexcluding io induced 219k leaves near future plan carry similar runs complex concepts billions examples 42 lesion studies weconductedaseriesoflesionstudiestoevaluatetheecid11ec tiveness components parameters vfdt system figure 6 shows accuracy learners 025 000 25209 12605 data set also shows slightmodicid12cation tothevfdtbootalgorithm wherethe tree producedbyc45 usedwithout cid12rstoverpruningit versions vfdt run cid14 10cid07 cid28 5 nmin 200 leaf reactivation rescans c45 better without noise vfdt still able use additional data signicid12cantly improve accu racy vfdtboot overprune setting ini tially better overpruning version make much progress eventually overtaken hy pothesize dicid14culty overcoming thepoorlowconcid12dencedecisionsc45madenearitsleaves intheremainderofthelesionstudiesvfdtwasrunonthe 025 010 25209 12605 data set cid14 10cid07 cid28 5 nmin 200 leaf reactivation rescans eval uated ecid11ect disabling ties vfdt make splits able identify clear winner 5the concept 015 010 74449 37225 turned atypically easy included graph avoid obscuring trend observed accuracies con cept c45 831 vfdt 890 vfdtboot 897 ycarucca c45 vfdt vfdtboot concept size leaves figure 4 accuracy function complexity true concept 90 85 80 75 70 65 60 55 50 ycarucca 001 0001 00001 000001 600e1 700e1 800e1 900e1 c45 vfdt examples figure 5 vfdt trained 160 million examples 95 90 85 80 75 70 65 60 55 50 45 ycarucca 001 0001 00001 000001 600e1 700e1 800e1 c45 vfdt vfdtboot overprune examples figure6 ecid11ectofinitializingvfdtwithc45with without overpruning weconductedtworunsholdingallparametersconstantex ceptthatthesecondrunneversplitwithatie withoutties vfdt induced tree 65 nodes 729 accu 744 racy compared 8k nodes 869 accuracy ties 742 vfdtbootwithouttiesproduced805nodesand833ac 74 curacycomparedto8knodesand885accuracywithties 738 also carried two runs holding parameters con 736 stant except nmin number new examples must 734 seen node gs recomputed cid12rstrun 732 recomputed g every 200 examples nmin 200 73 second every example nmin 1 g 728 computations every example vfdt gained 11 accu 726 racyandtook38timeslongertorun vfdtbootlost09 724 accuracy took 37 times longer learners induced 5 nodes frequent g computa tions thencarried two runs holding parameters vfdts memory limit constant cid12rst run al lowed 40 mb memory second allowed 80 mb vfdtandvfdtbootbothinduced78kmorenodeswith additional memory improved vfdts accuracy 30 vfdtboots 32 finally carried two runs holding parameters cid14 constant cid12rst run delta 10cid02 second delta 10cid07 lower cid14 vfdt vfdtboot induced 30 fewer nodes higher one vfdts accuracy 23 higher andvfdtboots accu racy 10 higher lower cid14 43 webdata currently applying vfdt mining stream web page requests emanating whole university washington main campus nature data described detail 23 experiments far used oneweek anonymized trace external webaccesses madefrom theuniversitycampus therewere 23000 activeclients duringthisoneweektrace period entire universitypopulation estimated 50000 peo ple students faculty stacid11 trace contains 828 million requests arrive peak rate 17400 per minute size compressed trace cid12le 20 gb6 request tagged anonymized organi zation id associates request one 170 organizations colleges departments etc within uni versity one purpose data used im prove web caching key predicting accu ratelyaspossiblewhichhostsandpageswillberequestedin thenearfuturegivenrecentrequests weapplieddecision tree learning problem following manner splitthecampuswiderequestlogintoaseriesofequaltime slicest0t1 ttintheexperimentswereporteach time slice hour foreach organization o1o2 oi o170 244k hosts appearing logs h1 hj h244k maintain count many times organization accessed host time slice cijt discretize counts four buckets repre sentingnorequests 112requests 1325requests 26 requests time slice host accessed time slice tthj generate ex ample attributes mod 24c1jt cijtc170jt 6thislogisfrommay1999 tracid14cinmay2000wasdouble size oneweek log approximately 50 gb com pressed ycarucca 0 000005 600e1 600e51 600e2 600e52 600e3 600e53 600e4 vfdtboot examples figure 7 performance web data class 1 hj requested time slice tt1 0 carried real time using modest resources bykeeping statistics last current time slicesctcid01 andct inmemoryonlykeepingcountsforhosts actually appear time slice never needed 30k counts outputting examples ctcid01 soon ct complete using procedure obtained dataset containing 189 million examples 611 werelabeledwiththemostcommonclassthatthehostdid appear nexttime slice testing carried examples last day 276230examples vfdtwasrunwithcid1410cid07cid28 5 nmin 200 runs carried 400 mhz pentium machine decision stump decision tree one node obtains 642 accuracy data decisionstumptook1277secondstolearnandvfdttook 1450 seconds one pass training data initialized c45s overprunedtree major ity time 983 seconds spent reading data disk thebootstrap runof c45 took 2975 seconds tolearn subsample 745k examples many would cid12t 40 mb ram achieved 733 accuracy thus vfdt learned faster 161 million examples c45 75k also used machine 1 gb ramto run c45 entire 161 million training examples runtook 24 hours andthe resulting treewas 75 accurate figure 7 shows vfdtboots performance dataset using1gboframweextendedvfdtsrunoutto4mil lion examples rescanning xaxis shows number examples presented vfdt c45 bootstrap phase complete accuracy improves steadily examples seen vfdt able achieve accuracy simi lartoc45s inasmallfraction thetime furtherc45s memory requirements batch nature allow scale traces much larger week vfdt easily incorporate data indecid12nitely nextstep ap ply vfdt predicting page requests given host also planto address issues related timechanging havior set vfdt running permanently learning andrelearning dictated bythe data stream 5 related work also study application context see 18 previousworkonmininglargedatabasesusingsubsampling directions future work include developing methodsincludesthefollowing catlett2proposedseveral application vfdt web log data studying ap heuristicmethodsforextendingrambasedbatchdecision plications vfdt eg intrusion detection using non tree learners datasets hundreds thousands discretizednumericattributesinvfdtstudyingtheuseof examples musickcatlettandrussell13 proposedand postpruninginvfdtfurtheroptimizing vfdtscompu tested implement learner theoretical tations eg recomputing gs exactly tell modelforchoosingthesizeofsubsamplestouseincompar thatthecurrentexamplemaycausethehoecid11dingboundto ingattributes maronandmoore9usedhoecid11dingbounds reached using adaptive cid14s studying use ex speed selection instancebased regression models via amplecacheinmainmemorytospeedinductionbyreusing crossvalidation see also 12 gratchs sequential id3 6 examplesatmultiplelevels comparing vfdttoid5rand used statistical method minimize number ex otherincrementalalgorithmsadaptingvfdttolearnevolv amples needed choose split decision tree se ing concepts timechanging domains adapting vfdt quentialid3sguaranteesofsimilaritytothebatchtreewere learning withimbalanced classes andasymmetric misclassi muchlooserthanthosederivedhereforhoecid11dingtreesand cid12cation costs adapting vfdt extreme case tested repeatedly sampled small datasets even cid12nal decision tree without stored sucid14cient gehrkeetals boat5learnedanapproximatetreeusing statisticsdoesnotcid12tinmainmemoryparallelizingvfdt cid12xedsize subsample recid12ned scanning applyingthe ideasdescribed heretoothertypesof learning fulldatabase provostetal 14studieddicid11erentstrategies eg rule induction clustering etc mining larger andlarger subsamples untilaccuracyap parently asymptotes contrast systems learn 7 conclusion main memory subsampling systems like sliq 10 thispaperintroducedhoecid11dingtreesamethodforlearning sprint17useallthedataandconcentrateonoptimizing online highvolume data streams increas access disk always reading examples precisely ingly common hoecid11ding trees allow learning verysmall attribute lists sequentially vfdt combines best constant time per example strong guarantees bothworlds accessing datasequentially andusing subsam highasymptoticsimilaritytothecorrespondingbatchtrees pling potentially require muchless thanone scan op vfdt highperformance data mining system based posed many allows scale larger databases hoecid11ding trees empirical studies show ecid11ectiveness thaneithermethodalone vfdthastheadditionaladvan takingadvantageof massivenumbersofexamples vfdts tages incremental anytime new examples applicationtoahighspeedstreamofweblogdataisunder bequicklyincorporatedastheyarriveandausablemodelis way availableafterthecid12rstfewexamplesandthenprogressively recid12ned acknowledgments asmentionedpreviouslythereisalargeliteratureonincre thisresearchwaspartlyfundedbyannsfcareeraward mentallearningwhichspacelimitationsprecludereviewing cid12rst author system closely related utgocid11s 20 id5r extended 21 id5r learns tree 8 references id3abatchmethodbyrestructuringsubtreesasneeded learning time linear number exam 1 l breiman j h friedman ra olshenand c j plesitisworstcaseexponentialinthenumberofattributes stone classicid12cation regression trees simple noisefree problems tested wadsworth belmont ca 1984 much slower id3 noise would presumably aggravate 2 j catlett megainduction machine learning thus id5r appear viable learning large databases phdthesis basser departmentof highspeed data streams computer science universityof sydneysydney australia 1991 number ecid14cient incremental singlepass algorithms forkddtasksotherthansupervisedlearninghaveappeared 3 g dietterich overcid12ttingand undercomputingin inrecentyearsegclustering4andassociation rulemin machine learning computing surveys 27326327 ing 19 substantial theoretical literature online al 1995 gorithms exists eg 8 focuses weak learners eg linear separators little proved 4 ester hpkriegel j sanderm wimmer strong ones likedecision trees xxu incrementalclustering mining data warehousing environmentin proceedings twentyfourth international conference 6 futurework large data bases pages 323333 new yorkny plan shortly compare vfdt sprintsliq 1998 morgan kaufmann vfdtmayoutperformtheseeveninfullydiskresidentdata sets learn less one scan 5 j gehrke vganti rramakrishnan andwl latter require multiple scans dominant component loh boatoptimistic decision tree construction cost often time required read examples proceedings 1999 acm sigmod international diskmultipletimes vfdtsspeedandanytimechar conference management data pages 169180 acter make ideal interactive data mining plan philadelphia pa1999 acm press 6 j gratch sequentialinductivelearning 15 j r quinlanc45 programs machine learning proceedings thirteenth national conference morgan kaufmann sanmateo ca 1993 articid12cial intelligence pages 779786 portland 16 j r quinlanandr cameronjones 1996 aaaipress oversearching andlayered search inempirical 7 w hoecid11ding probability inequalities sums learning proceedings fourteenth bounded randomvariables journal american international joint conference articid12cial statistical association 581330 1963 intelligence pages 10191024 montrcid19eal canada 1995 morgan kaufmann 8 n littlestone learning quicklywhen irrelevant attributes abound anew linearthreshold algorithm 17 j c shafer ragrawal andm mehta sprinta machine learning 2285318 1997 scalable parallel classicid12er data mining proceedings twentysecond international 9 maron amoore hoecid11dingraces conference large databases pages 544555 accelerating model selection search classicid12cation mumbai india 1996 morgan kaufmann andfunction approximation j cowan 18 p smythandd wolpert anytimeexploratory data g tesauro j alspector editors advances analysis massive data sets proceedings neural information processing systems 6 morgan third international conference knowledge kaufmann sanmateo ca 1994 discovery data miningpages 5460 newport 10 mehta aagrawal andj rissanen sliq afast beach ca 1997 aaaipress scalable classicid12er data mining proceedings 19 htoivonen sampling large databases association fifth international conference extending rules proceedings twentysecond database technology pages 1832 avignon france international conference large data bases 1996 springer pages 134145 mumbai india 1996 morgan kaufmann 11 r g miller jr simultaneous statistical inference springer new yorkny2ndedition 1981 20 p e utgocid11 incremental induction decision trees machine learning 4161186 1989 12 wmoore andm lee ecid14cient algorithms minimizing cross validation error inproceedings 21 p e utgocid11 animproved algorithm incremental eleventh international conference machine inductionof decision trees proceedings learning pages 190198 new brunswick nj 1994 eleventh international conference machine morgan kaufmann learning pages 318325 new brunswick nj 1994 morgan kaufmann 13 r musick j catlett ands russell decision theoretic subsampling inductionon large 22 g webbopusanecid14cientadmissible algorithm databases proceedings tenth international unorderedsearch journal articid12cial intelligence conference machine learning pages 212219 research 3431465 1995 amherst 1993 morgan kaufmann 23 awolman g voelker nsharma n cardwell 14 f provost djensen andt oates ecid14cient brown landrayd pinnel karlin progressive sampling proceedings fifth acm hlevyorganizationbased analysis webobject sigkdd international conference knowledge sharing caching inproceedings second discovery data mining pages 2332 sandiego usenix conference internet technologies ca 1999 acm press systems pages 2536 boulder co 1999

infoh515 part big data analytics mapreduce analytics gianlucabontempi machinelearninggroup boulevarddetriomphecp212 httpmlgulbacbe coarsegrained parallelism wewilldiscusshowtodistributemachinelearningcomputinginthe followingsetting trainingdatasetthatdoesnotfitinthemainmemoryofasingle machine computingclusterinfrastructureie alargecollectionofcommodity hardwareconnectedbyethernetorinexpensiveswitches distributedfilesystemeg hdfsprovidingreplicationand redundancy mapreduceprogrammingmodelpartitioningthecomputationintasks thatcanberestartedwithoutaffectingtheothersallowingthen scalabilityandrobustness mapscatterpiecesofaproblemacrosshosts reducegatheraggregatepartialsolutionintoafinalresult increasingperformanceisnotnecessarilythemaingoal faulttolerance andscalabilityavoidbottlenecksaremoreimportantones 2103 creator mapreduce partofthereasonwedidntdevelopmapreduceearlierwasprobably becausewhenwewereoperatingatasmallerscalethenour computationswereusingfewermachinesandthereforerobustness wasntquitesuchabigdeal itwasfinetoperiodicallycheckpointsome computationsandjustrestartthewholecomputationfromacheckpointif amachinedied onceyoureachacertainscalethoughthatbecomes fairlyuntenablesinceyoudalwaysberestartingthingsandnevermake anyforwardprogress jeffdean 3103 mapreduce spark sparkextendstheoriginalhadoopsmrinseveralways 1 fromarigidmapthereduceformattoamoregeneraldirectedacyclic graphofoperatorsdagswontbeexecuteduntilanactionistriggered noneedtowriteintermediateresultsonthedistributedfilesystem 2 richersetoftransformationsnumericalstringtime 3 extensionwithinmemoryprocessing 4 largeecosystemcassandrakafkaandmanyapispyspark 5 goodtradeoffbetweenexploratoryanalyticsmachinelearningand operationalanalyticsdeploymentproduction sparkcodeofthemrexamplesin httpsgithubcomgbontegbcodepyspark 4103 disclaimer muchofthefollowingmaterialis takenfromthebook6 5103 mapreduce examples moperatesonalistofvaluesinordertoproduceanewlistofvalues byapplyingthesamecomputationtoeachvalue roperatesonalistofvaluestocollapseorcombinethosevaluesinto asinglevalueorsomenumberofvaluesagainbyapplyingthesame computationtoeachvalue 7103 rowwise maximum 11 12 1n maxm 1 21 22 2n map maxm 2 n1 n2 nn maxm n verysimpleexampleembarassinglyparallel noreducestepnecessary alltheprocessingisdoneinarowwisemannerbythemapfunction notethatthefirstrowoftheoutputisdependentonlyonthefirstrowof theinput 8103 matrixvector multiplication mr letusconsiderasquarennmatrixmm andan1vectorv ij resultisan1vectorwhoseithelementis n cid88 x v i1n ij j j1 ifthevectorcanfitinmemoryitcanbebroadcastedtoeachchunkand madeavailabletoallapplicationsofthemapfunction 9103 matrixvector multiplication m11 m12 m1n v1 m11v1m12v2 m1nvn m21 m22 m1n v2 m21v1m22v2 m2nvn x mn1 mn2 mnn vn mn1v1mn2v2 mnnvn ismapalonesufficient isthefirstrowoftheoutputdependentonlyon thefirstrowoftheinputs 10103 matrixvector multiplication mr broadcast v1 v2 vn x1 x2 m11m12m1n 1m11 v1 1m12 v2 1m1n vn m21m22m2n 2m21 v1 2m22 v2 2m2n vn reduce map mn1mn2mnn nmn1 v1 nmn2 v2 nmnn vn xn 11103 1st part course 12103 n large decompositionintosstripestohavethevectorfittingintomemory cid88n cid88s cid88ns x v v ij j ij j j1 s1js1 wheres1sn aretheindicesofthesthstripe 13103 v v v v 11 12 1n 1 11 1 12 2 1n n v v v v 21 22 1n 2 21 1 22 2 2n n x v v v v n1 n2 nn n n1 1 n2 2 nn n 14103 relational databases arelationisatablewhoserowsarecalledtuplesandcolumnheaders attributes thesetofattributesiscalledschema relationalalgebraoperations selectionprojection unionintersectiondifferencejoin naturaljoin giventworelationsitisthesetofjointtuplessuchthat theoriginalonesagreeonallthecommonattributes groupingandaggregation itpartitiontuplesaccordingtothevaluesof thegroupingattributeeg sexandcomputesanaggregatedvalue maxmincountsumavgrelatedtoanattributeeg agewhichis notinthegroupingset theoutputisatuplepergrouphavingas groupingattributethecommonvalueeg maleandacomponentfor eachaggregationeg averagemaleages 15103 mr implementation relational operations 6presentsmrimplementationsofrelationaloperations selection map returnsttiftheselectionconditionissatisfied reduce returnsthevalue projection map returnstcid48tcid48wheretcid48istheprojectedsubsetofthetuplet reduce iteliminatesduplicateseg twodifferentoriginaltupleswhich becamethesameafterprojection unionoftworelationshavingthesameschema map returnttforeachtuplet reduce ifkeythasvaluetorttreturnt intersectionoftworelationshavingthesameschema map returnttforeachtuplet reduce ifkeythasvaluettthenreturnt otherwisereturnnothing 16103 mr example union two tables name sex gianluca gianluca_mgianluca lucia f lucia_flucia f gianluca_mgianluca mgianluca name sex gianluca mattia mattia_mmattia mattia_mmattia r lucia f gaia f gaia_fgaia f mattia lucia_flucia f name sex gaia f gianluca gianluca_mgianluca gaia_fgaia f stijn stijn_mstijn stijn stijn_mstijn unionrnamesexandsnamesex 17103 mr example intersection two tables name sex gianluca gianluca_mgianluca lucia f map lucia_flucia f gianluca_mgianluca mgianluca mattia mattia_mmattia mattia_mmattia red name sex gaia f gaia_fgaia f gianluca lucia_flucia f name sex gianluca map gianluca_mgianluca gaia_fgaia f stijn_mstijn stijn stijn_mstijn intersectionofrnamesexandsnamesex 18103 mr implementation relational operations differencers map foratupletinrproducekeyvaluepairtrandforatupletins producekeyvaluepairts reduce foreachkeytiftheassociatedvaluelistisrthenreturnt otherwisereturnnothing joinrabcid46cid47 sbc b map foreachtupleabofrproducethekeyvaluepairbra eachtuplebcofsproducethekeyvaluepairbsc reduce eachkeyvaluebwillbeassociatedwithalistofpairsoftheform raorsc constructfromthosepairsallpossibletriplesoftheform abc 19103 mr example difference two tables name sex gianluca gianluca_mr gianluca_mrs map lucia f lucia_fr r mattia mattia_mr mattia_mr name sex red gaia f gaia_fr lucia f lucia_fr mattia name sex gianluca map gianluca_ms gaia_fr gaia f stijn stijn_ms stijn_ms differencernamesexsnamesex 20103 mr example join two tables name id gianluca 11 1rgianluca lucia 2 2rlucia r name job map 1rgianluca sprof mattia 2 2rmattia gianluca prof 1 red gaia 3 3rgaia 2rluciarmattiasstudent lucia student 2 id job mattia student 2 3rgaia spupil 1 prof 1sprof gaia pupil 3 2 student 2sstudent 3 pupil 3spupil joinrnameidcid46cid47 sidjob id 21103 mr implementation relational operations considertherelationrabc groupingonaandaggregationonb γ rabc aθb map foreachtupleabcproducethekeyvaluepairab reduce eachkeyarepresentsagroup applytheaggregationoperatortothe listofbvaluesassociatedwithkeya theoutputisthepairaxwherexis theresultofaggregation incaseofseveralgroupingattributesthekeyisalistofvaluesofa tupleforthoseattributes incaseofseveralaggregationstheresultof aggregationisalistofvaluestoo 22103 mr example grouping aggregation job age prof 52 map prof52 job age_sum r student 21 student21 prof5247 red prof 99 prof 47 prof47 student211822 student 61 student 18 student18 student 22 student22 γ jobcid80agerjobage groupingonjobandaggregationsumonage 23103 matrix multiplication relational form letusconsidertheproductpmnwheremism nisn n r c r c n c r thegenericelementofpis cid88mc p n ik ij jk j1 thismultiplicationcanbeinterpretedinarelationalformparticularly usefulinthecaseofsparsematriceswhere matrixmisarelationmijvwithtuplesijm ij matrixnisarelationnjkvwithtuplesjkn jk multiplicationasthesequenceof 1 naturaljoinonattributej 2 groupingoniandkand 3 aggregationsum 24103 cid34 cid35cid34 cid35 cid34 cid35 n n n n n n n n n 11 12 11 12 13 11 11 12 21 11 12 12 22 11 13 12 23 n n n n n n n n n 21 22 21 22 23 21 11 22 12 21 12 22 22 21 13 22 23 j k n 1 1 n j 11 1 2 n 12 1 1 11 1 3 n r 1 1 2 12 r 2 1 3 2 1 21 3 1 n 31 2 2 22 3 2 n 32 3 3 n 33 25103 j k n 1 1 1 n 11 11 1 1 2 n 11 12 1 1 3 n k p 11 13 2 1 1 n 1 1 n n 21 11 11 11 12 21 2 1 2 n 1 2 n n 21 12 11 12 12 22 r 1 cid46cid47 j r 2 2 1 3 21 n 13 γ iksummn 1 3 11n 13m 12n 23 1 2 1 n 2 1 n n 12 21 21 11 22 21 1 2 2 n 2 2 n n 12 22 21 12 22 22 1 2 3 n 2 3 n n 12 23 21 13 22 23 2 2 1 n 22 21 2 2 2 n 22 22 2 2 3 n 22 23 26103 matrix multiplication mr two steps step1 map foreachelementofmproducejmim andforeachelement ij ofnproducejnkn jk reduce foreachkeyjconsiderallthepairsofelementscomingfromm andnandcomputem n andreturnikm n ij jk ij jk step2 map identityfunction reduce foreachkeyikreturnthesumofthelistofvalues associatedwiththekey 27103 matrix multiplication mr single step matrixmm ofsizem ik r c matrixnn ofsizen n andm n jk r c c r map 1 foreachelementofmproduceikjmm ij i12mk12n j1m r c c 2 foreachelementofnproduceikjnn jk i12mk12n j1m r c c reduce foreachkeyikconsiderallthepairsjmm andjnn ij jk sortthemaccordingtothejmultiplythevaluescorrespondingtothe samejandsumthem 28103 ikjmmij 111m1 131m1 121m1 141m1 112m2 132m2 122m2 142m2 1 2 211m3 231m3 1 1 2 221m3 241m3 111m1 2m2 1n1 2n5 1125 212m4 232m4 2 3 4 222m4 242m4 121m1 2m2 1n2 2n6 1226 311m5 331m5 3 5 6 321m5 341m5 131m1 2m2 1n3 2n7 1327 312m6 332m6 141m1 2m2 1n4 2n8 1428 322m6 342m6 map 211m3 2m4 1n1 2n5 3145 11 14 17 20 ikjnnjk 221m3 2m4 1n2 2n6 3246 r e 23 27 37 44 111n1 211n1 311n1 231m3 2m4 1n3 2n7 3347 68 121n2 1 2 3 4 221n2 321n2 241m3 2m4 1n4 2n8 3448 131n3 1 1 2 3 4 231n3 331n3 2 5 6 7 8 141n4 241n4 341n4 341m5 2m6 1n4 2n8 5468 112n5 212n5 312n5 122n6 222n6 322n6 132n7 232n7 332n7 142n8 242n8 342n8 29103 roadmap scalability big data analytics practice twopossiblesolutions 1 usesomerecentopensourcemachinelearningplatformsforbigdata mahoutontopofhadoop itimplementsclassificationperceptronlogistic regressionnaivebayesrandomforestsvmclusteringlocallyweighted regressionsvdsingularvaluedecompositionpcaica tridentmlontopofstorm itimplementslinearclassificationperceptron linearregressionclusteringfeaturescalingstandardizationnormalization mllibontopofspark itimplementsiniterativemannerbinaryclassification svmandlogisticregressionlinearregressionclusteringkmean collaborativefilteringforrecommendersystem 2 redesignandoradaptalgorithmsinascalablemanneraccordingto themrparadigm 31103 distribution learning algorithms wewillconsideranumberofalgorithms supervisedlearning regression leastsquares classification naivebayesclassification regressionclassification knn averagingalgorithms unsupervisedlearning kmeans featureselection rankingcorrelation mrmrfeatureselection 32103 nota bene mrisnot asolutiontoeveryproblemnoteveneveryproblemthat profitablycanusemanycomputenodesoperatinginparallel theuseofadistributedfilesystemmakessenseonlywhenfilesare verylargeandarerarelyupdatedinplace thecommunicationcostisthedominantcostsinceeachsingletaskis typicallyverysimpleandtheinterconnectspeedofacomputingcluster ismuchlowerthantheprocessorexecutionspeed differentmrimplementationsofthesamealgorithmmayinduce differentcommunicationcosts 33103 routes scalability makenotallmlalgorithmsscalablebyusingoneoftheseprinciples 1 summationprinciple expectation costfunction gradient loglikelihood frequency meanvariance 2 independenttasks featureranking crossvalidationbagging testing modelselectionracing 3 statisticalsamplingpropertiesestimation 4 divideandconquerstrategy nonlineardependencedecomposedinto localsimplerdependencies 34103 summation form principle mlandsumsarestronglyrelated accuracyofalearnereg meansquarederrorexpressedasafunction typicallyexpectationofthedatadistribution estimationoftheaccuracyonthebasisofdataeg bycrossvalidation requiresthecomputationofsampledversionsoftheexpectationie meansorsumsoverthedata averagingapproacheseg baggingrelyonaverages probabilityestimationsviafrequencyrequirecomputationsofsums trainingcostfunctionscanbewrittenassums forinstancesumof squarederrors iftheerrorcostfunctionisasumthegradientoftheerrorisasumtoo 35103 summation form principle onceanalgorithmdoessumsoverthedatawecaneasilydistributethe calculationsovermultipleworkerseg cores abcdefabcdef wefirstdividethedatasetintoasmanypiecesasworkerspartitioning thenwegiveeachworkerablockofdatascatter eventuallyweaggregatetheresultsgather wellknownexampleofalgorithminsummationformisleastsquares sumisanexampleofmonoidoperatorthatcanbeeasilymadeparallel inthereducestep 36103 monoids mr amonoidiscomposedbyasetsanassociativebinarymapping sssandanidentityelement anassociativeoperationsatisfiesthefollowingequalityforallabc abcabc inmramapperisnotconstrainedwhilethereducermaybedistributed byimplementingtheiteratedapplicationofanassociativeoperation commutativemonoids maxadditionmultiplicationunion intersection maxamaxbcmaxmaxabc nonmonoids meanmediansubtraction avg12345cid54avgavg123avg45 sinceaverageisnotassociativebutsumandcountarethereduce implementationofaveragereliesonsumandcounting 37103 key value job age prof 52 map prof52 red job age_mean r st pu rd oe fnt 2 41 7 pu rd oe fn 4t 72 1 stu dp ero nf 5 22 1 4 17 8 22 noid stp ur ef nt 24 09 35 3 student 18 student18 student 22 student22 key value count job age prof 52 map prof521 r student 21 2u 1d 1e n prof521471 prof992 mr one od job age_mean prof 47 prof471 student211181 student392221 prof 495 student 18 student student 2033 181 student 22 student student 221 221 38103 independence severalphasesofmlpipelinemaybecarriedoutindependentlywhere independencecanbeinterpretedbothincomputationalno communicationandstatisticalnoinformationsense predictionfordifferenttestvaluesnotethatpredictionisthemost expensivestepinsomealgorithmslazy crossvalidation averagingofestimatorsbagging univariaterankingoffeaturesassumptionofindependence generationofiid subsamples conditionalindependencenaivebayes 39103 statistical sampling statisticalsampling itallowsinferencesaboutapopulationtobemade fromobservationsmadeonarelativelysmallnumberofindividualsin thepopulationthesample theideaistousetheoriginalbigdatasetasthepopulationand portionsofitassamplestoinferpropertiesoftheoriginaldataset mapcancreateseveralsubsamplesthatcanbeparallellyanalysedat thereducelevel 40103 divideandconquer learningstrategythatattacksacomplexproblembydividingitinto simplerproblemswhosesolutionscanbecombinedtoyieldasolutionto theoriginalproblem thisprinciplepresentstwomainadvantages 1 simplerproblemscanbesolvedwithsimplerestimationtechniquesin statisticsthismeanstoadoptlineartechniqueswellstudiedand developedovertheyears 2 thelearningmethodcanbetteradjusttothepropertiesoftheavailable dataset examples radialbasisfunctionknnlocallearning 41103 12 1 08 06 04 02 0 1 05 1 0 05 0 05 05 1 1 42103 mr ml algorithms multiple linear dependency consideralinearrelationbetweenanindependentvariablexx rn andadependentrandomvariableyy r yβ β x β x β x w 0 1 1 2 2 n n wherewrepresentsarandomvariablewithmeanzeroandconstant varianceσ2 w inmatrixnotationtheequationcanbewrittenas yxtβw wherexstandsforthep1vectorx1x x x 1 2 n β β β isthevectorofparametersandpn1isthetotal 0 n numberofmodelparameters nbinthefollowingx willdenotetheithvariableofthevectorxwhile x willdenotetheithobservationofthevectorx 44103 multiple linear regression model considernobservationsd cid104xycid105i1nwhere n x 1x x generatedaccordingtothepreviousmodel i1 supposethatthefollowingmultiplelinearrelationholds yxβw whereyisthen1responsevectorxisthenpdatamatrixwhose jth columnofxcontainsreadingsonthejth regressorβ isthep1 vectorofparameters y1 1 x11 x12 x1n xt 1 β0 2 x 1 x2 1 x2 2 x2 n x 2 β β 1 yn 1 xn1 xn2 xnn xt n βn 45103 normal equations theleastsquaresestimatorβˆ minimizesthecostfunction n βˆargmincid88 xtb2 argmincid0 yxbtyxbcid1 b b i1 itcanbeshownthatitsatisfiestheleastsquaresnormalequations xtxβˆxty assumingxisoffullcolumnrankweobtain βˆxtx1xty wherethextxmatrixisasymmetricppmatrix 46103 mr distribution leastsquares solution twosettings verylargennwithnsmall distributionofthecomputationsofxtx andxty verylargenandn iterativesolutionbystochasticgradientdescent seeclassonsequentialalgorithms 47103 leastsquares summation form supposen3p2 cid34 cid35 x x 11 12 x x x xtx 11 21 31 x x x x x 21 22 12 22 32 x x 31 32 cid34 cid35 x2 x2 x2 x x x x x x 11 21 31 11 12 21 22 31 32 x x x x x x x2 x2 x2 12 11 22 21 32 31 12 22 32 cid34 cid35 cid34 cid35 x cid104 cid105 x2 x x xtx 11 x x 11 11 12 1 1 x 11 12 x x x2 12 11 12 12 n cid88 xtx xtx i1 wherex isthe1pvectordenotingtheithrowofthematrixx 48103 leastsquares summation form analogouslyitcanbeshownthat n cid88 xty xty i1 wherey istheithvalueofthevectoryandxtyhasasizep1 49103 distributed computation xtx xty createamatrixzofsizenp1byconcatenatingxandy 1 map computationofpartialxtx foreachithrowofxcomputextx sizepp 2 reduce sumallxtx 3 map computationofpartialxty foreachithrowofxi1n computexty ofsizep1 4 reduce sumallxty 5 collect collectthereducedmatricesxtxandxtyandcomputeβˆ ina conventionalmanner 50103 considerations insteadofarowwisecomputationitispossibletoimplementa partitionwisecomputation summationformallowsalinearspeedupwiththenumberofworkers intermediateresultsofleastsquaresareusefulinothertaskstoo xtxisanestimationofthecovariancematrixifallthecolumnsofxare normalizedie zeromeanandunitvariance frequentlyusedinlinear statisticseg principalcomponentanalysislineardiscriminant analysis xtyisanestimationofthecorrelationinputoutputvectoragainin normalizedsettings frequentlyusedinlinearstatisticseg rankingof features notethatabovecomputationsarescalablewithnbutnotn 51103 speedup assessment 52103 naive bayes classifier nbclassifierhasshowninsomedomainsanaccuracycomparableto thatofneuralnetworksanddecisiontreelearning considerabinaryclassificationproblemwithninputsandarandom outputvariableythattakesvaluesinthesetc c 1 k thebayesoptimalclassifiershouldreturn cxarg max probyc x k k1k wecanusebayestheoremtorewritethisexpressionas probxyc probyc cxarg max k k k1k probx arg max probxyc probyc k k k1k 53103 naive bayes classifier howtoestimatethesetwotermswithfinitedataset probyc frequencywithwhicheachtargetclassoccursinthe k trainingset estimationofprobxyc muchharder k nbisbasedonthesimplifyingassumptionthatinputsareconditionally independentgiventhetargetvalue n cid89 cid8 cid9 probxyc probx x yc prob xyc k 1 n k j k j1 thenbclassificationisthen n cid89 cid8 cid9 c xarg max probyc prob xyc nb k j k k1k j1 54103 distributed computation naive bayes classifier letussupposethatalldataarecategorical cid8 cid9 ifinputsx arebinariestheestimationofprob xyc boilsdownto j j k thecountingofthefrequenciesoftheoccurrencesofthedifferent valuesofx foraclassc j k weneedtosumoverx 0andx 1foreachlabelc j j k oncedataarepartitioneditisenoughtocomputeasumforeachblock andthenaggregatetheresults 55103 example letusconsiderthefollowingdatasetexamplefrom5 56103 example map themapfunctioncountstheattributesandtheirassociationswiththe classificationclasses forinstancethemapofthefirstrowwillreturn sunnyno 1 hotno 1 highno 1 weakno 1 classno 1 57103 example reducer input 58103 example reducer output 59103 example conditional probability fromtheoutputofthereduceritiseasytocomputetheconditional probability forinstanceforthevariablex outlookthatcantakeasvaluessunny 1 overcastrainweobtain probx sunnyyno 3 probx sunnyyno 1 1 probyno 5 where3isthevalueofsunnynoand5isthevalueofclassno theprobabilitytableisavailableitispossibletouseittoperformnaive bayesclassificationononnewinputvectors 60103 kmeans givenk0wherekisthenumberofclustersandasetofn ndimensionalobjectsclusteringistheprocessofgroupingasetofn ndimensionalintokclustersofsimilarobjects objectsshouldbesimilartooneanotherwithinthesameclusterand dissimilartothoseinotherclusters kmeansisadistancebasedunsupervisedclusteringalgorithm kmeansclusteringhasmanyusefulapplications forexampleitcan beusedtofindagroupofconsumerswithcommonbehaviorsorto clusterdocumentsbasedonthesimilarityoftheircontents selectionofkisspecifictotheapplicationorproblemdomain thereis nomagicformulatofindit 61103 20 20 15 15 10 10 5 5 0 0 5 5 1010 8 6 4 2 0 2 4 1010 8 6 4 2 0 2 4 62103 kmeans initiallykpointscc c arechosenasclustercentersthese 1 k arecalledclustercentroids therearemanywaystoinitializetheclustercentroidsoneofwhichis tochoosethekpointsrandomlyfromthesampleofnpoints oncethekinitialclustercentroidsarechosenwecalculatethe distancefromeverypointintheinputsettoeachofthekcentersand thenassigneachpointtothespecificclustercenterwhosedistanceis closest whenallobjectshavebeenassignedthenweagainrecalculatethe positionsofthekcentroids thesetwostepsarerepeateduntiltheclustercentroidsnolonger changeorchangeverylittle 63103 distributed computation kmeans costfunctionisagainasum k cid88 cid88 arg min cid107x c cid107 k c1ck k1xic k wherec isthesetofpointsofsize1nwhichbelongtothekthcluster k ofsize1n foraverylargenthebottleneckisthecomputationofthedistancesof thenpointstothecentroids 64103 mr computation kmeans 1 wepartitionthedatasetxofsizennandwebroadcastthesetof centroidcoordinatesofsizekn 2 wedistributethecomputationofthedistancestoanumberofparallel workers 3 amapfunctionreturnsforeachx akeyvaluepairwherethekeyisthe indexk oftheclosestcentroidandthevalueisapairx1thatwill allowtocomputethesumandnumberofobservationsineachcluster 4 areducestepreturnstheupdatedcoordinatesofeachclustercentroid byaveraging 5 byiteratingthestepsaboveweconvergetothesolution 65103 broadcast c1 c2 c1xi1 ck c1xj1 x x1 2 cc k3 x x1 2 1 1 reduce c c2 2 x xk l 11 map ckxs1 xn c2xn1 ckxt1 66103 knn classifier supposeatrainingsetisavailableandtheclassificationisrequiredfora 1nqueryvector theclassificationprocedureofaknnclassifiercanbe summarisedinthesesteps 1 computethedistancebetweenthequeryandthentrainingsamples accordingtoapredefinedmetric 2 ranktheneighboursonthebasisoftheirdistancetothequery 3 selectasubsetoftheknearestneighbors eachoftheseneighbours hasanassociatedclass 4 returntheclasswhichcharacterisesthemajorityoftheknearest neighbors 67103 distributed knn lazyalgorithm nothingisdoneuntilaqueryisdone notrainingcomputationalcost allcomputationaleffortconcernsthepredictionstep foreachquery pointneighbourshavetobeidentifiedandclassificationreturned problem comparingaquerypointwitheachsampleinahugedatabase isinfeasiblebecauseofthelinearcomplexityon curseofdimensionalitymakestheproblemstillworse howeverretrievingasetofapproximatenearestneighboursannis oftensufficient distributionofthepredictioncomputationaleffort 68103 mrknn in4theauthorsproposeamapreducebasedapproachforknearest neighbourclassification inthisapproachthetestsetofsizetnisbroadcasttoallworkers map foreachchunkofdataitcomputesthesimilaritybetweenallthe testexamplesandaportionofthetrainingset theknearest neighboursandtheircomputeddistancevalueswillbeemittedtothe reducestage reduce itdetermineswhicharethefinalknearestneighboursfromthe listprovidedbythemaps itisimplementedbymergingtwosortedlists ofsizek 69103 mrknn from4 70103 pasting random subspaces supervisedtaskforwhichthedatasetcannotresideinmemory breimanthegenialinventorofregressiontreerandomforest bagging proposedthepastingmethod1 pasting learninganensembleofmodelsindividuallybuiltonrandom subsetsofthetrainingexampleshencealleviatingthememory requirementssincethebasemodelswouldbebuiltononlysmallparts ofdataset ho2proposedtolearnanensembleofmodelsindividuallybuilton randomonrandomsubsetsoftheinputvariablesorfeatures 71103 random patches thetwoideashasbeenmergedby3intherandompatchalgorithm whichconsistsincreatinganumberofdatasetscoveringaportionof thefeaturesandaportionoftheobservationsfitthemwithamodel andcombinethepredictions theportionofthefeatureandsamplespaceiscontrolledbytwo hyperparameters highlyparallel eachpatchcanbemanagedindependentlybya processor ifthesizeofthepatchiscompatiblewiththecentralmemory conventionalmemoryarchitecturesmaybeused openchallenges subsetsofdatamaypresentdifferentstatistical propertiesthanoveralldataset forexampleconfidenceintervals basedonsubsetsofdatawillgenerallybewiderthanconfidence intervalsbasedontheoriginaldatapapersofmjordanonstatistics computationandscalability 72103 hashes anhashfunctiontakesakeyvaluedatumofanytypeasinputand returnsabucketintegernumberinarange0b1asresult aimofhashfunctionistorandomizekeysie tosendapproximatelyan equalnumberofkeystoeachbucketnumber 73103 locality sensitivity hashing supposewehaveaverylargesetofelementsandwewishtocompute thesimilarityofeverypair lshidea hasheachelementseveraltimesinsuchawaythatsimilar elementsaremorelikelytobehashedtothesamebucketthan dissimilarones anypairhashedtothesamebucketisacandidatepairforsimilarity falsepositives dissimilarelementsinthesamebucket falsenegatives trulysimilarelementsnothashingtothesamebucket lshalsoknownasnearneighboursearch 74103 lsh functions cosine distance letusconsidertwovectorsx rnx rn inandimensionalspaceand 1 2 thecosinedistance thelocalitysensitivefamilyforthecosinedistanceisasetofbbinary codesfunctionseachbuiltfromarandomlychosenvector h rnb1b b f xsignh xie f x f x ifandonlyifthedotproducts b b b 1 b 2 h x andh x havethesamesign b 1 b 2 eachndimensionalvectoristransformedinabdimensionalbinary code sincenbcodingreducesstorage andhammingdistancecan becalculatedefficientlyinabitwisemanner amodifiedversionisthesketchwhichconsistsinrestrictingtovectors h whosecomponentsare1and1 b 75103 hyperplance cosine cosinesimilarityisthecosineoftheanglebetweentwovectors fromhttpswwwpineconeiolearn localitysensitivehashingrandomprojection 76103 n2inputdimensionsb4bitsbinarycodeeachcorrespondingtoa hyperplane from8 77103 mr divide conquer incaseofmassivetrainingsetsofdimensionalitynitcouldbeeffectiveto partitionitaccordingtoalocalityprinciple bydefininganumberbofsketchvectorsh 11nitisispossibleto b projecteachtrainingpointinabdimensionalspaceandusethehashing fordefiningsetsofneighbouringpoints training 1 broadcastthesmatrixofdimensionalitynbwhereeachcolumn correspondstoasketchvector 2 map foreachtraininginputvectorx computethematrixproductx andstorethesignvectorashashingkey foreachtestpointq 1 computethematrixproductqsandstorethesignvector 2 filterthetrainingsetbyselectingonlythesubsetwiththesame hashing 3 makeaconventionallearningprocedurebyusingasubsetofthe trainingset 78103 learning hash lsharedataindependenthashfunctions inspiteoftheireleganttheoreticalpropertiestheirperformancehas beenshowninsufficientinmanyrealworldsettings toachievehighprecisionlonghashcodesarerequiredandsemantic proximityisnotalwaysrespectedsemanticgap learningtohash8arerecentapproacheswhichusemachine learningtoexploitdatadistributionorclasslabelstooptimizethehash function particularlyusefulinimageswherethesimilarityordistancebetween imagepairsisusuallynotdefinedviaasimplemetric 79103 ranking feature selection itassessestheimportanceorrelevanceofeachvariablewithrespect totheoutputbyusingaunivariatemeasure theyaresupervised techniquesofcomplexityon measuresofrelevancewhicharecommonlyusedare pearsoncorrelationthegreaterthemorerelevantwhichassumeslinearity mutualinformationthegreaterthemorerelevant aftertheunivariateassessmentthemethodranksthevariablesina decreasingorderofrelevance thesemethodsarefastcomplexityonandtheiroutputisintuitive andeasytounderstand atthesametimetheydisregardredundancies andhigherorderinteractionsbetweenvariableseg genes thebestkfeaturestakenindividuallydonotnecessarilyconstitutethe bestkvariatevector 80103 mr filter selection supervisedtaskwithninputfeaturesandonetarget traininginputoutputdatainarectangularformat eachrowcontains boththeinputvaluesandtheassociatedoutput map key featureindex value paircomposedofthefeaturevalueandthecorrespondingtargetvalue reduce thetupleswiththesamekeyreferringtothesamefeatureare groupedandthebivariatescoreeg correlationormutualinformation computed theoutputisavectorofsizencontainingtheassociationscoreofeach inputfeatures formoreadvancedstrategieslookat7 81103 1 2 1 2 1 2 1 1 2 1 2 1 2 0 1 1 2 1 2 1 2 0 1 1 2 1 2 0 1 2 1 2 0 1 1 2 1 2 0 2 0 0 1 2 1 2 0 1 1 2 1 2 0 2 0 0 1 2 1 1 2 1 2 0 1 1 2 1 2 0 2 0 0 1 2 1 2 2 1 1 2 1 2 0 1 1 2 1 1 2 1 2 0 1 2 0 1 2 1 2 0 0 2 0 1 2 0 0 1 2 1 2 2 1 2 2 1 1 2 1 2 0 1 1 2 1 1 2 1 2 0 1 mi 1 1 2 0 1 2 1 2 0 0 2 0 1 2 0 0 2 mi 2 1 2 1 2 2 1 2 2 1 homework considerabigrectangularmatrix 11 12 1n 21 22 2n n1 n2 nn writeamapreducepseudocodetotransposethematrix notethat apseudocodedescriptionofthemapandreducefunctionsare required youcanusedrawingsandtexttoillustrateanddocumentyourcode 92103 solution reduce rank 11 m11 1n mn1 12m21 1 m11 m21 mn1 1 m11 m12 m1n 11 m11 21 m12 n1m1n 23 m13 22 m22 2nmn2 2 m12 m22 mn2 2 m21 m22 m2n p 12 m21 22 m22 n2m2n n m1n m2n mnn n mn1 mn2 mnn 1n mn1 2n mn2 nnmnn n4 m4n n2 m2n n1m1n 93103 exam question 94103 solution 1st step 1st step reduce broadcast fit compute residuals 1x11y1 1x21y2 1xn1yn mse1 1 x11 x12 x1n y1 1x11y1 2x12y1 nx1ny1 2 x21 x22 x2n y2 map 1x21y2 2x22y2 nx2ny2 2x12y1 2x22y2 2xn2yn mse2 n xn1 xn2 xnn yn 1xn1yn 2xn2yn nxnnyn nx1ny1 nx2ny2 nxnnyn msen suppose 3arg minj msej 95103 solution 2nd step 2nd step reduce 1 x11 x13 y1 broadcast 3 2 x21 x23 y2 fit compute residuals mse13 1 x11 x12 x1n y1 1x11x13y1 2x12x13y1 nx1nx13y1 n xn1 xn3 yn 2 x21 x22 x2n y2 ap 1x21x23y2 2x22x23y2 nx2nx23y2 1 x12 x13 y1 2 x 22 x 23 y2 mse23 n xn1 xn2 xnn yn 1xn1xn3yn 2xn2xn3yn nxnnxn3yn n xn2 xn3 yn select arg minj msej3 96103 complexity issues rationale mr paradigm mainmotivations decomposeacomplextaskinsimpleronesreducersthatcanbe executedinmainmemory reducelatency increasethroughput strategy 3steps mapdatapreparationcommunicationreduceoutput production tradeoff findabalancebetweenamountofcommunicationandsizeofsingle reducer thelesscommunicationisusedthehigheristhecomplexityeg size memoryoccupationandcomputingtimeofeachsingletask nosinglemrsolutionsbutaspectrumofalternativesolutions 98103 inputoutput dependency mr letusconsideracomputingproblemcharacterizedby 1 asetofpinputs 2 asetofoutputs 3 amanymanyrelationshiplinkingoutputstoinputs whenisaproblemsolvablebymraproblemcanbesolvedbyasingle mrjobifforeveryoutputoftheproblemthereisatleastareducerthatis assignedalltheinputsthatarerelatedtothatoutput problemsmaygreatlydifferintermsoftheirinputoutputrelationships eg onetoonemanytomanyandmaybedescribedbyagraph 99103 graph representation numberofinputs p reducerinputsize faninq mapperreplicationrate fanoutr inthisexamplep8q4r2 100103 reducer size replication rate thecomplexityofamralgorithmdependsonthetradeoffoftwo quantities6 1 mapperreplicationrater numberofkeyvaluepairsproducedbymap tasksperinput thisquantityhasanimpactonthecommunication cost 2 reducerinputsizeq thenumberofinputsofreducerfanin quantityisrelatedtothecomplexityofthereducerandthenumberof reducers thelargerqthesmallerthenumberofreducersandthe parallelismandthehighertheircomplexityeg memorysize quantitiesaretypicallyinverselyproportionalie qrconstantie higherthecomplexityofthereducerthelowerthecommunicationcost andviceversa forinstancewecandecreasethecommunicationcostby groupinginputs 101103 paired assessment verylargenumberpofobjectseg imagesvectorsvariables computesomestatisticsrelatedtoallpairseg mostsimilarormost correlated inputmatrixofsizedpnandwewanttohaveddt ofsizepp instanceeachrowcorrespondtoanimageofnpixels givenpinputswehavecid0pcid1 pp12outputsie complexityop2 2 1 2 1 1 3 2 1 4 3 p inputs pp12 outputs 4 4 5 5 4 6 6 5 6 102103 paired assessment mr wepartitionthepinputsinto2gpequalsizedgroups wehavecid0gcid1 reducersoneforeachpairofgroups 2 eachreducergetsinputsfromtwogroupssoitwillhaveanumberof inputsq2pg eachobjectbelongingtoagroupisreplicatedrg1times since qr1qg2p isconstantthereducersizeisinverselyproportionaltothereplication size 1 2 1 3 reducer 1 gp rp1 q2 gp2 rp21 q 4 3 4 1 1 2 reducer 1 2 1 1 3 4 rr ee dd uu cc ee rr 32 1 2 1 2 3 4 3 4 1 5 reducer 2 5 4 5 reducer 13 5 6 5 6 4 6 reducer 14 6 5 6 reducer 15 3 4 3 5 reducer 3 5 6 103103 tradeoff communication vs reducer complexity p6 inputs 12 13 14 1 15 2 16 3 12 12 34 6 pairs 123 4 23 34 12 56 6 pairs 15 pairs 456 5 56 34 56 6 pairs 26 6 34 g3 groups g2 groups replication rate r2 replication rate r1 36 3223 reducers 2121 reducers q2634 inputs q2626 inputs 56 g6 groups one per element replication rate rg15 gg1265215 reducers qr12p q2pg2 inputs 104103 leobreiman pastingsmallvotesforclassificationinlargedatabasesandonline machinelearning36185103jul1999 tinkamho therandomsubspacemethodforconstructingdecisionforests ieeetransactionsonpatternanalysisandmachineintelligence 208832844aug1998 gilleslouppeandpierregeurts ensemblesonrandompatches inpeteraflachtijldebieandnellocristianinieditorsmachine learningandknowledgediscoveryindatabasespages346361 berlinheidelberg2012springerberlinheidelberg jmailloitrigueroandfherrera amapreducebasedknearestneighborapproachforbigdata classification in2015ieeetrustcombigdataseispavolume2pages167172aug 2015 105103 mparsian dataalgorithms recipesforscalingupwithhadoopandspark oreillymedia2015 anandrajaramanjureleskovecandjeffreydullman miningmassivedatasets 2014 claudioreggianiyannaëlleborgneandgianlucabontempi featureselectioninhighdimensionaldatasetusingmapreduce inbartverheijandmarcowieringeditorsartificialintelligencepages 101115cham2018springerinternationalpublishing jwangwliuskumarandsfchang learningtohashforindexingbigdata asurvey proceedingsoftheieee10413457jan2016 106103



online learning stochastic approximations leon bottou att labsresearch red bank nj07701 abstract theconvergence ofonlinelearningalgorithms isanalyzed usingthe tools stochastic approximation theory proved weak conditions general framework online learning algorithms first presented framework encompasses common online learning algorithms use today illustrated several examples stochastic approximation theory provides general results de scribing convergence learning algorithms revised version may 2018 1 introduction almost early work learning systems focused online algo rithms hebb 1949 rosenblatt 1957 widrow hoff 1960 amari 1967 kohonen 1982 early days algorithmic simplicity online algorithms requirement still case comes handling large reallife training sets le cun et al 1989 muller gunzinger guggenbuhl 1995 early recursive adaptive algorithms introduced years robbins andmonro 1951 often people widrow stearns 1985 first developed engineering world recursive adap tationalgorithmshaveturnedintoamathematicaldiscipline namelystochas tic approximations kushner clark 1978 ljung soderstrom 1983 benveniste metivier priouret 1990 although domains enjoyed spotlights scientific fashion different times different reasons essentially describe el ementary ideasmanyauthorsofcoursehavestressed thislessthanfortuitous similarity learning algorithms recursive adaptation algorithms mendel fu 1970 tsypkin 1971 present work builds upon similarity online learning algorithms analyzed using stochastic approximation tools convergence char acterized weak conditions expected risk must reasonably well behaved learning rates must decrease appropriately 1 2 leon bottou main discussion describes general framework online learning al gorithms presents number examples analyzes dynamical prop erties several comment sections illustrate ideas generalized relate aspects learning theory words main discussion gives answers comments raise questions casual readers may skip comment sections 2 framework online learning systems starting point mathematical study online learning must mathematical statement subjective understanding learning system difficult agree statement learning systems andoftenresent mathematical reduction anessential personal experience contributionborrows theframework introducedby therussian school tsypkin 1971 vapnik 1982 formulation used understand ing significant number online learning algorithms demonstrated examples presented section 3 21 expected risk function tsypkin 1971 tsypkin 1973 goal learning system consists finding minimum function jw named expected risk function function decomposed follows jw e qzw qzw dpz 21 z z minimization variable w meant represent part learning system must adapted response observing events z occurring real world loss function qzw measures performance learning system parameter w circumstances described event z common mathematical practice suggests represent w z elements adequately chosen spaces w z occurrence events z modeled random independent obser vations drawn unknown probability distribution dpz named grand truth distribution risk function jw simply expectation loss function qzw fixed value parameter w risk function jw poorly defined grand truth distribution dpz unknown hypothesis consider instance neural network system optical zip code recog nition described le cun et al 1989 observation z pair xy composed zip code image x intended interpretation param eters w adaptable weights neural network loss function online learning stochastic approximations 3 qzw measures economical cost hard currency units delivering letter marked zip code z given answer produced network image x cost minimal network gives right answer otherwise loss function measures higher cost detecting error rerouting letter comments probabilities used framework representing unknown truth underlying occurrences observable events using successive observations zt learning system uncover part truth form param eter values wt hopefully decrease risk functional jwt use probabilities different bayesian practice probability distribution represents increasing knowledge learning system approaches however reconciliated defining parameter space w another space probability distributions analysis must carefully handle two different probability distributions different meanings framework every known fact real world removed distribution dpz properly redefining observation space z theloss functionqzw considerfor instancethataknownfraction thezip code images spoiled image capture system observation z factored triple κxy composed envelope x intended zip code binary variable κ indicating whether zip code image spoiled loss function redefined follows jw qzw dpκxy z qzwdpκxy dpxy z cid18z cid19 inner integral decomposition new loss function qxyw measuresthesystemperformanceonredefinedobservationsxythisnewloss function accounts known deficiencies image capture system factorization technique reveals new probability distribution dpxy longer representative priori knowledge technique apply knowledge involving learning system say instance unknown function smooth mean pays bias learning algorithm towards finding smoother functions statement describe property grand truth distribution meaning attached particular learning system suggests redefinition problem merely suggests modification learning system like introduction regularization parameter 22 gradient based learning expected risk function 21 cannot minimized directly grand truth distribution unknown however possible compute 4 leon bottou w w t1 g j l delay figure 1 batch gradient descent parameters learn ing system updated using gradient empirical risk jˆ defined training set l approximation jw simply using finite training set independent observations z z 1 l l 1 jw jˆ w qz w 22 l n l n1 x general theorems vapnik 1982 show minimizing empirical risk jˆ w provide good estimate minimum expected risk jw l training set large enough line work provided way understand generalization phenomenon ie ability system learn finite training set yet provide results valid general 221 batch gradient descent minimizing empirical risk jˆ w achieved using batch gradient l descent algorithm successive estimates w optimal parameter com puted using following formula figure 1 learning rate γ positive number l 1 w w γ jˆ w w γ qz w 23 t1 w l w l i1 x properties optimization algorithm well known section 42 learning rate γ small enough algorithm converges wards local minimum empirical risk jˆ w considerable convergence l speedups achieved replacing learning rate γ suitable def inite positive matrix dennis schnabel 1983 iteration batch gradient descent algorithm figure 1 however involves burdening computation average gradients loss function qz w entire training set significant computer w n sources must allocated order store large enough training set compute average online learning stochastic approximations 5 z world g j w t1 w delay figure 2 online gradient descent parameters learn ing system updated using information extracted real world observations 222 online gradient descent elementary online gradient descentalgorithmisobtained bydropping averaging operation batch gradient descent algorithm 23 instead averagingthegradientofthelossoverthecompletetrainingset eachiteration online gradient descent consists choosing example z random updating parameter w according following formula w w γ qz w 24 t1 w averaging update possible choices training example z would restore batch gradient descent algorithm online gradient de scent simplification relies hope random noise introduced procedure perturbate average behavior algorithm significant empirical evidence substantiate hope online gradient descent also described without reference train ing set instead drawing examples training set directly use events z observed real world shown figure 2 formu lation particularly adequate describing adaptive algorithms simul taneously process observation learn perform better adaptive algorithms useful tracking phenomenon evolves time airplane autopilot instance may continuously learn commands affect route airplane system would compensate changes weather petrol weight comments formulating online gradient descent without reference training set presents theoretical interest iteration algorithm uses example zt drawn grand truth distribution instead finite training set average update therefore gradient descent algorithm directly optimizes expected risk 6 leon bottou direct optimization shortcuts usual discussion differences tweenoptimizingtheempiricalriskandtheexpectedriskvapnik1982vapnik 1995 proving convergence online algorithm towards minimum theexpected riskprovides analternative thevapnik proofsof theconsistency learning algorithms discussing convergence speed online algo rithm provides alternative vapnikchervonenkis bounds alternative comes severe restrictions convergence proofs pro posed section 5 address convergence algorithm towards local minimum safely conjecture general study con vergence online algorithm towards global minimum handle central concepts necessary sufficient conditions consistency learning algorithm vapnik 1995 23 general online gradient algorithm rest contribution addresses single general online gradient algo rithm algorithm minimizing following cost function cw cw e qzw qzwdpz 25 z z iteration algorithm consists drawing event z distri bution dpz applying following update formula w w γ hz w 26 t1 learning rates γ either positive numbers definite positive matrices update term hzw fulfills following condition ezhzw wcw 27 distribution function dpz understood grand truth distribution cost function cw minimized algorithm equal expected risk jw setup addresses adaptive version online gradient descent without reference training set results however remain valid consider discrete distribution func tion defined particular training set z z cost function cw 1 l minimizedbythisalgorithmisthenequaltotheempiricalriskjˆ thissecond l setup addresses use online gradient descent optimizing training error defined finite training set comments typography conscious readers notice subtle difference ob servable events z used cost function 25 events z drawn iteration algorithm 26 simplestcase indeedthese two vari ables represent similar objects single example drawn iteration online learning stochastic approximations 7 online gradient descent algorithm framework described also applies complex cases like minibatch noisy gradient descent mini batch gradient descent uses several examples iteration collectively ferred z noisy gradient descent uses noisy update term wcwtξt analysis presented contribution holds long update term fulfills condition 27 finally examples z assumed independently drawn sin gle probability distribution function dpz practice however examples often chosen sequentially training set tools indeed dealing examples z drawn using markovian process benveniste metivier priouret 1990 3 examples section presents number examples illustrating diversity learn ing algorithms expressed particular cases general online gradient descent algorithm section 23 classical algorithms found tsypkin 1971 algorithms designed well defined cost function like adaline section 311 multilayer perceptron section 312 al gorithms initially refer particular cost function refor mulated online gradient descent procedures like kmeans section 322 orlvq2section323thecostfunctionthenprovides auseful characteriza tion algorithm finally certain algorithms like kohonens topological maps kohonen 1982 poorly represented minimization cost function yet authors found useful coerce algorithms online gradient descent anyway schumann retzko 1995 31 online least mean squares 311 widrows adaline adaline widrow hoff 1960 one learning systems designed beginning computer age online gradient descent attractive proposition requiring little hardware adaline could fit refrigerator sized cabinet containing forest potentiometers electrical motors adaline figure 3 learning algorithm adapts parameters single threshold element input patterns x recognized class 1 1 according sign wx β practical consider augmented input pattern x containing extra constant coefficient equal 1 bias β represented extra coefficient parameter vector w 8 leon bottou g x wx yx world figure 3 widrows adaline adaline computes binary dicatorby thresholding linear combinationof itsinput learning achieved using delta rule convention output threshold element written yˆ x signwx sign w x 31 w x training adaline provided pairs z xy representing put patterns desired output adaline parameter w adjusted using delta rule prime denotes transposed vectors w w γ wx x 32 t1 thisdeltaruleisnothingmorethananiterationoftheonlinegradientdescent algorithm 24 following loss function q zw wx2 33 adaline loss function take discontinuity threshold element 31 account linear approximation real breakthrough apparently natural loss function yˆ x2 discontinuous loss w functionisdifficulttominimizebecauseitsgradientiszeroalmosteverywhere furthermore solutions achieving misclassification rate would cost cw regardless margins separating examples decision boundary implemented threshold element 312 multilayer networks multilayer networks initially designed overcome computational limitation threshold elements minsky papert 1969 arbitrary binary mappings implemented stacking several layers thresh old elements layer using outputs previous layers elements inputs adaline linear approximation could used frame workbecauseignoringthediscontinuitieswouldmaketheentiresystemlinear online learning stochastic approximations 9 regardless number layers key learning algorithm multi layer networks rumelhart hinton williams 1986 consisted noticing discontinuity threshold element could represented smooth nonlinear approximation signwx tanhwx 34 using sigmoidal elements reduce computational capabilities multilayer network approximation step function sigmoid made arbitrarily good scaling coefficients parameter vector w multilayer network sigmoidal elements implements differentiable function fxw input pattern x parameters w given input pattern x desired network output backpropagation algorithm rumelhart hinton andwilliams 1986provides anefficient way compute gradients mean square loss function 1 q zw fxw2 35 mse 2 batch gradient descent 23 online gradient descent 24 used considerable success large redundant data sets online version converges much faster batch version sometimes orders magnitude muller gunzinger guggenbuhl 1995 intuitive explanation found following extreme example consider train ing set composed two copies subset batch algorithm 23 averages gradient loss function whole training set causing redundant computations hand running online gradient descent 24 examples training set would amount performing two complete learning iterations duplicated subset 32 non differentiable loss functions many interesting examples involve loss function qzw differ entiable subset points probability zero intuition suggests minor problems iterations online gradient descent zero probability reach one points even reach one points draw another example z intuition formalized using general online gradient descent algorithm 26 general algorithm use update term hzw fulfills condition 27 assume cost function cw made differentiable loss function qzw integrated probability distribution dpz following update term amounts drawing another example whenever 10 leon bottou retina associative area treshold element signw x w x x figure 4 rosenblatts perceptron composed fixed prepro cessing trainable threshold element reach non differentiable point loss function qzw differentiable w hzw 36 0 otherwise foreach parameter valuew reached bythe algorithm weassume thatthe loss function qzw differentiable everywhere except subset examples z probability zero condition 27 rewritten using 36 explicit integration operators hzwdpz qzwdpz qzwdpz 37 w w z z z lebesgue integration theory provides sufficient condition swapping integration differentiation operators parameter w value w reached online algorithm sufficient find integrable r function φzw neighborhood ϑw w z v ϑw qzvqzw wvφzw 38 condition 38 tests maximal slope loss function qzw isconveniently bounded isobviously truewhenthelossfunctionqzw differentiable integrable gradient obviously false loss function continuous given previous assumption concern ing zero probability non differentiable points condition 38 sufficient condition safely ignoring non differentiable points 321 rosenblatts perceptron early days computer age perceptron rosenblatt 1957 generated considerable interest possible architecture general pur pose computers interest faded disclosure computational online learning stochastic approximations 11 limitations minsky papert 1969 figure 4 represents perceptron architecture associative area produces feature vector x applying predefined transformations retina input feature vector processed threshold element section 311 perceptron learning algorithm adapts parameters w thresh old element whenever misclassification occurs parameters updated according perceptron rule w w 2γ x 39 t1 learning rule derived online gradient descent applied following loss function q zw signwxywx 310 perceptron although loss function non differentiable wx null meets condition 38 soon expectation ex defined therefore ignore non differentiability apply online gradient descent algo rithm w w γ signwx x 311 t1 since desired class either 1 1 weights modified pattern x correctly classified therefore parameter update 311 equivalent perceptron rule 39 perceptron loss function 310 zero pattern x correctly recognized member class 1 otherwise value positive proportional dot product wx corresponding cost function reaches minimal value zero examples properly recognized weight vector w null training set linearly separable ie threshold element achieve zero misclassification perceptron algorithm finds linear separation probability one otherwise theweights w quickly tend towards zero 322 kmeans kmeans algorithm macqueen 1967 popular clustering method dispatches k centroids w k order find clusters set points x x algorithm derived performing online gradient 1 l descent following loss function k q kmeansxw min xw k2 312 k1 loss function measures quantification error say error position point x replace closest centroid corresponding cost function measures average quantification error 12 leon bottou w3 w2 w1 figure 5 kmeans dispatches predefined number cluster centroids way minimizes quantification error loss function differentiable points located voronoı boundaries set centroids meets condition 38 soon expectationsexandex2aredefined ontheremainingpoints thederiva tive loss derivative distance nearest centroid w therefore ignore nondifferentiable points apply online gradient descent algorithm w w γ x w 313 t1 formula describes elementary iteration kmeans algorithm efficient choice learning rates γ suggested section 332 323 learning vector quantization ii kohonens lvq2 rule kohonen barna chrisley 1988 powerful pattern recognition algorithm like kmeans uses fixed set reference points w k class k associated reference point unknown pattern x recognized member class associated nearest reference point given training pattern x let us denote w nearest reference point denote w nearest reference point among associated correct class adaptation occurs closest reference point w associated incorrect class closest correct reference point w far away x misclassified w 6 w xw2 1δxw2 314 w w ε xw t1 w w ε xw t1 online learning stochastic approximations 13 class 1 reference points decision boundary class 2 reference points figure 6 kohonens lvq2 pattern recognition scheme outputs class associated closest reference point input pattern reference points updated pattern x misclassified fur thermore distance closest correct reference point w must exceed distance closest incorrect reference point w percentage defined parameter δ conditions met algorithm pushes closest incorrect reference point w away pattern x pulls closest correct reference point w towards pattern x intuitive algorithm derived performing online gradient descent following loss function 0 x well classified w w q zw 1 xw2 1δxw2 315 lvq2 xw δ x2 w x 2w2 otherwise function acontinuous approximation binary variable indicating whether pattern x misclassified corresponding cost function therefore continuous approximation system misclassification rate bottou 1991 analysis helps understanding lvq2 algorithm works although loss function differentiable values w meets condition 38 soon expectations ex ex2 defined therefore ignore nondifferentiable points apply online gradient descent algorithm x misclassified w 6w xw2 1δxw2 316 w w γ k xw t1 1 w w γ k xw t1 2 1 x w2 k k k 317 1 δx w2 2 1 x w2 14 leon bottou online gradient descent algorithm 316 equivalent usual lvq2 learning algorithm 314 two scalar coefficients k k merely modify 1 2 proper schedule decreasing learning rates γ 33 quasinewton online algorithms theoretical empirical evidences demonstrate batch gradient descent algorithms converge much faster scalar learning rates γ replaced definite positive symmetric matrices thatapproximate theinverse hessian cost function socalled superlinear algorithms achieve high terminal convergence speed number correct figures inthenumericalrepresentationofthesolutionincreasesexponentiallydennis schnabel 1983 techniques arealso effective speeding online gradient algo rithmstheresultshowever aremuchlessimpressive thanthoseachieved batch algorithms online gradient descent algorithm achieve super linear convergence cf comments section 4 terminal convergence online gradient algorithm limited size learning rates shown sections 4 5 decreasing learning rates quickly prevent convergence accuracy superlinear algorithm however largely irrelevant learning system severe approximations using finite training set would spoil benefits algorithm practitioners prefer techniques blessed robust convergence properties levenbergmarquardt algorithm dennis schnabel 1983 furthermore storing processing full learning rate matrix quickly becomes expensive dimension parameter vector w increases less efficient approximations designed becker le cun 1989 proven effective enough large size applications le cun et al 1989 331 kalman algorithms kalman filter theory introduced efficient way compute approximation inverse hessian certain cost functions idea easily demonstrated case linear algorithms adaline section 311 consider online gradient descent applied minimization following mean square cost function cw qzwdpz qzw wx2 318 z iteration algorithm consists drawing new pair z x distribution dpz applying following update formula w w h1 qz w w h1y wx x 319 t1 w online learning stochastic approximations 15 h denotes thehessianoftheonline empirical costfunction theonline empirical cost function simply empirical estimate cost function cw based examples z z observed far 1 1 1 c w qz w wx 2 320 2 2 i1 i1 x x h 2c w x x 321 w i1 x directlycomputingthematrixh1 ateachiterationwouldbeveryexpensive take advantage however recursion h h x x using t1 well known matrix equality abb1 a1 a1bi ba1b1 a1b 322 algorithm 319 rewritten recursively using kalman matrix k h1 resulting algorithm 323 converges much faster t1 delta rule 32 yet remains quite easy implement k x k x k k t1 1x tk tx 323 w w k wx x t1 t1 comments linear algorithm interesting optimality property tsypkin 1973 cost function 320 exactly quadratic easy prove inductionthat323minimizestheonlineempiricalcostctwateachiteration assuming wt minimum ct1w following derivation shows wt1 minimum ctw wctwt1 wctwthtwt1 wt wct1wtwqztwthth t1wqztwt 0 although property illustrates rapid convergence algorithm 323 describes algorithm tracks empirical approximation 320 cost functionthisapproximation may notprovide good generalization properties vapnik 1995 non linear least mean square algorithms multilayer networks section 312 also benefit nonscalar learning rates idea consists using approximation hessian matrix second derivatives loss function 35 written 1 2 wyfxw2 wfxw wfxwyfxw2 wfxw 2 wfxw wfxw 324 16 leon bottou approximation 324 known gauss newton approximation neglects impact non linear function f curvature cost function approximation hessian empirical online cost takes simple form htw wfxiw wfxiw 325 i1 x although real hessian negative approximated hessian always positive useful property convergence expression 325 reminiscent linear case 321 inverse computed using similar recursive equations 332 optimal learning rate kmeans second derivative information also used determine efficient learning rates kmeans algorithm section 322 simple analysis loss function 312 shows hessian cost function diagonal matrix bottou bengio 1995 whose coefficients λ k equal probabilities example x associated corresponding centroid w k probabilities estimated simply counting many ex amples n k associated centroid w k iteration corresponding online algorithm consists drawing random example x finding closest centroid w k updating count centroid following equations n t1k n tk1 326 w t1k w tk nt1 1kx w tk algorithm 326 quickly locates relative position clusters data terminal convergence however slowed noise implied random choice examples experimental evidence bottou bengio 1995 suggest best convergence speed obtained first using online algorithm 326 switching batch superlinear version kmeans 4 convex online optimization next two sections illustrate nicely convergence online learn ing algorithm analyzed modern mathematical tools designed stochastic approximations particular section addresses simple convex case focusing mathematical tools relation classical analysis batch algorithms presentation owes much remarkable lecture michel metivier metivier 1981 online learning stochastic approximations 17 41 general convexity analysis presented section addresses convergence gen eral online gradient algorithm section 23 applied optimization differentiable cost function cw following properties cost function cw single minimum w cost function cw satisfies following condition ε 0 inf w w cw 0 41 w ww2ε condition 41 simply states opposite gradient cw al w ways pointstowards theminimum w particular formulationalso rejects cost functions plateaus gradient vanishes without making us closer minimum condition weaker usual notion convexity indeed easy think non convex cost function single minimum satisfies condition 41 hand proving differentiable strictly convex functions satisfy condition neither obvious useful 42 batch convergence revisited convergence proof general online learning algorithm follow ex actly three steps convergence proofs batch learning algorithms steps consist defining lyapunov criterion con vergence b proving criterion converges c proving convergence implies convergence algorithm steps illustrated cases continuous gradient descent batch gra dient descent 421 continuous gradient descent continuous gradient descent mathematical description ideal convergence gradient descent algorithm differential equation defines parameter trajectory wt continuous function time dw cw 42 w dt step convergence proof begins definition lyapunov function ie positive function indicates far target ht wtw2 43 18 leon bottou step b computing derivative ht shows lyapunov function ht monotonically decreasing function dh dw 2ww 2w w cw 0 44 w dt dt since ht positive decreasing function limit step c since monotonic function ht converges grows gra dient tends towards zero dh 2w w cw 0 45 w dt let us assume lyapunov function ht converges value greater thanzero certain time distance ht wtw2 would remain greater positive value ε result incompatible condition 41andresult 45thelyapunov functionhtthereforeconverges tozero result proves convergence continuous gradient descent 42 wt w 46 422 discrete gradient descent batch gradient descent algorithm introduced section 221 context learning algorithms cost function cw minimized iteratively applying following parameter update w w γ cw 47 t1 w equation 47 discrete version continuous gradient descent 42 although discrete dynamics brings new convergence issues analysis convergence follows three elementary steps step convergence proof begins definition lyapunov sequence ie sequence positive numbers whose value measure far target h w w2 48 lemma useful point introduce sufficient criterion convergence positive sequence u intuitively sequence u converges bounded oscillations damped oscillations monitored summing variations u u whenever u u t1 t1 positive variations represented thick lines figure 7 infinite sum positive variations converges certain oscillationsaredampedifalltermsofthesequencearepositivethiscondition also ensures sequence bounded online learning stochastic approximations 19 figure 7 convergence infinite sum positive increases thick lines sufficient although necessary con ditionfortheconvergence ofapositivesequence h thiscondition ensures sequence bounded ii oscil lations damped intuition easily formalized decomposing term u sequence using sum positive variations t1 u u x x x 0 49 i1 0 otherwise i1 x sum negative variations t1 u u x 0 x 0 410 i1 x otherwise i1 x sum positive variations converges decomposition provides upper bound positive sequence u 0 u u u u 411 1 1 0 furthermore since u 0 decompositions also provides lower bound sum negative variations 0u 0 412 1 since bounded monotonically decreasing sequence converges limitssincebothsequences ands converge thesequenceu converges u u 1 u 0 u u 0 413 u t1 u t1 x 20 leon bottou convergence infinite sum positive variations therefore sufficient condition convergence sequence since positive variations positive sufficient prove bounded summand convergent infinite sum step b second step consists proving lyapunov sequence h converges using definition 48 gradient descent algorithm47 write anexpression variations lyapunov criterion h h 2γ w w cw γ2 cw 2 414 t1 w w convexity criterion 41 ensures first term expression always negative unlike continuous variations 44 expression contains positive second term reflects discrete dynamics algorithm additional conditions must introduced order contain effects second term first condition 415 states learning rates γ decrease fast enough expressed convergence infinite sum squared learning rates γ2 415 i1 x second condition 416 ensures size gradients grow fast move away minimum linear condition met soon eigenvalues hessian matrix bounded cw2 abw w2 ab 0 416 w condition required polynomial decrease learn ing rates would easily canceled exponentially growing gradients transform equation 414 using bound size gradients 4161 h 1γ2bh 2γ w w cw γ2a γ2a 417 t1 w define two auxiliary sequences µ h t1 1 µ µ 0 h µ h 418 1γ2b i1 convergence µ µ 0 easily verified writing logµ using condition 415 multiplying lefthandside right hand side 417 µ obtain h h γ2µ 419 t1 1thanks wei wen pointing sign typo inequalities 417 april 2017 online learning stochastic approximations 21 since right hand side 419 positive positive variations h equal γ2µ summand convergent infinite sum according lemma 413 sequence h converges since µ converges µ 0 convergence implies convergence h step c wenowprovethattheconvergence ofthelyapunovsequenceimplies theconvergence ofthe discrete gradient descent algorithm since thesequence h converges equation 417 implies convergence following sum γ w w cw 420 w i1 x must introduce additional condition learning rates γ condition limits rate decrease learning rates condition required decreasing thelearning rates tooquickly could stopthe pro gression algorithm towards minimum condition expressed divergence infinite sum learning rates γ 421 i1 x condition 421 intuitively natural imagine current param eter far away minimum area gradient approx imately constant successive updates γ cw allowed move w parameter arbitrary distances conditions 420 421 play fact role 45 continuous case however unlike continuous case know h ismonotonicallydecreasingandwecannotconcludefromthesetwoconditions alone w w cw 0 422 w however canstill reachthe conclusion know h converges reasoning contradiction let us assume h w w2 converges value greater zero therefore certain time remains greater ǫ 0 assumption 41 implies w w cw w remains greater strictly positive quantity since would cause sum 420 diverge since case conclude h converges zero w w 423 thanks 416 convergence also implies 422 holds2 besides existence single minimum w general convexity criterion 41 introduce three additional conditions obtain 2thanks francescoorabonafor pointing originalversionof section proof poorly written may 2018 22 leon bottou convergence two conditions 415 421 directly address learning rateschedule thelastcondition416statesthatthegrowthofthegradients limited comments condition 416 states gradient increase linearly whenwemoveawayfromtheminimumboundingtheeigenvaluesofthehessian easy way make sure condition holds general theorems howeveronlyrequireapolynomialboundonthesizeofthegradientbenveniste metivier priouret 1990 proof presented section addresses case decreasing learning rates different approach step b leads convergence results case constant learning rates instead boundingthe second term thevariations 414 compare sizes terms assuming condition 416 0 appears choosing constant learning rate smaller 2b makes variations 414 negative result consistent usual p criterion since minimal value b square highest eigenvalue hessian matrix analysis also provides convergence speed results bounding right hand side 414 gives measure quickly lyapunov sequence decreases expected best bounds obtained wt w γtwcwt aligned thiscan beachieved choosing learningrate matrix γt approximates inverse hessian non scalar learning ratesonlyintroducesminorchangesintheproofsthelearningratematrixmust besymmetricand definitepositive conditions 415 and421 mustrefer highest lowest eigenvalues learning rate matrix 43 lyapunov process convergence proofsfor general online gradient algorithmsection 23 established using approach obvious however online learning algorithm mislead consistent choice improbable examples therefore hope prove algorithm always converges best possible result almost sure convergence say algorithm converges towards solution probability 1 eachiterationofthegeneralgradientdescent algorithmconsistsofdrawing event z distribution dpz applying update formula w w γ hz w 424 t1 update term hz w fulfills condition ezhzw wcw 425 online learning stochastic approximations 23 learning rates γ positive numbers definite positive trices main discussion section addresses scalar learning rates using learning rate matrix introduces minor changes discussed comments step first step proof consists defining lyapunov process measures far solution h w w2 426 although definition 426 looks similar discrete batch gradient case 48 notation h 426 denotes random variable depends previous choices example events z step b batch gradient case expression variations h derived using equations 424 426 h h 2γ w whz w γ2hz w 2 427 t1 convergence proof discrete gradient descent section 422 relies lemma 413 establish convergence lyapunov criterion lemma defines sufficient condition based variations criterion expression 427 however explicitly refers random example z using lemma 413 would anattempt prove algorithm converges forall imaginablechoice oftheexamples including themost improbable continuously drawing example correct approach consists removing dependency taking conditional expectation variations 427 given information p available iteration p z z w w γ γ 428 t1 0 0 conditional expectation variations gives sufficient information apply quasimartingale convergence theorem 44 quasimartingales thequasimartingaleconvergence theoremisinfactverysimilartothelemma 413 presented section 422 following discussion presents theorem without proof exposes analogy lemma proofs found metivier 1983 fisk 1965 given past information p wish define deterministic cri terion distinguishing positive variations negative varia tions process u sign variation u u acceptable t1 choice depends u fully determined given p t1 24 leon bottou problem solved considering conditional expectation variations δ 1 eu t1 u p 0 429 0 otherwise variable δ defined 429 defines variations considered posi tive convergence infinite sum positive expected variations sufficient condition almost sure convergence positive process u u 0 u u 0 430 eδ tu t1 u t1 x thisresult isaparticularcaseoftheorem94andproposition95inmetivier 1983 name quasimartingale convergence theorem comes fact condition 430 also implies process u quasimartingale fisk 1965 comparing theorem 430 lemma 413 explains easily quasimartingales useful studying convergence online algorithms fact known since gladyshev 1965 45 convergence online algorithms convex case convergence result allow us proceed step b proof step b continued following expression obtained taking conditional expectation 427 factoring constant multipliers eh h p 2γ w wehz w p t1 γ2 e hz w 2 p 431 cid16 cid17 expression simplified using condition 425 eh h p t1 2γ tw wezhzw γ t2 ezhz tw t2 2γ tw w wcw γ t2 ezhz tw t2 432 first term upper bound negative according condition 41 section 422 two additional conditions required address discrete dynamics algorithm first condition 433 similar 415 states learning rates decreasing fast enough γ2 433 i1 x second condition 434 serves purpose condition 416 term bounds growth second moment update hzw ezhzw2 abww2 ab 0 434 online learning stochastic approximations 25 transform equation 432 using condition e h 1γ2bh p 2γ w w cw γ2a 435 t1 w cid16 cid17 define two auxiliary sequences µ h 418 multiplying lefthandside right hand side 432 µ obtain e h h p γ2µ 436 t1 cid16 cid17 simple transformation gives bound positive expected varia tions h eδ h h eδ e h h p γ2µ 437 t1 t1 cid16 cid17 since bound summand convergent infinite sum theorem 430 implies h converges almost surely since sequence µ converges µ 0 lyapunov process h also converges almost surely step c prove convergence lyapunov process implies theconvergence ofthediscretegradientdescent algorithmsinceh converges equation 435 implies convergence following sum γ w w cw 438 w i1 x must introduce additional condition learning rates γ limits rate decrease learning rates condition similar condition 421 γ 439 i1 x reason contradiction shown discrete gradient descent case know h converges probability 1 assume h converges quantity greater zero nonzero probability assump tion 41 implies w w cw eventually remains greater w strictly positive quantity since would cause divergence sum 438 conclude w w 440 discrete gradient case also w w cw 0 441 w besides general convexity criterion 41 introduce three additional conditions obtain convergence two conditions 433 439 directly address learning rate schedule batch gradient case last condition 434 similar condition 416 contains additional variance term reflects stochastic dynamics online gradient descent 26 leon bottou comments equations414 and432 look verysimilarthesecondtermoftherighthand side 432 however refers second moment updates instead norm gradients term decomposed follows γ t2e zhzw2 γ t2wcw2 γ t2var zhzw 442 second term decomposition depends noise implied stochastic nature algorithm variance remains strictly positive general even solution w fact main explanation dy namical differences batch gradient descent online gradient descent let us assume algorithm converges first term right hand side 432 tends towards zero well first term 442 thereforewriteanasymptoticequivalenttotheexpectedvariationthelyapunov criterion eht1htpt γtγtvar zhzwwtwwcw 443 thisresultmeansthatthequantitiesγtvar zhzwandwtwwcwkeep order magnitude convergence since latter quantity related distance optimum cf comments section 422 convergence speed depends fast learning rates γt decrease decrease rate turn limited condition 439 analysis repeated non scalar learning rates approximating inverse hessian algorithm converges faster using scalar learning rate equal inverse largest eigenvalue hessian result course assume learning rates still fulfill criterions 433 439 batch gradient descent case cf comments section 422 final comment expands section 32 discussing online gradient descent non differentiable functions proof presented section never uses fact wcw actually gradient cost cw references gradient eliminated merging conditions 41 425 ε 0 inf wwe zhzw 0 444 ww2ε condition 444 together usual conditions 433 439 434 sufficient ensure convergence algorithm 424 result makes reference differentiable cost function 5 general online optimization thissection analyzes theconvergence ofthegeneral online gradient algorithm section 23 without convexity hypothesis words cost function cw several local minima online learning stochastic approximations 27 two ways handle analysis first method consists partitioning parameter space several attraction basins discussing conditions algorithm confines parameters w single attractionbasindefiningsuitablelyapunovcriterionskrasovskii1963and proceeding convex case since online gradient descent algorithm never completely confines parameter single attraction basin must also study algorithmhops fromone attraction basin another much simpler method quickly gives subtly different result instead proving parameter w converges prove cost function cw gradient cw converge discussion presented w expanded version proof given bottou 1991 note sections 51 52 revised october 20123 51 assumptions convergence results rely following assumptions cost function cw three times differentiable continuous derivatives4 bounded ie cw c min sume without loss generality cw 0 ii usual conditions learning rates fulfilled γ2 γ 51 t1 t1 x x iii second third fourth moments update term grow fast norm parameters conditions slightly restrictive condition 434 order better control distribution tails k k k 234 ezkhzwk k b kkwk 52 iv norm parameter w larger certain horizon opposite gradient cw points towards origin w inf w cw 0 53 w w2d assumption 53 prevents possibility plateaus parameter vector grow indefinitely without ever escaping beyond certain horizon update terms always moves w closer origin average 3thanks silvere bonnabel silverebonnabelminesparistechfr 4weakening nontrivialassumption demands serious efforts fort pages1995 28 leon bottou condition easy verify case kmeans algorithm sec tion 322 instance cost function never reduced moving cen troids beyond envelope data points multilayer networks section 312 however always fulfill condition sigmoid flat asymptotes practice however common choose desired values smaller sigmoid asymptotes add small linear term sigmoid makes sure rounding errors make sigmoid gradient negative well known tricks fact ensure condition 53 fulfilled similar discussion applies lvq2 algorithm section 323 52 global confinement first part analysis consists taking advantage assumption 53 proving parameter vector w almost surely confined bounded region proof relies three steps step define suitable criterion f ϕw2 54 0 x ϕx xd2 x step b note definition ϕ implies ϕyϕx xϕxy x2 inequality becomes equality x greater applying inequality difference f f t1 f f 2γ w hz w γ2hz w 2ϕw2 t1 4γ2w hz w 2 4γ3w hz w hz w 2 55 γ4hz w 2hz w 2 thanks inequality cauchyschwartz write f f 2γ w hz w ϕw2γ2khz w k2ϕw2 t1 4γ2kw k2khz w k2 4γ3kw kkhz w k3 γ4khz w k4 taking expectation ef t1 f p 2γ tw wcw tϕw t2γ t2ezkhzw tk2 ϕw t2 4γ t2kw tk2ezkhzw tk2 4γ t3kw tkezkhzw tk3 γ t4ezkhzw tk4 online learning stochastic approximations 29 thanks assumption 52 positive constants b 0 0 ef f p 2γ w cw ϕw2γ2a b kw k4 t1 w 0 0 therefore positive constants b ef f p 2γ w cw ϕw2γ2abf t1 w w2 first term right hand side inequality null ϕw2 0 w2 first term right hand side inequality negative assumption 53 therefore write ef f p γ2abf 56 t1 proceed along well known lines first transform bound expected variations 435 define two auxiliary quantities µ f 418 expected variations f bounded shown equation 436 bound positive expected variations f eδ f f eδ e f f p γ2µ 57 t1 t1 cid16 cid17 theorem430thenimpliesthatf convergesalmostsurelythisconvergence implies f converges almost surely step c let us assume f converge value f greater 0 large enough convergence implies w2 w2 greater t1 inequality 55 equality equality implies following infinite sum converges almost surely γ w cw ϕw2 58 w i1 x since γ limϕw2 0 result compatible sumption 53 must therefore conclude f converges zero p global confinement convergence f means norm w2 parameter vector w bounded words assumption 53 guarantees thattheparameterswillbeconfinedinaboundedregioncontainingtheorigin confinement property means continuous functions w bounded assume course parameter space finite dimen sion include w t2 ezhzw2 derivatives cost func tion cw rest section positive constants k k etcare 1 2 introduced whenever bound used 30 leon bottou 53 convergence online algorithms general case proceed analysis general online gradient algorithm step define following criterion h cw 0 59 step b bound variations criterion h using first order taylor expansion bounding second derivatives k 1 h h 2γ hzw cw γ2hzw 2k 510 t1 w 1 inequality rewritten h h 2γ hzw cw γ2hzw 2k 511 t1 w 1 take conditional expectation using 27 eh t1 h p 2γ wcw t2 γ t2ezhzw tk 1 512 result leads following bound eh h p γ2k k 513 t1 2 1 positive expected variations h bounded eδ h h eδ eh h p γ2k k 514 t1 t1 2 1 since bound summand convergent infinite sum theorem 430 implies h cw converges almost surely cw c 515 step c last step proof departs convex case proving cw converges zero would strong result equivalent proving convergence global minimum however prove gradient cw converges zero almost surely w taking expectation 512 summing 1 see theconvergence cw implies theconvergence thefollowing infinite sum γ cw 2 516 w t1 x convergence imply yet squared gradient cw w converges define second criterion g cw 2 517 w online learning stochastic approximations 31 poor bad ok good figure 8 extremal points include global local minima also include poor solutions like saddle points asymptotic plateaus every user multilayer network training algorithms well aware possibilities variations g easily bounded using taylor expansion procedure demonstrated variations h g g 2γ hzw2cw cw γ2hzw2k 518 t1 w w 3 taking conditional expectation bounding second derivatives k 4 eg g p 2γ cw 2k γ2k k 519 t1 w 4 2 3 bound positive expected variations g eδ g g eδ eg g p t1 t1 γ cw 2k γ2k k 520 w 4 2 3 two terms onthe right hand side summands convergent infinite sums 516 51 theorem 430 implies g converges almost surely result 516 implies limit must zero g 0 cw 0 521 w 54 convergence extremal points let us summarize convergence results obtained general gradient descent algorithm section 23 results based four assump tions presented section 51 parameter vectors w confined probability 1 bounded region parameter space result essentially consequence hypothesis 53 32 leon bottou ii cost function cw converges almost surely cw c iii gradient cw converges almost surely 0 w cw 0 w theconvergence ofthegradient isthemost informativeresult figure8shows several regions gradient goes zero regions include local minima saddle points local maxima plateaus confinement result prevents parameter vector w diverge asymptoticplateauexperienceshowsthathypothesis53isverysignificant well known indeed divergence occurs easily desired outputs multilayer network equal asymptotes sigmoid saddle points local maxima usually unstable solutions small isotropic noise algorithm convergence move parameter vec tor away cannot however discard solutions easy construct cases stochastic noise introduced online gradient descent procedure sufficient isotropic convergence extremal points concludes discussion 6 conclusion online learning framework presented document addresses signif icant subset online learning algorithms including limited adaline perceptron multilayer networks kmeans learning vector quantiza tion kalman style algorithms formalism provides clear statement goal learning procedure includes provisions handling non differentiable cost functions quasinewton algorithms general convergence results based theory stochastic approx imations main results address convergence minimum convex cost function convergence extremal points gen eral cost function final convergence speed online learning algorithm amenable theoretical analysis using tools possibility analyzing long range convergence speed achieved restricted cases saad solla 1996 remains open question acknowledgments grateful vladimir vapnik yoshua bengio com ments suggestions resulted many improvements document online learning stochastic approximations 33 references amari si 1967 theory adaptive pattern classifiers ieee transac tions electronic computers ec16299307 becker le cun 1989 improving convergence back propagation learning secondorder methods touretzky hinton g sejnowski editors proceedings 1988 connec tionist models summer school pages 2937 san mateo morgan kauf man benveniste metivier priouret p 1990 adaptive algorithms stochastic approximations springer verlag berlin new york bottou l 1991 une approche theorique de lapprentissage connexion niste applications la reconnaissance de la parole phd thesis uni versite de paris xi orsay france bottou l bengio 1995 convergence properties kmeans algorithm advances neural information processing systems vol ume 7 mit press denver dennis j andschnabel r b1983 numerical methods unconstrained optimization nonlinear equations prenticehall inc englewood cliffs new jersey fisk l 1965 quasimartingales transactions american math ematical society 120359388 fort j c pages g 1995 convergence kohonen algorithm general neighborhood function annals applied probability 5411771216 gladyshev e g 1965 stochastic approximations theory probabil ity applications 10275278 hebb 1949 organization behavior wiley new york kohonen 1982 selforganized formation topologically correct fea ture maps biological cybernetics 435969 kohonen barna g chrisley r 1988 statistical pattern recogni tion neural network benchmarking studies proceedings ieee second international conference neural networks volume 1 pages 6168 san diego krasovskii 1963 dynamic continuous selforganizing systems fizmatgiz moscow russian 34 leon bottou kushner h j clark 1978 stochastic approximation con strained unconstrained systems applied math sci 26 springer verlag berlin new york le cun boser b denker j henderson howard r e hubbard w jackel l 1989 backpropagation applied handwritten zip code recognition neural computation 14541551 ljung l soderstrom 1983 theory practice recursive iden tification mit press cambridge macqueen j 1967 methods classification analysis mul tivariate observations lecam l neyman j editors pro ceedings fifth berkeley symposium mathematics statistics probabilities volume 1 pages 281297 berkeley los angeles calif university california press mendel j fu k 1970 adaptive learning pattern recog nition systems theory applications academic press new york metivier 1981 martingale et convergence ps dalgorithmes stochas tiques outils et modeles mathematiques pour lautomatique et le traitement du signal volume 1 pages 529552 editions du cnrs paris france metivier 1983 semimartingales walter de gruyter berlin minsky papert 1969 perceptrons mit press cambridge muller u gunzinger guggenbuhl w 1995 fast neural net sim ulation dsp processor array ieee trans neural networks 61203213 robbins h monro 1951 stochastic approximation model ann math stat 22400407 rosenblatt f1957 theperceptron aperceiving andrecognizingautoma ton technical report 854601 project para cornell aeronautical lab rumelhart e hinton g e williams r j 1986 learning ternal representations error propagation parallel distributed pro cessing explorations microstructure cognition volume pages 318362 bradford books cambridge saad solla 1996 dynamics online gradient descent learning multilayer neural networks touretzky mozer c hasselmo e editors advances neural information online learning stochastic approximations 35 processing systems volume 8 pages 302308 cambridge mit press schumann andretzko r1995 self organizingmaps forvehicle rout ing problems minimizing explicit cost function fogelmansoulie f gallinari p editors proc icann95 int conf artificial neural networks volume ii pages 401406 nanterre france ec2 tsypkin 1971 adaptation learning automatic systems aca demic press new york tsypkin 1973 foundations theory learning systems academic press new york vapnik v n 1982 estimation dependences based empirical data springer series statistics springer verlag berlin new york vapnik v n 1995 nature statistical learning theory springer verlag berlin new york widrow b hoff e 1960 adaptive switching circuits ire wescon conv record part 4 pages 96104 widrow b stearns 1985 adaptive signal processing prentice hall

infoh515 big data analytics streaming analytics gianlucabontempi machinelearninggroup boulevarddetriomphecp212 httpmlgulbacbe human data stream processor humanbeingsperceiveeachinstantoftheirlifethroughanarrayof sensoryobservationsvisualauralnervousetc howeveroverthecourseoftheirlifetheymanagetoabstractandstore inmemoryonlypartoftheobservations humansmayfunctionadequatelyeveniftheycannotrecallevery detailofeachinstantoftheirlives 246 major trends 1 massivedatastreamsbankingandcredittransactionssatellite measurementsastronomicalsurveysinternetsensornetworksdueto realtimemeasurementtechnologieseg wirelesssensornetworks industry40iot 2 streamingnatureofdata impossibilityofstoringallthedataorof processingasamplemorethanonceonepassonly 3 complexanalytics detectingoutliersextremeeventsmonitorcomplex correlationstracktrendspatternrecognitionclassification forecasting 4 advancesinhardware parallelanddistributedarchitectureswith multicoreandcloudcomputingplatformsprovidingaccesstohundreds orthousandsofprocessors 346 challenges multiplepassesondataarenomorepossible onecanprocessadata itematmostonce traditionalapproachofcollectingorganisingstoringandanalysing dataisinadequateforapplicationswherethereactiontoeventsmust beimmediate asynchronousandfastarrivalofstreamelementsrateofarrivalnot undercontrol nonstationarity datadistributionmayevolvewithtime largedimensionalityanddistributednatureofstreamssensor networks adhocqueries arbitraryandnonstandardqueryaboutthestream 446 strategies streaming analytics possiblestrategiestomovefrombatchtostreaminganalytics fromexactsolutiontoapproximationeg useofrandomized algorithms summarizationtechniqueseg samplingwindowingor budgetedstorage iterativeonlineversionsofbatchlearningalgorithms parallelizationtosequentializationeg leastsquares stateestimationtechniques dataarenotmeaningfulbyitselfbutas observationsofsomeimportanthiddenstate hierarchicalanalysis simpleandfasteranalysisatlowlevelmore dataandmoresophisticatedanalysisathigherlevelsfewdata lambdaarchitecture 546 stateful stream processing 646 online learning online learning goal sequenceofaccuratepredictionsbasedonerrorcorrectionand additionalavailableinformation bothsupervisedincludingfeatureselectionandunsupervised learningtasks supervisedlearningbothforparametricidentificationonlinelearning ofparametersandorstructuralidentificationonlinelearningof modelstructure 846 online learning itdifferentiatesfrombatchlearningsincethetrainingsamplesare treatedinasequentialmannerwithorwithoutrepetition onepass learning firsteffectiveapproachusedtotrainneuralnetworkshebb1949 rosenblatt1957 itrevivesthankstothebigdatasettingandlarge scaletasks duringtheearlydaysitwasmainlybasedonheuristicstodealwith convergenceissues morerecenttheoreticalanalysisledtoadeeper understandingofthealgorithms notethatitisthesequentialtreatmentandtheonepassavailabilityof thetrainingsetwhichdeterminesthebatchonlinenatureandnotthe iterativenatureofthealgorithm 946 online vs batch learning onlinelearningistypicallysimplereasiertograspsinceobtainedasa functionofthelatestestimatestateandthecurrentobservation batchlearningtypicallyfasterforsmalldatasetsbutmoreinefficientfor largenetworksandlargetrainingsets batchlearningmorepronetolocalminima onlinelearningmorenaturalsolutionfordealingwithnonstationarity onlinelearningmoresensitivetothechoiceoftrainingparameterseg learningrate 1046 streaming parameter identification incremental calculations sample mean sampleaveragebatch µ 1 cid80n z n n i1 since µ cid80n 11z iz n cid80n 11z in1 z n n1µ n1 z n n n n1 n n n n sequentialerrorcorrectionformulations 1 µ µ z µ µ α z µ n n1 n n n1 n1 n1 n n1 notethatα 1n 0 n n 1246 incremental calculations sample variance samplevariance batchformulation σ2 1 cid80n z µ 2 sn cid16 1 cid80n z2cid17 cid16 1 cid80n zcid172 n n i1 n n n i1 n i1 sequentialformulation σ2 nwhere n n z µ z µ n n1 n n1 n n nn1µ µ 2 n n1 n n1 recursiveformulasforweightedmeansandvariancesexiststoolook attfincharticleinthecoursewebpage 1346 parameter identification learning corestepinsupervisedlearning solutionofamultivariatenonlinearoptimizationproblem costfunctiontominimizeistrainingerror cid80n hxθ2 j θ i1 n n outcome setoftunedparameterseg neuralnetworkweights θˆargminj θ n θ batchlearning optimizationiscarriedoutwithrespecttotheentire trainingsetsimultaneously closedformsolutionsvsiterativesolutions 1446 online linear learning batch linear leastsquares theleastsquaresestimatorβˆ minimizesthecostfunction n βˆargmincid88 xtb2 argmincid0 yxbtyxbcid1 b b i1 inthelinearcaseitexistsaclosedformsolution βˆxtx1xty wherethextxmatrixisasymmetricppmatrix inthemrclasswesawhowtodistributethecomputationoftheclosed formsolution inwhatfollowweseehowtosolveitinastreaming fashion 1646 recursive linear leastsquares letusconsideramultiplelinearregressionproblemandasequenceof inputoutputsamplesxy wherex rp andy r v α v v x1 v 1t 1 x tx vt x ttv 1 x t1 e βˆ βˆt 1x βˆ αt 1 e whereβˆ istheestimateaftertobservations typicalinitialization βˆ isusuallyputequaltoazerovector 0 v aiwitha0andiidentitymatrixsincev representsthe 0 0 varianceoftheestimator bysettingaequaltoalargenumbertherls algorithmwilldivergeveryrapidlyfromtheinitializationβˆ 0 1746 rls forgetting factor considertwosituations thephenomenonunderlyingthedataislinearbutnonstationary thephenomenonunderlyingthedataisstationaryandnonlinearbut canbeapproximatedbyalinearmodellocallyintime solution assignhigherweightstomorerecentdataandforgetolder data linearcase rlstechniqueswithforgettingfactorν 1 1846 rls forgetting factor ii v α vν1 tcid16 xv tt1 v 1t 1 x tx vt x ttv 1 x t1cid17 e βˆ βˆt 1x βˆ αt 1 e thesmallerνthehighertheforgetting notethatforν 1wegobacktotheconventionalrlsformulation 1946 online nonlinear learning batch parametric identification regression genericnonlinearbatchlearnerwithnoclosedformminimizerofthecost functionj θ n let θˆ θˆd argminmcid91 ise θargminj θ n n emp n θλ θλ bethesetofparameterswhichminimizetheempiricalriskortraining error cid80n lzθ cid80n hxθ2 j θ i1 i1 n n n 2146 iterative algorithms first order approximation iterativealgorithmbuildsasequenceθˆ suchthatj θˆ j θˆ n n t1 letusconsidertheiteration θˆ θˆ α j θˆ t1 θ n t1 j θˆisthegradientofthecostfunctionα 0isthe θ n learningrateandaisapositivedefinitematrix whentherecursionconvergesthegradientwillbenullsoitwill eventuallyconvergetoaminimumofthecostfunctionpossiblyalocal minimum ifai therecursionisthesteepestdescentalgorithm n nbonlineiterativebutiterativecid54online 2246 batch gradient descent algorithm sincej θ cid80n i1lziθ forai n n n θˆ θˆ α j θˆθˆ α 1 cid88 lzθˆ t1 θ n t1 n θ t1 i1 computationateachstepoftheaverageofthegradientsoftheloss functionovertheentiretrainingset ifthelearningrateuserdesignedunlikerlsissmallenoughthe algorithmconvergestowardsalocalminimumofj n convergencespeedupisachievedbyreplacingthelearningratebya suitabledefinitepositivematrix notethatthiscouldbeeasilydistributedinamapreduceform becauseofthesummationform 2346 online gradient descent algorithm thisisasimplemodificationofthebatchversionobtainedbydropping theaveragingoperationandtakingatrandomasamplez n θˆ θˆ α lzθˆ t1 θ t1 assumption estimationerrorduetoreplacingtheaveragewithasingle termdoesnotperturbtheaveragebehaviour adaptiveonlinesettingwherenotrainingsetneedstobestoredand observationsareprocessedimmediatelytoimproveperformance 2446 learning rate verydifficulttoset iftoosmall updatewillbeveryslowanditwilltakeverylongtimeto achieveanacceptableloss iftoolarge theparameterwillmovealloverthesearchspaceandmay neverachieveacceptablelossatall highdimensionalnonconvextaskscouldleadtodifferentsensitivity oneachdimension toosmallinsomedimensionandcouldbetoo largeinanotherdimension hardtosetaprioridifferentlearningrates 2546 ada grad adagradadaptivelyscalesthelearningrateforeachdimension 1 θˆ θˆ α g cid12 t1 cid113 cid80t g2 cid15 τ1 τ wherecid12denotesthecomponentwiseproductandg lzθˆ θ t1 betterforsparsedatasinceitdecreasesthelearningratefasterfor frequentlychangedparametersandslowerforparametersinfrequently changed drawback effectivelearningratedecreasingtoofastbecauseofthe accumulationofthegradients nomoreadaptationsincelearningrate isalmostzero 2646 rmsprop theupdateis 1 θˆ θˆ α g cid12 t1 cid112 cid15 wheres βs 1βg2 t1 adamadaptivemomentestimation β 1β g 1 t1 1 β 1β g2 2 t1 2 theupdatebecomes 1 θˆ θˆ α cid12 t1 cid112 cid15 wheremisthemomentumtermstakingintoaccountprevious gradientsandallowingmovingacrossflatspotsofthesearchspace 2746 perceptron rule binaryclassificationtaskclass1or1 perceptronadaptstheparametersofthethresholdclassifier ˆysgnxtθ onlywhenamisclassificationoccursandaccordingtotherule θˆ θˆ α cid0 sgnxtθˆ cid1 x t1 t1 soifˆy noupdatetakesplace otherwiseifamisclassification occursthefollowingruleapplies θˆ θˆ 2α yx t1 costfunctionreachesitsminimalvaluezerowhenallexamplesare properlyrecognizedorwhenthevectorθˆ isnull ifthetrainingsetislinearlyseparablethealgorithmfindsalinear separationwithprobabilityone 2846 online kmeans trainingsetofnpointsx rni1n kmeansisaclusteringalgorithmwhichforafixedk0returnsthe coordinatesθ rn ofkcentroidssuchthatthecostfunction k cid80n lxθisminimizedwhere i1 k lxθminxθ 2 xθ2 k k1 andθ theclosestcentroidtox wecanapproximatethederivativeofthelosswiththederivativeofthe distancetotheclosestcentroid eachtimeanewsamplearrivesthecentroidwhichistheclosesttothe newsamplehasitscoordinatesupdated θˆ θˆ α x θˆ t1 t1 2946 streaming structural identification streaming structural identification modelselectionandvalidationistypicallyperformedinabatchmode byuseofcrossvalidationtechniques bruteforceofflineapproach estimatethegeneralizationaccuracyof thealternativesbymeansofasufficientlylargenumberoftrainingand testphases thecandidatewiththesmallestestimatederrorisselected samecomputationalresourcesareallocatedtoeachmodel configuration manifestlypoorconfigurationsarethoroughlytestedto thesameextentasthebestonesare 3146 racing algorithms racing6evaluatesasetofmodelcandidatesinastreamingfashion assoonasstatisticalevidenceisgatheredagainstsomecandidates thoseareeliminatedandtheracecontinueswiththesurvivingones racingusesonetrainingsampleatthetimeandgeneratingasequence ofnestedsetsofcandidates statisticalteststodiscardalternativesthataresignificantlyworse hoeffdingtestnonparametricandnonblockingandfriedmantest nonparametricandblocking3 3246 hoeffding formula letusconsideracontinuousrandomvariablezzsuchthatthesizeof theintervaldomainzissmallerthanb giventheexpectationµez andthesamplemeanµˆ cid80n z itcanbeshownthat i1 probµˆµcid152exp2ncid152b2 ifweset cid114 b2log2δ δ 2exp2ncid152b2 cid15 2n wehavewithconfidence1δ cid114 b2log2δ µˆµ 2n wehaveseenbeforehowthequantitiesµˆ maybeupdatedsequentially 3346 hoeffding race from6 3446 nonstationary evolving environments nonstationarity adatageneratingprocessisstationaryiftheprobabilisticprocess underlyingthegenerationofdatadoesnotdependontime inpracticea timevaryinggeneratingprocessiscommoninmanysettingslike finance energy climate web environmentalmonitoring sensornetworks malwarespamfrauds recommendationsystems causes seasonalityperiodicityeffects changesinusershabitsorpreferenceschangesinoperatingregimes faults naturalevolutiondrifts agingeffects hiddenstate 3646 timeseries vs time varying atimeseriesisnotnecessarilytimevarying forfixedvaluesofaandbtheprocess yt1aytbyt1cid15 generatesatimeserieswhichistimeinvariant ifaandbchangewithtimetheseriesbecomesnonstationary 3746 nonstationarity assumptionofidenticaldistributionintrainingandtestisthen unrealistic existenceofdrifts probabilitydistributionchangeswithtime nonadaptivemodelissuboptimalandboundtobecomeobsolete evidentimportanceoflearninginnonstationaryenvironments twomainapproaches4 activemethodsrelyonanexplicitdetectionofthechangeinthedata distribution passive continuouslyupdatethemodelovertimewithoutexplicitdetection activemethodspreferableinabruptchanges activemethodssensitivetothefalsepositivefalsealarmratio passivemethodssuitableingradualdriftsandrecurringconcepts settings 3846 passive approaches continuousupdatingthemodelwithoutexplicitlyknowingwhethera conceptdriftoccurredornot neitherapriorinorderivedinformationavailableaboutconceptdrift strategies singlelearner onlinelearningforgettingstrategies ensemblelearning forinstancecid80m w h xwheretheweightsofthe m1 aggregationareinverselyproportionaltotheerrorofthecomponent instanceife isvalidationerrorofthemthmodelwecouldset 1 w em cid80m 1 m1 em theymanagethedriftbyaddingorremovingclassifiers5 racingstrategies 3946 passive approaches issues noexplicitdecisionthennoriskoffalsealarmslikeinactive stabilityplasticitydilemma tradeoffbetweenstabilityie retainingexistingandstillrelevantknowledgeandplasticityie learningnewknowledge 4046 active approaches theytakeadvantageofaprioriinformationorautomaticsystemsable todetectchangesineitherinputorconditionaldistributions useoffeaturesmarginalandconditionalassumedtobestationaryin stationaryconfigurations detectandreactapproach onceanexplicitdetectionofthechangeis doneobsoleteknowledgeisdiscardedandanadaptationmechanism isactivated riskoffalsepositivesandfalsenegatives statisticaltools sequentialhypothesistests example changedetectiontests 4146 detect react 4246 change detection tests designedtooperateinafullysequentialmanner reducedcomputationalcomplexity typicallyrelyingonthresholdshoeffdingboundvalidationerror difficulttosetthethresholdatdesigntimetoolowvaluesinducemany falsepositiveswhiletoolargevaluesinducefalsenegatives 4346 adaptation detection threemainstrategies windowing sampleswithinarecentwindowareusedtoretrainthe classifierwhereasolderonesarediscarded weighting allsamplesaretakenintoconsiderationbutsuitably weightedaccordingtorecencyorrelevancy biasedreservoirsampling eg reservoirsamplingguaranteeinga uniformsamplingalsoforverylongstreams howeveracompletely unbiasedsampleisundesirablesincethedatastreammayevolveand thevastmajorityofthepointsinthesamplemayrepresenttheold historyofthestream biasedreservoirsamplingtechniqueshavebeen proposedin2 4446 charucaggarwal datastreams modelsandalgorithmsadvancesindatabase systems springerverlagnewyorkincsecaucusnjusa2006 charucaggarwal onbiasedreservoirsamplinginthepresenceofstreamevolution inproceedingsofthe32ndinternationalconferenceonverylarge databasesvldb06pages607618vldbendowment2006 maurobirattarithomasstützleluispaqueteandklausvarrentrapp aracingalgorithmforconfiguringmetaheuristics inproceedingsofthegeneticandevolutionarycomputation conferencegecco02pages1118sanfranciscocausa2002 morgankaufmannpublishersinc gditzlermrovericalippiandrpolikar learninginnonstationaryenvironments asurvey ieeecomputationalintelligencemagazine1041225nov2015 relwellandrpolikar 4546 incrementallearningofconceptdriftinnonstationaryenvironments ieeetransactionsonneuralnetworks221015171531oct2011 omaronandamoore theracingalgorithm modelselectionforlazylearners artificialintelligencereview11151932251997 4646

infoh515 big data analytics recommendation engines gianlucabontempi machinelearninggroup boulevarddetriomphecp212 httpmlgulbacbe recommender systems goal supportusersinonlinedecisionmakingproductsmovies news provideeasilyaccessiblehighqualityandpersonalized recommendationtoalargeusercommunity physicalvsonlineretailers physicalretailerscanhaveonlyasubsetofproductsbutshowallofthem onlineretailersmaykeepalltheproductsbutshowonlyapartofthem needofrecommendationsystems issuesrelatedtovolumeandvelocityofdata needofusermodeloruserprofileimplicitvsexplicitapproach communityrelatedinformation issuesofsecurityandconfidentiality 247 disclaimer muchofthefollowingmaterialis takenfromthebook1 347 main approaches 3mainapproachesandtheirhybridcombination 1 collaborative 2 contentbased 3 knowledgebased twomainentities 1 users 2 productsoritemsforwhichusershavepreferences 447 collaborative filtering idea ifuserssharedthesameinterestsinthepasttheywilldoitalso inthefuture itreliesonlyon pastuserbehavioreg previoustransactionsorproductratings relationshipsbetweenusersandinterdependenciesamongproducts largelyusedandsuccessfulapproachonlineretailsales issues howtofinduserswithsimilartastesandwhatdoesitmeansimilar whataboutusersforwhichnobuyinghistoryisavailable whataboutrecentitemsnotboughtyet 547 collaborative filtering procons contentagnostic noneedofitemdescription largenumberofbenchmarkdatasets coldstartissue 647 collaborative filtering trainingset amatrixofuserproductratings query auserandanotyetratedproduct outputs numericalpredictionoftheuserratingofaproductorlistof mostrelevantproductsforauser 3typesofcf 1 userbasednearestneighborrecommendation givenaratingdatabaseand theidofthecurrentuseridentifyotheruserswithsimilarpreferencesinthe past useneighborsratingsforpredictingtheratingofthetargetproduct 2 itembasednearestneighborrecommendation identifysimilarproductsand usetheratingsofthesameuserforsimilarproducts 3 latentfactormodelsbasedonfactorization itaimstoexplaintheratingsby characterizingbothitemsandusersonacertainnumberkoffactorsinferred fromtheratingspatterns factorsmaybeinterpretablecomedyvsactionfor kidsvsadultsornot 747 ratings utility matrix givenasetofn usersandan productsweneedamatrixnot u p necessarilyfullrofsizen n ofratings u p giventheratinghistoryofaliceand4userswewishtopredictherrating foritem5 wecanuseinformationaboutuserstorecommendproductsandabout productstorecommendusers 847 challenges nearestneighbor approaches volumeofdatamillionsofusersanditems similaritymetric jaccarddistanceignoretheratingsvaluepearson correlationspearmanrankcosinesimilarityeuclideandistances howtoconsidermissingvaluesinthedistance isamissingsample informative shouldratingbenormalizedeg subtractingtheaveragerateofeach user weightingofitems varianceeg givemoreimportancetomore controversialonessignificanceeg takeintoaccountthenumberof userratings lackofsymmetrybetweenitemsandusers itemstypicallybelongtoa specificclustereg agenrewhileusersmayhaveseveralclustersof interesteg hardrockandclassicalmusic numberofneighbours preprocessing precomputingofneighbourhoodmatrices 947 challenges nearestneighbor approaches clusteringofusers ofitems impliciteg browsingsearchpurchaseclickinghistoryvsexplicit ratingseg netflix granularityofratingsscale coldstartproblem nonstationarity memorybasedvsmodelbased 1047 remarks ideallygiventheratingmatrixthegoalwouldbetopredictallthe blanks inpracticeitisnotreallynecessarytopredicteveryblankentry itisonlynecessarytodiscoversomeentriesthatarelikelytobehigh sometimesitisenoughtofindalargesubsetofblankentrieswiththe highestratings 1147 matrix factorization successfulapproachinthe1mnetflixcompetition ratingmatrixastheproductoftwolongandthinmatrices theyderiveasetoflatenthiddenfactorsfromtheratingmatrixand characterizebothusersanditemswithsuchfactors notethatthosefactorsarenotnecessarilyinterpretable accordingtothesvdtheoremthen n ratingmatrixcanbe u p decomposedasfollows ruσvt whereun kandvn karetheleftandtherightsingularvectorsand u p thediagonalofσkkarecalledsingularvalues byretainingonlythemostimportantsingularvalueseg k2wecan associatetoeachuserandeachitemasmallvectorofvalues eachquerypointhastobeprojectedinthecompressedspacebefore beingusedforprediction goodcompromisebetweenscalabilityandaccuracy 1247 example book ratingrmatrixwithn 7andn 5 twomainconceptsintherating u p scheme sciencefictionandromancemovies2 1347 matrixuconnectspeopletoconceptsboyslikesciencefictionand matrixvconnectsitemstoconceptsthefirstthreemoviesbelongtothe conceptone matrixσgivesthestrengthoftheconceptrelatedtothe numberofexamples 1447 svd r11 r12 r1np u11 u1k s11 0 v11 v1np r2 1 r2 2 r2np u2 1 r2k 0 0 v2 1 v2np rnu1 rnu2 rnunp unu1 unuk 0 skk vk1 vknp matrixuencodesuserswhilematrixvencodesitems eachuserandeachitemisrepresentedinakdimensionalspace supposewehaveanewuserrepresentedbyaratingvectorxofsize 1n andwewanttosituateitinthelatentcompressedspace wecan p doitbythetransformation yxvς1 whichreturnsavectoryofsize1k 1547 challenges matrix factorization assessmentofqualityofdecompositioneg whichcostfunctionto measurethedistancebetweenranduσv computationalcomplexityofsvddecompositionison2n n3isnot p u u manageableinverylargedimensionalspaces iterativeminimizationprocess sparseness conventionalsvdisundefinedwhenknowledgeaboutthe matrixisincomplete costofimputationriskofadverseimpactofimpreciseimputation overfitting 1647 latent factors model bothusersanditemsaremappedtoajointlatentfactorspaceof dimensionalityksuchthatuseriteminteractionsaremodeledas innerproduct rupt eachitemjisassociatedtothevectorp rk andeachuseritothe j vectoru rk suchthat r upt ij j letubethen kusermatrixandpthen kitemmatrix u p optimalvalueofthesematricesminimizesthequantity cid88cid16 cid17 cid88cid16 cid17 r upt2λcid107ucid1072cid107pcid1072 e2 λcid107ucid1072cid107pcid1072 ij j j ij j ij ij wherethesumislimitedtoobservedr termsandtheλparameteracts ij asaregularizationtermie toavoidoverfitting 1747 latent factors model fitting notethattheobjectivecostisnonconvexbecauseoftheupttermand j thennphardtominimize twosolutionsoftheminimizationproblem 1 alternatingleastsquares itworksbyiterativelysolvingaseriesof leastsquaresregressionproblems ineachiterationoneoftheuseror theitemmatricesistreatedasfixedwhiletheotheroneisupdated usingthefixedmatrixinputandtheratingdataoutput thisprocess continuesuntilithasconverged alsispowerfulachievesgood performancecanbemassivelyparallelized 2 stochasticgradientdescent recommandations performapreprocessingnormalizationstepeg firstremoveaverage userratingandthenaverageitemrating runseveraliterationswithdifferentinitialconditionsandaveragethe results 1847 r n un p un uk pt k n p rt n pn u pn pk utkn u 1947 multioutput least squares letusconsideramultiinputmultioutputormultitargetregression problemwherenisthenumberofobservationspisthenumberof inputsandmisthenumberofoutputs inmatrixform yxbe whereyisthenmoutputmatrixxisthenpinputmatrixeisthe errorsmatrixnmandbistheparametermatrixofsizepm ifwemakethehypothesisofuncorrelatederrorstheleastsquares estimationhasthesameformasthesingleoutputcase bˆ xtx1xty 2047 mimo multiinput multioutput leastsquares yn1 xnp b p 1 ynm xnp b p inbothcasesbˆ xtx1xty 2147 u rt n pn u pn pk utkn u p j r n un p un uk pt k n p 2247 als matrix form initializeu andp 0 0 repeatuntilconvergence 1 rt p ut yrt n n xp n kandbut kn p u p u ut bˆ xtx1xtypt p 1pt rt t1 ismultioutputlssolutionwheren isthenumberofoutputsn isthe u p numberofexamplesandkn isthenumberofparameters u 2 rupt yrn n xu n kandbpt kn u p u t1 p pt bˆ xtx1xtyut ut 1u r t1 ismultioutputlssolutionwheren isthenumberofoutputsn isthe p u numberofexampleandkn isthenumberofparameters p 2347 als pseudocode initializep andu 0 0 repeat fori1n u ut cid80 ptp1cid80 r pt j1np j j j1npwij1 ij j end forj1n p pt cid80 utu1cid80 r ut j i1nu i1nuwij1 ij end untilconvergence r istheratingofitemjmadebyuseri ij u 1kistheithrowi1n ofu u p 1kisthejthrowj1n ofp j p w 1meansthattheithuserhasratedthejthproduct ij 2447 computational cost thematrixtobeinvertedhassizekkandthecomputationalcostof theinversionisok3 updateeachu costsonkk3wheren isthenumberofitemsrated byuseri updateeachp costsonkk3wheren isthenumberofusersrating j j j itemj oncepanduarecomputedpredictalluseritemratingscosts n k3 u p 2547 als distributed computation therearethreemethodstodistributethealscomputation3 join thisisinspiredtothemethodalreadyintroducedtodistributeina mapreducefashiontheleastsquarescomputationincaseoflow numberofinputs thisisthecaseheresincekn kn note p u alsothatthemultioutputleastsquarescanbeeasilyparallelized becauseoftheindependenceofeachestimation broadcast itmakestheassumptionthatuandparesmallenoughto bestoredlocallyoneachmachine block itisablockversionofthebroadcastmethodwhereonlyapart oftheuandpmatricesaresenttoeachworker theseportionsare determinedbyprecomputingthesubsetofusersaddressedbyeach workerandtherelateditems 2647 u rt n pn u pn pk utkn u p j r n un p un uk pt k n p 2747 als distributed computation broadcast 1 partitionrrowwisesuchthatthesameuserisonthesamemachine letuscallitr 1 2 partitionrt suchthatthesameitemisonthesamemachine letuscall itr 2 3 broadcastuandp 4 giventhelocalavailabilityofponeachworkermapr suchtocompute 1 foreachusertheupdate cid88np cid88np ut ptp1 r pt i1n j j ij j u j1 j1 5 giventhelocalavailabilityofuoneachworkermapr suchto 2 computeforeachproducttheupdate cid88nu cid88nu pt utu1 r ut j1n j ij p i1 i1 2847 stochastic gradient descent bycomputingthegradientofthecostfunctionweobtainthe gradientbasediterativeformulas cid40 ut1 utγe ptλut ij j pt1 ptγe utλpt j j ij j wheretistheiterationindexandet r utptt ij ij j ifweassumethatthefactormatricesuandpfitinmemorywecan implementitinastreamingfashionwhereratingsdatar arrive ij sequentially effectiveinnonstationarysettingsproductperceptionandpopularity maychangeastimepassesby 2947 learned lessons form netflix competition in2006netflixofferedaprizeof1000000tothefirstpersonorteam tobeattheirownrecommendationalgorithmcalledcinematchby10 afterover3yearstheprizewasawardedinseptember2009 trainingdatasetwasarating15starsmatrixofhalfamillionuserson 17000movies thisdatawasselectedfromalargerdatasetand proposedalgorithmsweretestedontheirabilitytopredicttheratings inasecretremainderofthelargerdataset theuvdecompositionalgorithmwasfoundbythreestudentsandgave a7improvementovercinematchwhencoupledwithnormalization andafewothertricks thewinningentrywasactuallyacombinationofseveraldifferent algorithmsthathadbeendevelopedindependently genreandimdbinformationappearedtobeuseless timeofratingturnedouttobeuseful blockbustersareratedfaster thanothers 3047 content based filtering itreliesontheavailabilityofmanuallyorautomaticallyextracted descriptionofitemseg topicgenreactordirectorsandofauser profileeg sexagethatgivesimportancetosuchitems issues howacquireandimproveuserprofiles howtodeterminethematchbetweenitemanduserprofiles howtoautomaticallyextracttheitemdescription itdoesnotrequirelargeusergroups itemscanberecommendedindependently hardtodescribequalitativefeatures needofmanualandtimeconsumingannotation 3147 example item profile iftheitemisamovietheprofilecouldbecomposedof setofactors director year genreeg fromtheimdbdatabase 3247 extracting content unstructured data documents itispossibletoextractfeaturesrelatedtowordstatistics eg tfidfscores images availabilityofusertags 3347 nearest neighbour approach supposewewanttorateanewitem likeincollaborativefilteringcfalsoincontentbasedapproachwe mayadoptanearestneighbourapproach incftheneighborhoodisdeterminedbytheratinghistoryofmany users twoitemseg moviesaresimilarifmanyusersratedthemin similarmanner heretheneighborhooddependsonanumberofdescriptivefeatures eg sametopicauthor othergeneralpurposeclassificationsystemsmaybeusedaswelleg createaclassificationmodelforeachuserorclassofusersreturning thedegreeofinterestforagivenitemprofile useoflshfunctionsforhugedatasetstodefinebucketsofsimilar itemsusers 3447 knowledge based recommendation contentbasedandcollaborativefilteringmaketheassumptionthatwe oftenbuytheproductbookormovierental nowthisisnotalways thecasehousecar inthiscasethenumberofratingsistoosmall itaddressesonetimebuyerscommoninconsumerelectronics nopurchasehistory preferencesandtasteseg carcomputerevolvewithtime interactionwithuser whicharethedesiredcharacteristicsprice fromfilteringapproachtoconversationalapproach rsisnotafilter butmoreapersonalizedguide issues howtoacquireuserprofileknowledgeindomainswithnopurchasehistory whatisthemostadequateinteractionpattern sometimeuserswantto definetheirownrequirementspricelowerthan noneedofuserhistory 3547 knowledge based recommendation inkbtheusermustspecifytherequirementsandthesystemtriesto identifyasolutionandprovideexplanations ifnosolutioncanbefound theusermustchangetherequirements twotypesofkbapproaches constraintbased relyonanexplicitlydefinedsetofrecommendation rulesandattacktherecommendationasaconstraintsatisfaction problem implementedasafilteringoperationonrelationaltable casebased focusontheretrievalofsimilaritemswithapredefined thresholdonthebasisofdifferenttypesofsimilaritymeasures notion ofdistanceproductrequirement givenasetrdescribingallthe customerrequirementsitispossibletodefineforeachproductpa similarity cid80 w srp srp r cid80r r w rr r wherew istheimportanceweightoftherequirementrandsrpis r thesimilaritybetweentheproductpandthatspecificrequirementr 3647 vs less better givenarequirementrexpressedasarangeandtherelatedvalueofthe productφ pthesimilaritydependsonthenatureoftherelation r formibmoreisbetterpropertieseg energysavings φ pminr srp r maxrminr forliblowerisbetterpropertieseg price maxrφ p srp r maxrminr forcibcloserisbettereg sizeofascreen maxrφ p srp1 r maxrminr 3747 interaction issues needofgraphicaleg webbasedinteraction interactionpattern 1 usersfirstspecifytheirpreferencesinaformorbyapreferenceelicitation process 2 userispresentedwithasetofmatchingitems 3 userrevisetherequirements issues noadequateitemalternativeproposalsuseofdefaults adaptationtousersearchlearningoninteractionlogsselectionof nextquestionsspaceexplorationuseofinformationofrelated customerseg mostpopularprice 3847 hybrid approaches 3947 hybrid approaches rationale allpreviousmethodshaveprosandcons whynotto combinethem strategiesforhybridizationdesign monolithic asinglealgorithmicimplementationthatincorporatesallthe availablefeaturesfeatureaugmentation parallelized alternativerecommendersystemsoperateindependentlyand produceseparaterecommendationlistswhicharecombinedintoafinalset pipelined theoutputofarecommenderisusedasinputofasubsequent one 4047 interesting issues adversarialattacks privacyaspects fairness theseaspectsarecommontomanysimilarbigdataapplications1 4147 attacking recommendation systems recommendationsystemshasanimportantbusinessimpact whatif usersarenothonestandwanttoinfluencethebuyingbehavioreg damageacompetitorsabotagethesystem fakeuseraccountsmaybecreatedtobeascloseaspossibletoother profilesinordertoinfluencetheirrecommendation pushincreasetherecommendationofanitemvsnukedecreasethe recommendationattacks randomattackaverageattackbandwagonie highratingsfor blockbustersproductssotohavemanyneighborssegmentattack targetingaspecificsegmentofthepopulationeg fansof sciencefictionlovehateattackratesomeitemattheminimumvalue whileratingtheotheritemsatthehighestpossiblevalues automatedcrawlerthatsimulateswebbrowsingsessionswiththegoal ofassociatingatargetitemthepagetobepushedwithotherpagesin suchawaythatthetargetitemsappearonrecommendationlistsmore often 4247 attacking rs countermeasures usingmodelbasedorhybridtechniquesinsteadofmemorybased techniquestoreducevulnerabilitytoalterationpoisoningofthe trainingset useofrsthatdonotrelyonlyonratingprofiles useoftrustmeasurestoattributeweightstodifferentprofiles increasethecostofinjectingnewprofilesuseofcaptchalimitations basedonipaddresses automatedattackdetectiondetectingshillingbehavioreg clusterof usersratinginshorttimeinsuspiciouslysimilarmanner detectionfakeprofilesinunsupervisedbasedonagreementwith otherusersdegreeofsimilaritywithtopneighborsorratingdeviation fromaverageorsupervisedlabelingofgenuineprofilesmanner risk offalsepositives monitoringratingtimeevolution 4347 privacy aspects ratingdatabasesofcollaborativefilteringrecommendersystems containdetailedinformationabouttheindividualtastesand preferencesoftheirusers sensitiveandvaluablealsoinmonetarytermsinformation needtoavoidleakstopreservecustomersconfidenceandtrust cf cannotexistwithoutuserswillingtorate naivearchitecturesinglecentralserverstoringplainunobfuscated andnonencryptedratingsprovidesacentraltargetpointofattack fewreportofattacksonrealworldsystemsaremadepublictoavoid economicbacklash 4447 privacy countermeasures twomainstrategies datadistribution avoidstoringtheinformationinacentralplaceeg useofp2parchitecturesinwhicheverynodemaintainsitsinformation andcommunicatesonlyondemand dataperturbation dataobfuscationbyapplyingrandomdata perturbationeg addrandomnoiseortransformquantitativerating intoconcordancesinsuchamannerthattheoutcomeoftheanalytics eg computeaveragestatisticsorscalarproductsisrobustwrt perturbation tradeoffobfuscationaccuracy problemswith perturbationsizenotadequateforalltheratings combinationofthetwostrategiesarerecommendedtoo 4547 fairness fromthearticlefairnessinrankingsandrecommendations anoverview socialmediahasbecomethemainsourceofonlinenewswithmore than24billioninternetusers searchandrecommendationplayacentralroleinshapingexperiences andinfluencingtheperceptionoftheworld issues suggestionsloopsfilterbubbleamplificationofexistingbias andreducingdiversity arecentstudyshowedthatfemaleartistsonlyrepresented25ofthe musiclistenedbyusers genderfairnessisoneoftheartistsmain concernsasfemaleartistsarenotgivenequalexposureinmusic recommendations astudyrevealedthattherecommendationsystemdisproportionately showedcertaintypesofjobadstomeneg engineersandwomen eg cleaningladies thesystemwasmorelikelytopresentjobadsto usersiftheirgenderidentityreflectedtheconcentrationofthatgender inaparticularpositionorindustry 4647 djannachmzankerafelfernigandgfriedrich recommendersystems cambridge2012 anandrajaramanjureleskovecandjeffreydullman miningmassivedatasets 2014 rezazadeh matrixcompletionviaalternatingleastsquareals technicalreportjanuary2015 4747

cover feature matrix factorization techniques recommender systems yehuda koren yahoo research robert bell chris volinsky att labsresearch netflix prize competition dem onstrated matrix factorization models systems particularly useful entertainment superior classic nearestneighbor products movies music tv shows many cus tomers view movie customer techniques producing product recom likely view numerous different movies customers mendations allowing incorporation proven willing indicate level satisfaction additional information implicit particular movies huge volume data available feedback temporal effects confidence movies appeal customers com levels panies analyze data recommend movies particular customers odern consumers inundated recommender system strategies choices electronic retailers content broadly speaking recommender systems based providers offer huge selection prod one two strategies content filtering approach ucts unprecedented opportunities creates profile user product characterize meet variety special needs nature example movie profile could include tastes matching consumers appropriate tributes regarding genre participating actors products key enhancing user satisfaction loy box office popularity forth user profiles might alty therefore retailers become interested include demographic information answers provided recommender systems analyze patterns user suitable questionnaire profiles allow programs interest products provide personalized recommenda associate users matching products course tions suit users taste good personalized contentbased strategies require gathering external infor recommendations add another dimension user mation might available easy collect experience ecommerce leaders like amazoncom known successful realization content filtering netflix made recommender systems salient part music genome project used internet websites radio service pandoracom trained music analyst scores 42 computer published ieee computer society 00189162092600 2009 ieee song music genome project 3 based hundreds distinct musical characteristics attributes genes capture songs musical identity also many significant qualities 2 relevant understanding listeners musi cal preferences alternative content filtering relies past user behaviorfor example previous transactions product ratings without requiring creation explicit 1 profiles approach known col laborative filtering term coined joe developers tapestry first recom mender system1 collaborative filtering analyzes relationships users 4 interdependencies among products identify new useritem associations major appeal collaborative fil tering domain free yet address data aspects often elusive figure 1 useroriented neighborhood method joe likes three difficult profile using content filter movies left make prediction system finds similar ing generally accurate users also liked movies determines movies liked case three liked saving private ryan first contentbased techniques collaborative recommendation two liked dune next filtering suffers called cold start problem due inability ad dress systems new products users aspect welldefined dimensions depth character de content filtering superior velopment quirkiness completely uninterpretable two primary areas collaborative filtering dimensions users factor measures much neighborhood methods latent factor models neighbor user likes movies score high correspond hood methods centered computing relationships ing movie factor items alternatively users item figure 2 illustrates idea simplified example oriented approach evaluates users preference two dimensions consider two hypothetical dimen item based ratings neighboring items sions characterized female versus maleoriented user products neighbors products tend serious versus escapist figure shows several get similar ratings rated user wellknown movies fictitious users might fall example consider movie saving private ryan two dimensions model users predicted neighbors might include war movies spielberg movies rating movie relative movies average rating tom hanks movies among others predict par would equal dot product movies users lo ticular users rating saving private ryan would look cations graph example would expect gus movies nearest neighbors user actually love dumb dumber hate color purple rated figure 1 illustrates useroriented approach rate braveheart average note mov identifies likeminded users complement iesfor example oceans 11and usersfor example others ratings davewould characterized fairly neutral latent factor models alternative approach two dimensions tries explain ratings characterizing items users say 20 100 factors inferred matrix factorization methods ratings patterns sense factors comprise successful realizations latent factor computerized alternative aforementioned human models based matrix factorization basic form created song genes movies discovered factors matrix factorization characterizes items users might measure obvious dimensions comedy versus vectors factors inferred item rating patterns high drama amount action orientation children less correspondence item user factors leads 43 august 2009 cover feature serious vector q f user u associ braveheart ated vector p f given item color purple amadeus elements qu measure extent item possesses factors positive negative given user u elements p measure extent u lethal weapon interest user items high sense corresponding factors posi geared sensibility oceans 11 geared tive negative resulting dot product toward toward females males qt p captures interaction user u u item ithe users overall interest dave items characteristics approximates lion king user us rating item denoted dumb r leading estimate dumber ui princess independence diaries day rˆ qt p 1 ui u gus escapist major challenge computing map ping item user factor vectors figure 2 simplified illustration latent factor approach q p f recommender system characterizes users movies using two axesmale versus female u completes mapping easily esti serious versus escapist mate rating user give item using equation 1 recommendation methods become popular model closely related singular value decom recent years combining good scalability predictive position svd wellestablished technique identifying accuracy addition offer much flexibility model latent semantic factors information retrieval applying ing various reallife situations svd collaborative filtering domain requires factoring recommender systems rely different types useritem rating matrix often raises difficulties input data often placed matrix one due high portion missing values caused sparse dimension representing users dimension ness useritem ratings matrix conventional svd representing items interest convenient data undefined knowledge matrix incom highquality explicit feedback includes explicit plete moreover carelessly addressing relatively input users regarding interest products known entries highly prone overfitting example netflix collects star ratings movies tivo earlier systems relied imputation fill missing users indicate preferences tv shows pressing ratings make rating matrix dense2 however im thumbsup thumbsdown buttons refer explicit putation expensive significantly increases user feedback ratings usually explicit feedback com amount data addition inaccurate imputation prises sparse matrix since single user likely might distort data considerably hence recent rated small percentage possible items works36 suggested modeling directly observed rat one strength matrix factorization allows ings avoiding overfitting regularized incorporation additional information explicit model learn factor vectors p q system u feedback available recommender systems infer minimizes regularized squared error set user preferences using implicit feedback indirectly known ratings reflects opinion observing user behavior including pur chase history browsing history search patterns even min r qtp2 λ q 2 p 2 2 mouse movements implicit feedback usually denotes qp uiκ ui u u presence absence event typically repre κ set ui pairs r known ui sented densely filled matrix training set system learns model fitting previously basic matrix factorization model observed ratings however goal generalize matrix factorization models map users items previous ratings way predicts future unknown joint latent factor space dimensionality f ratings thus system avoid overfitting useritem interactions modeled inner products observed data regularizing learned parameters space accordingly item associated whose magnitudes penalized constant λ controls 44 computer extent regularization usually determined data aspects applicationspecific requirements crossvalidation ruslan salakhutdinov andriy mnihs requires accommodations equation 1 staying probabilistic matrix factorization7 offers probabilistic within learning framework equation 1 tries cap foundation regularization ture interactions users items produce different rating values however much observed learning algorithms variation rating values due effects associated two approaches minimizing equation 2 stochastic either users items known biases intercepts indepen gradient descent alternating least squares als dent interactions example typical collaborative filtering data exhibits large systematic tendencies stochastic gradient descent users give higher ratings others items simon funk popularized stochastic gradient descent receive higher ratings others products optimization equation 2 httpsifterorgsimon widely perceived better worse others journal20061211html wherein algorithm loops thus would unwise explain full rating value ratings training set given interaction form qtp instead system tries u training case system predicts r computes identify portion values individual user ui associated prediction error item biases explain subjecting true interaction portion data factor modeling firstorder approxi def e r qt p mation bias involved rating r follows ui ui u ui modifies parameters magnitude pro b µ b b 3 ui u portional g opposite direction gradient yielding bias involved rating r denoted b ac ui ui counts user item effects overall average qiqigeuipuλqi rating denoted µ parameters b b indicate u pupugeuiqiλpu observed deviations user u item respectively average example suppose want popular approach46 combines implementation firstorder estimate user joes rating movie ease relatively fast running time yet cases titanic say average rating movies µ beneficial use als optimization 37 stars furthermore titanic better average movie tends rated 05 stars average alternating least squares hand joe critical user tends rate q p unknowns equation 2 03 stars lower average thus estimate u convex however fix one unknowns op titanics rating joe would 39 stars 37 05 03 timization problem becomes quadratic solved biases extend equation 1 follows optimally thus als techniques rotate fixing qs fixing ps ps fixed system rˆ µ b b qtp 4 u u ui u u recomputes qs solving leastsquares problem vice versa ensures step decreases equation observed rating broken four 2 convergence8 components global average item bias user bias user general stochastic gradient descent easier item interaction allows component explain faster als als favorable least two cases part signal relevant system learns first system use parallelization als minimizing squared error function45 system computes q independently item factors computes p independently min r µ b b ptq2 λ user factors gives rise u potentially massive pqb uiκ ui u u parallelization algorithm9 second case p 2 q 2 b2 b2 5 u u systems centered implicit data training set cannot considered sparse looping single since biases tend capture much observed training caseas gradient descent doeswould signal accurate modeling vital hence works practical als efficiently handle cases10 offer elaborate bias models11 adding biases additional input sources one benefit matrix factorization approach col often system must deal cold start problem laborative filtering flexibility dealing various wherein many users supply ratings making 45 august 2009 cover feature difficult reach general conclusions taste way prove accuracy decomposing ratings distinct terms relieve problem incorporate additional sources allows system treat different temporal aspects sepa information users recommender systems rately specifically following terms vary time item use implicit feedback gain insight user preferences biases bt user biases bt user preferences p u u indeed gather behavioral information regardless first temporal effect addresses fact users willingness provide explicit ratings items popularity might change time example tailer use customers purchases browsing history movies go popularity triggered learn tendencies addition ratings external events actors appearance new customers might supply movie therefore models treat item bias b simplicity consider case boolean implicit function time second temporal effect allows users feedback nu denotes set items user u change baseline ratings time example expressed implicit preference way system user tended rate average movie 4 stars might profiles users items implicitly preferred rate movie 3 stars might reflect several new set item factors necessary item factors including natural drift users rating scale associated x f accordingly user fact users assign ratings relative recent showed preference items nu characterized ratings fact raters identity within house vector hold change time hence models parameter b function time u xi temporal dynamics go beyond also affect inu user preferences therefore interaction normalizing sum often beneficial example users items users change preferences time working example fan psychological thrillers genre might become fan crime dramas year later simi nu05 xi45 larly humans change perception certain actors inu directors model accounts effect taking another information source known user attributes user factors vector p function time u example demographics simplicity consider hand specifies static item characteristics q boolean attributes user u corresponds set unlike humans items static nature attributes au describe gender age group exact parameterizations timevarying parameters11 zip code income level distinct factor vector lead replacing equation 4 dynamic prediction f corresponds attribute describe user rule rating time set userassociated attributes rˆ µ bt bt qt p 7 ui u u ya aau inputs varying confidence levels matrix factorization model integrate several setups observed ratings deserve signal sources enhanced user representation weight confidence example massive adver tising might influence votes certain items rˆ µ b b qt p nu05 x 6 aptly reflect longerterm characteristics similarly ui u u inu aau system might face adversarial users try tilt rat previous examples deal enhancing user ings certain items representationwhere lack data common another example systems built around implicit items get similar treatment necessary feedback systems interpret ongoing user behavior users exact preference level hard temporal dynamics quantify thus system works cruder binary far presented models static real representation stating either probably likes product ity product perception popularity constantly change probably interested product cases new selections emerge similarly customers inclina valuable attach confidence scores esti tions evolve leading redefine taste thus mated preferences confidence stem available system account temporal effects numerical values describe frequency actions flecting dynamic timedrifting nature useritem example much time user watched certain interactions show frequently user bought certain item matrix factorization approach lends well numerical values indicate confidence obser modeling temporal effects significantly im vation various factors nothing user 46 computer preferences might cause onetime 15 event however recurring event likely reflect user opinion matrix factorization model 10 readily accept varying confidence levels let give less weight less meaningful observations con 05 fidence observing r denoted ui c model enhances cost ui 00 function equation 5 account confidence follows 05 min c r µ b b pqb uiκ ui ui u p utq i2 λ p u 2 q 2 10 b2 b2 8 u information reallife ap 15 plication involving schemes 15 10 05 00 05 10 refer collaborative filtering factor vector 1 implicit feedback datasets10 netflix prize competition 2006 online dvd rental company netflix announced con test improve state recommender system12 winning entries consist 100 differ enable company released training set ent predictor sets majority factorization 100 million ratings spanning 500000 anony models using variants methods described mous customers ratings 17000 discussions top teams postings movies movie rated scale 1 5 stars public contest forum indicate popu participating teams submit predicted ratings test set lar successful methods predicting ratings approximately 3 million ratings netflix calculates factorizing netflix usermovie matrix allows us rootmean square error rmse based heldout discover descriptive dimensions predict truth first team improve netflix algo ing movie preferences identify first rithms rmse performance 10 percent wins important dimensions matrix decomposition 1 million prize team reaches 10 percent goal explore movies location new space figure 3 netflix gives 50000 progress prize team first shows first two factors netflix data matrix place year competition factorization movies placed according factor contest created buzz within collaborative fil vectors someone familiar movies shown see tering field point publicly available data clear meaning latent factors first factor vector collaborative filtering research orders magni xaxis one side lowbrow comedies horror tude smaller release data competitions movies aimed male adolescent audience half baked allure spurred burst energy activity according freddy vs jason side contains drama contest website wwwnetflixprizecom comedy serious undertones strong female leads 48000 teams 182 different countries sophies choice moonstruck second factorization loaded data axis yaxis independent critically acclaimed quirky teams entry originally called bellkor took films punchdrunk love heart huckabees top top spot competition summer 2007 bottom mainstream formulaic films armaged 2007 progress prize best score runaway bride interesting intersections time 843 percent better netflix later aligned boundaries top left corner team big chaos win 2008 progress prize indie meets lowbrow kill bill natural born kill score 946 percent time writing still ers arty movies play violent themes first place inching toward 10 percent landmark bottom right serious femaledriven movies meet 47 august 2009 2 rotcev rotcaf freddy g fo rt e f di dn g sr r e h j oad tl af hd eb htna fr ei k alpe stod n ag nen sa ttt hu ark e ra al r fll uc abb ri ia gol rl ef un da c j sv c aku e til ol li l n e1 n cmd oaio ynn h ok tee ea r u pb gh luo yuy nc ck ha tb hd te r e e hus men w aik ir z dl iryo nv l e b ot mfe ie aonn n rzge h ujn ntob ata h wu n nb al em ylom ls sa betl ri ik dn e ev si jr tc oa eh un pr cs sil imt sa tt zi ee mo r nn ak ctane tt hh ta ee hs n eo n wwi p ae lh oi uh e na n nwsl l st e c r oh u sf wco ei ek amrc se ue osi nc 1 figure 3 first two vectors matrix decomposition netflix prize data selected movies placed appropriate spot based factor vectors two dimensions plot reveals distinct genres including clusters movies strong female leads fraternity humor quirky independent films cover feature 091 40 atrix factoriza 60 tion techniques 0905 90 become 128 50 180 dominant meth 100 odology within 200 09 collaborative filtering recom menders experience 0895 50 datasets netflix prize 100 data shown deliver 200 accuracy superior classical 089 nearestneighbor techniques time offer com 0885 pact memoryefficient model 100 200 500 50 systems learn relatively 100 200 500 1000 easily makes tech 088 1500 niques even convenient models integrate natu 0875 rally many crucial aspects 10 100 1000 10000 100000 data multiple forms millions parameters feedback temporal dynamics confidence levels references 1 goldberg et al using col laborative filtering weave information tapestry comm acm vol 35 1992 pp 6170 mainstream crowdpleasers sound music 2 bm sarwar et al application dimensionality reduc smack middle appealing types tion recommender systema case study proc kdd workshop web mining ecommerce challenges wizard oz opportunities webkdd acm press 2000 plot movies neighboring one another typi 3 funk netflix update try home dec 2006 cally would put together example annie hall httpsifterorgsimonjournal20061211html citizen kane next although 4 koren factorization meets neighborhood mul stylistically different lot common tifaceted collaborative filtering model proc 14th acm highly regarded classic movies famous directors sigkdd intl conf knowledge discovery data mining indeed third dimension factorization end acm press 2008 pp 426434 5 paterek improving regularized singular value de separating two composition collaborative filtering proc kdd cup tried many different implementations pa workshop acm press 2007 pp 3942 rameterizations factorization figure 4 shows 6 g takács et al major components gravity recom different models numbers parameters affect mendation system sigkdd explorations vol 9 2007 pp rmse well performance factorizations 8084 evolving implementationsplain factorization adding 7 r salakhutdinov mnih probabilistic matrix fac biases enhancing user profile implicit feedback torization proc advances neural information processing systems 20 nips 07 acm press 2008 pp 12571264 two variants adding temporal components accuracy 8 r bell koren scalable collaborative filtering factor models improves increasing jointly derived neighborhood interpolation weights proc number involved parameters equivalent ieee intl conf data mining icdm 07 ieee cs press 2007 increasing factor models dimensionality denoted pp 4352 numbers charts 9 zhou et al largescale parallel collaborative filter complex factor models whose descriptions ing netflix prize proc 4th intl conf algorithmic involve distinct sets parameters accu aspects information management lncs 5034 springer 2008 pp 337348 rate fact temporal components particularly 10 yf hu koren c volinsky collaborative filtering important model significant temporal ef implicit feedback datasets proc ieee intl conf data fects data mining icdm 08 ieee cs press 2008 pp 263272 48 computer esmr plain biases implicit feedback temporal dynamics v1 temporal dynamics v2 figure 4 matrix factorization models accuracy plots show rootmeansquare error four individual factor models lower better accuracy improves factor models dimensionality denoted numbers charts increases addition refined factor models whose descriptions involve distinct sets parameters accurate comparison netflix system achieves rmse 09514 dataset grand prizes required accuracy rmse 08563 11 koren collaborative filtering temporal dynam robert bell principal member technical staff ics proc 15th acm sigkdd intl conf knowledge discovery att labsresearch research interests survey data mining kdd 09 acm press 2009 pp 447455 research methods statistical learning methods 12 j bennet lanning netflix prize kdd cup ceived phd statistics stanford university bell workshop 2007 wwwnetflixprizecom member american statistical association institute mathematical statistics contact rbell researchattcom yehuda koren senior research scientist yahoo chris volinsky director statistics research att search haifa research interests recommender labsresearch research interests largescale data systems information visualization received mining social networks models fraud detection phd computer science weizmann institute received phd statistics university wash science koren member acm contact ington volinsky member american statistical yehudayahooinccom association contact volinskyresearchattcom 49 august 2009

scalability cost frankmcsherry michaelisard derekgmurray unaffiliated unaffiliated unaffiliated abstract 50 offer new metric big data platforms cost 10 configuration outperforms single thread thecostofagivenplatformforagivenproblemisthe hardwareconfigurationrequiredbeforetheplatformout 1 performs competent singlethreaded implementation 1 10 100 300 cores cost weighs systems scalability headsintroducedbythesystemandindicatestheactual performancegainsofthesystemwithoutrewardingsys temsthatbringsubstantialbutparallelizableoverheads survey measurements dataparallel systemsre cently reported sosp osdi find many systems either surprisingly large cost often hundreds cores simply underperform one thread foralloftheirreportedconfigurations 1 introduction youcanhaveasecondcomputeronceyouve shownyouknowhowtousethefirstone paulbarham thepublishedworkonbigdatasystemshasfetishized scalabilityasthemostimportantfeatureofadistributed data processing platform nearly publi cations detail systems impressive scalability directlyevaluatetheirabsoluteperformanceagainstrea sonable benchmarks degree systems trulyimprovingperformanceasopposedtoparallelizing overheadsthattheythemselvesintroduce contrary common wisdom effective scal ing evidence solid systems building system canscalearbitrarilywellwithasufficientlackofcarein implementation two scaling curves figure 1 present scaling naiad computation sys temaandaftersystembaperformanceoptimization applied optimization removes paralleliz ableoverheadsdamagestheapparentscalabilitydespite resultinginimprovedperformanceinallconfigurations michaelisardwasemployedbymicrosoftresearchatthetimeof hisinvolvementbutisnowunaffiliated derekgmurraywasunaffiliatedatthetimeofhisinvolvement butisnowemployedbygoogleinc pudeeps 1000 system system b 100 8 1 10 100 300 cores sdnoces system system b figure 1 scaling performance measurements dataparallel algorithm system system b simple performance optimization theunoptimizedimplementationscalesfarbetter despiteorratherbecauseofitspoorperformance whilethismayappeartobeacontrivedexamplewewill arguethatmanypublishedbigdatasystemsmoreclosely resemblesystemathantheyresemblesystemb 11 methodology inthispaperwetakeseveralrecentgraphprocessingpa pers systems literature compare ported performance simple singlethreaded im plementations datasets using highend 2014 laptop perhaps surprisingly many published sys temshaveunboundedcostienoconfigurationout performs best singlethreaded implementationfor alloftheproblemstowhichtheyhavebeenapplied comparisons neither perfect always fair conclusions sufficiently dramatic concern must raised cases single threadedimplementationsaremorethananorderofmag nitude faster published results systems using hundreds cores identify reasons gaps someareintrinsictothedomainsomeareentirelyavoid ableandothersaregoodsubjectsforfurtherresearch westressthattheseproblemslienotnecessarilywith systems may improved time rather measurements authors provide standard reviewers readers de mand hope shed light issue future research directed toward distributed systems whosescalabilitycomesfromadvancesinsystemdesign ratherthanpoorbaselinesandlowexpectations 1 name twitter rv13 uk20070556 scalablesystem cores twitter uk200705 nodes 41652230 105896555 graphchi12 2 3160s 6972s edges 1468365182 3738733648 stratosphere8 16 2250s size 576gb 1472gb xstream21 16 1488s spark10 128 857s 1759s giraph10 128 596s 1235s table1 thetwitter rvanduk200705graphs graphlab10 128 249s 833s graphx10 128 419s 462s fn pagerank20graph graphiterator alpha f32 let mut vec0f32 graphnodes singlethreadssd 1 300s 651s let mut b vec0f32 graphnodes singlethreadram 1 275s let mut vec0f32 graphnodes graphmap_edgesx dx 1 table2 reportedelapsedtimesfor20pagerankit iter 020 0graphnodes erations compared measured times single bi alpha ai di threadedimplementationsfromssdandfromram ai 1f32 alpha graphchi xstream report times 5 page rankiterationswhichwemultipliedbyfour graphmap_edgesx ay bx fn labelpropagationgraph graphiterator let mut label 0graphnodesto_vec let mut done false figure2 twentypagerankiterations done done true graphmap_edgesx 2 basic graph computations labelx labely done false labelx minlabelx labely labely minlabelx labely graph computation featured prominently recent sospandosdiconferences andrepresentsoneofthe simplest classes dataparallel computation triviallyparallelized convenientlygonzalezetal10 evaluatedthelatestversionsofseveralgraphprocessing figure3 labelpropagation systemsin2014 weimplementeachoftheirtasksusing singlethreaded c code evaluate implementa tionsonthesamedatasetstheyuseseetable11 table 2 compares reported times several singlethreaded implementations use simple systems singlethreaded implementations boostlikegraphtraversalpatternagraphiterator pagerank reading data either ssd typeacceptsactionsonedgesandmapstheactionacross ram graphchi xstream allgraphedges theimplementationusesunbufferedio readedgedatafromdiskallsystemspartitionthegraph read binary edge data ssd maintains per data among machines load memory nodestateinmemorybackedbylargepages2mb thangraphlabandgraphxsystemspartitionedgesby sourcevertexgraphlabandgraphxusemoresophisti 21 pagerank catedpartitioningschemestoreducecommunication scalable system table 2 consistently pagerankisancomputationondirectedgraphswhichit performs single thread even single thread erativelyupdatesarankmaintainedforeachvertex19 repeatedlyrereadsthedatafromexternalstorage iteration vertexs rank uniformly divided graphlab graphx outperform singlethreaded amongitsoutgoingneighborsandthensettobetheac executions although see section 31 cumulation scaled rank incoming neighbors singlethreaded implementation outperforms sys dampeningfactoralphaisappliedtotheranksthelost temsonceitreordersedgesinamannerakintothepar rank distributed uniformly among nodes figure 2 titioningschemesthesesystemsuse presentscodefortwentypagerankiterations 22 connectedcomponents 1ourcimplementationsrequiredsomemanualinliningandare lesstersethanourrustimplementations intheinterestofclaritywe connected components undirected graph presentthelatterinthispaperbothversionsofthecodeproducecom parableresultsandwillbemadeavailableonline disjointsetsofverticessuchthatallverticeswithinaset 2 scalablesystem cores twitter uk200705 scalablesystem cores twitter uk200705 stratosphere8 16 950s graphlab 128 249s 833s xstream21 16 1159s graphx 128 419s 462s spark10 128 1784s 8000s vertexorderssd 1 300s 651s giraph10 128 200s 8000s vertexorderram 1 275s graphlab10 128 242s 714s hilbertorderssd 1 242s 256s graphx10 128 251s 800s hilbertorderram 1 110s singlethreadssd 1 153s 417s table4 reportedelapsedtimesfor20pagerankit table 3 reported elapsed times label propa erations compared measured times single gation compared measured times single threadedimplementationsfromssdandfromram threadedlabelpropagationfromssd singlethreaded times use identical algorithms butwithdifferentedgeorders aremutuallyreachablefromeachother inthedistributedsettingthemostcommonalgorithm worker enables systems exchange less computing connectivity label propagation 11 data910 figure3 inlabelpropagationeachvertexmaintainsa singlethreaded graph algorithm perform labelinitiallyitsownidanditerativelyupdatesitsla explicit communication edge ordering beltobetheminimumofallitsneighborslabelsandits pronounced effect cache behavior example currentlabel theprocesspropagatesthesmallestlabel theedgeorderingdescribedbyahilbertcurve2akin ineachcomponenttoallverticesinthecomponentand ordering edges ab interleaving bits iteration converges happens every com b exhibits locality b rather ponent updates commutative associative vertex ordering table 4 compares andconsequentlyadmitascalableimplementation7 running times singlethreaded pagerank edges table 3 compares reported running times la presentedinhilbertcurveorderagainstotherimplemen bel propagation several dataparallel systems tationswhereweseethatitimprovesoverallofthem singlethreaded implementation reading ssd de convertingthegraphdatatoahilbertcurveorderisan spite using orders magnitude less hardware single additionalcostinpreprocessingthegraph theprocess threadedlabelpropagationissignificantlyfasterthanany amountstotransformingpairsofnodeidentifiersedges systemabove intoanintegeroftwiceasmanybitssortingthesevalues transforming back pairs node identifiers 3 better baselines implementation transforms twitter rv graph 179 seconds using one thread perfor mancewinevenifpreprocessingiscountedagainstthe thesinglethreadedimplementationswehavepresented runningtime werechosentobethesimplestmostdirectimplementa tionswecouldthinkof thereareseveralstandardways improve yielding singlethreaded implementa 32 improvingalgorithms tions strictly dominate reported performance ofthesystemswehaveconsideredinsomecasesbyan theproblemofproperlychoosingagoodalgorithmlies additionalorderofmagnitude attheheartofcomputerscience thelabelpropagation algorithm used graph connectivity isagoodalgorithmbutbecauseitfitswithinthethink 31 improvinggraphlayout like vertex computational model 15 whose imple oursinglethreadedalgorithmstakeasinputsedgeitera mentations scale well unfortunately case torsandwhiletheyhavenorequirementsontheorderin manyotherstheappealingscalingpropertiesarelargely whichedgesarepresented theorderdoesaffectperfor duetothealgorithmssuboptimality labelpropagation mance uptothispointoursinglethreadedimplemen simplydoesmoreworkthanbetteralgorithms tationshaveenumeratededgesinvertexorderwhereby consider algorithmic alternative unionfind edges one vertex presented moving weighted union 3 simple omlogn algorithm next vertex graphlab graphx whichscansthegraphedgesonceandmaintainstwoin steadpartitiontheedgesamongworkerswithoutrequir tegers graph vertex presented figure 4 ingthatalledgesfromasinglevertexbelongtothesame table 5 reports performance compared imple 3 scalablesystem cores twitter uk200705 20 graphlab 128 242s 714s 10 graphx 128 251s 800s singlethreadssd 1 153s 417s unionfindssd 1 15s 30s 1 16 100 512 cores table5 timesforvariousconnectivityalgorithms fn unionfindgraph graphiterator let mut root 0graphnodesto_vec let mut rank 0u8 graphnodes graphmap_edgesmut x mut x rootx x rootx rooty rooty x match rankxcmpranky less rootx greater rooty x equal rooty x rankx 1 figure4 unionfindwithweightedunion mentations label propagation faster fastest ofthemthesinglethreadedimplementationbyoveran orderofmagnitude therearemanyotherefficientalgorithmsforcomput ing graph connectivity several paralleliz abledespitenotfittinginthethinklikeavertexmodel algorithms may best fit given distributed system still legitimate alternativesthatmustbeconsidered 4 applying cost prior work developed singlethreaded implementations basis evaluating cost systems asanexerciseweretrospectivelyapplythesebaselines tothepublishednumbersforexistingscalablesystems 41 pagerank figure 5 presents published scaling information powergraph 9 graphx 10 naiad 16 well astwosinglethreadedmeasurementsashorizontallines intersection upper line indicates point system outperforms simple resource constrained implementation suitable baseline forsystemswithsimilarlimitationseggraphchiand xstreamtheintersectionwiththelowerlineindicates thepointatwhichthesystemoutperformsafeaturerich implementation including preprocessing sufficient sdnoces vertex ssd 460 graphlab hilbert ram 100 naiad 50 64 100 512 cores sdnoces graphx vertex ssd hilbert ram cores figure5 publishedscalingmeasurementsforpage rank twitter rv first plot time per warmiteration thesecondplotisthetimefortenit erationsfromacoldstarthorizontallinesaresingle threadedmeasurements 1000 100 5 1 10 100 300 cores sdnoces naiad uf slow naiad uf union find figure6 twonaiadimplementationsofunionfind memoryandisasuitablebaselineforsystemswithsim ilarresourceseggraphlabnaiadandgraphx curves would say naiad costof16coresforpagerankingthetwitter rvgraph although presented part scaling data graphlabreportsa36smeasurementon512coresand achieves cost 512 cores graphx tersect corresponding singlethreaded measurement andwewouldsayithasunboundedcost 42 graphconnectivity thepublishedworksdonothavescalinginformationfor graph connectivity given absolute performance label propagation scalable systems relative singlethreaded unionfind optimistic suchscalingdatawouldhaveleadtoaboundedcost insteadfigure6presentsthescalingoftwonaiadim plementations parallel unionfind 14 ex amplesfromfigure1thetwoimplementationsdifferin theirstorageofpervertexstatethesloweroneuseshash tables faster one uses arrays faster im plementation cost 10 cores slower implementationhasacostofroughly100cores use hash tables root cause factor oftenincreaseincostbutitdoesprovidesomevalue nodeidentifiersneednotlieinacompactsetofintegers evaluation makes tradeoff clearer sys temimplementorsandpotentialusers 4 5 lessons learned provide qualitative advantages including integra tionwithanexistingecosystemhighavailabilityorse several aspects scalable systems design imple curity simpler solution cannot provide sec mentationcontributetooverheadsandincreasedcost tion 4 demonstrates nonetheless important eval computational model presented system uatethecostbothtoexplainwhetherahighcostis stricts programs one may express target hard intrinsictotheproposedsystemandbecauseitcanhigh ware may reflect different tradeoffs perhaps favoring lightavoidableinefficienciesandtherebyleadtoperfor capacity throughput high clock frequency fi manceimprovementsforthesystem nally implementation system may add heads single thread doesnt require understanding eachoftheseoverheadsisanimportantpartofassessing 6 future directions area thecapabilitiesandcontributionsofascalablesystem achieve scalable parallelism big data systems note may appear critical research dis strictprogramstomodelsinwhichtheparallelismisev tributed systems believe still good work ident thesemodelsmaynotalignwiththeintentofthe doandourgoalistoprovideaframeworkformeasuring programmer efficient parallel implementa andmakingthebestforwardprogress tions problem hand mapreduce intention numerous examples scalable algorithms ally precludes memoryresident state interest computational models one needs look scalabilityleadingtosubstantialoverheadforalgorithms backtotheparallelcomputingresearchofdecadespast thatwouldbenefitfromit pregelsthinklikeavertex boruvkas algorithm 1 nearly ninety years old par modelrequiresagraphcomputationtobecastasaniter allelizes cleanly solves general problem atedlocalcomputationateachgraphvertexasafunction label propagation bulk synchronous parallel state neighbors captures lim model24issurprisinglymoregeneralthanmostrelated itedsubsetofefficientgraphalgorithmsneitherofthese worksectionswouldhaveyoubelieve thesealgorithms designsarethewrongchoicebutitisimportanttodis models richly detailed analyzed many tinguishscalabilityfromefficientuseofresources casesalreadyimplemented cluster computing environment different manyexamplesofperformantscalablesystemsexist environment laptop former often values bothgalois17andligra23aresharedmemorysys high capacity throughput latency slower temsthatsignificantlyoutperformtheirdistributedpeers cores storage memory laptop embod run single machines naiad 16 introduces iesthepersonalcomputerwithlowercapacitybutfaster new general purpose dataflow model outperforms coresstorageandmemory whilescalablesystemsare evenspecializedsystemsunderstandingwhatthesesys oftenagoodmatchtoclusterresourcesitisimportantto temsdidrightandhowtoimprovethemismoreimpor consideralternativehardwareforpeakperformance tantthanrehashingexistingideasinnewdomainscom finally implementation system may intro paredagainstonlythepoorestofpriorwork duceoverheadsthatconcealtheperformancebenefitsof wearenowstartingtoseeperformancestudiesofthe scalable system highlevel languages may facilitate currentcropofscalablesystems18 challengingsome developmentbuttheycanintroduceperformanceissues conventionalwisdomunderlyingtheirdesignprinciples garbagecollection boundschecks memorycopies similar studies come previous genera especially common research setting evaluate tions systems 22 including work explicitly critical new idea partial primitive implementations oftheabsoluteperformanceofscalablesystemsascom otherpartsofthesystemserializationmemorymanage pared simpler solutions 20 4 25 mentnetworkingassertingthatexistingtechniqueswill surelyvaluabletounderstandandlearnfromtheperfor improve performance many issues manceofpopularscalablesystemswemightalsolearn mightbeimprovedwithengineeringeffortthatdoesnot thatwekeepmakingandpublishingthesamemistakes otherwise advance research nonetheless fundamentallyapartofgoodresearchismakingsure difficulttoassesswhetherthebenefitsthesystemclaims weareaskingtherightquestions cansystemsbemade willstillmanifestoncethefatisremoved toscalewellistriviallyansweredintheintroduction many good reasons system might andisnotitselftherightquestion thereisasubstantial high cost compared fastest amountofgoodresearchtodobutidentifyingprogress purposebuilt singlethreaded implementation sys requires upfront existing alternatives temmaytargetadifferentsetofproblemsbesuitedfor cost scalable system uses simplest al adifferentdeploymentorbeaprototypedesignedtoas ternativesbutisanimportantpartofunderstandingand sesscomponentsofafullsystem thesystemmayalso articulatingprogressmadebyresearchonthesesystems 5 references 20 alexanderrasmussen georgeporter michaelconley harsha vmadhyastharadhikaniranjanmysorealexanderpucherand 1 httpenwikipediaorgwikiboruvkas_ aminvahdattritonsort abalancedlargescalesortingsys algorithm temnsdi2011 2 httpenwikipediaorgwikihilbert_curve 21 amitabha roy ivo mihailovic willy zwaenepoel x stream edgecentricgraphprocessingusingstreamingparti 3 httpenwikipediaorgwikiunion_find tionssosp2013 22 mehulashahsamuelmaddenmichaeljfranklinandjoseph 4 ericandersonandjosephtucekefficiencymattershotstorage mhellersteinjavasupportfordataintensivesystemsexperi 2009 encesbuildingthetelegraphdataflowsystemsigmodrecord 5 paoloboldiandsebastianovignathewebgraphframeworki 2001 compressiontechniqueswww2004 23 julianshunandguyblellochligraalightweightgraphpro cessingframeworkforsharedmemoryppopp2013 6 paoloboldibrunocodenottimassimosantiniandsebastiano vignaubicrawler ascalablefullydistributedwebcrawler 24 leslie g valiant bridging model parallel computation softwarepracticeexperience2004 communicationsoftheacmvolume33issue8aug1990 7 austin clements frans kaashoek nickolai zeldovich 25 cezhangandchristopherredimmwitted astudyofmain roberttmorrisandeddiekohnlerthescalablecommutativ memorystatisticalanalyticsvldb2014 ityrule designingscalablesoftwareformulticoreprocessors sosp2013 8 stephanewenmoritzkaufmannkostastzoumasandvolker marklspinningfastiterativedataflowsvldb2012 9 josephegonzalez yuchenglow haijiegu dannybickson carlosguestrinpowergraph distributedgraphparallelcom putationonnaturalgraphsosdi2012 10 joseph e gonzalez reynold xin ankur dave daniel crankshaw michael j franklin ion stoica graphx graphprocessinginadistributeddataflowframeworkosdi 2014 11 ukang charalamposetsourakakis andchristosfaloutsos pegasusminingpetascalegraphsicdm2009 12 aapo kyrola guy blelloch carlos guestrin graphchi largescalegraphcomputationonjustapcosdi2012 13 haewoonkwakchanghyunleehosungparkandsuemoon whatistwitterasocialnetworkoranewsmediawww2010 14 silvio lattanzi benjamin moseley siddharth suri sergei vassilvitskiifilteringamethodforsolvinggraphproblemsin mapreducespaa2011 15 grzegorzmalewiczmatthewhausternaartjcbikjames c dehnert ilan horn naty leiser grzegorz czajkowski pregel asystemforlargescalegraphprocessingsigmod 2010 16 derekgmurrayfrankmcsherryrebeccaisaacsmichaelis ardpaulbarhamandmartınabadinaiad atimelydataflow systemsosp2013 17 donald nguyen andrew lenharth keshav pingali lightweightinfrastructureforgraphanalyticssosp2013 18 kayousterhoutryanrastisylviaratnasamyscottshenker andbyunggonchunmakingsenseofperformanceindataan alyticsframeworksnsdi2015 19 lawrencepagesergeybrinrajeevmotwaniandterrywino gradthepagerankcitationrankingbringingordertotheweb technicalreportstanforddigitallibrarytechnologiesproject 1998 6

incremental calculation weighted mean variance tony finch hfanf2camacuki hdotdotatati university cambridge computing service february 2009 abstract notes explain derive formulae numerically stable calculation mean andstandarddeviationwhicharealsosuitableforincrementalonlinecalculation ithengeneralize formulae weighted means standard deviations unpick difficulties arise generalizing normalized weights finally show exponentially weighted moving averageisaspecialcaseoftheincrementalnormalizedweightedmeanformulaandderiveaformula exponentially weighted moving standard deviation 1 simple mean straightforward translation equation 1 code suffer loss precision difference magnitude sample sum samples equation 4 calculates mean way numerically stable avoids accumulating large sums n 1 x µ x 1 n n i1 n1 1 x x x 2 n n i1 1 x n1µ 3 n n n1 1 µ x µ 4 n1 n n n1 formula also provides us useful identities x µ nµ µ 5 n n1 n n1 x µ nµ µ µ µ n n n n1 n n1 n1µ µ 6 n n1 2 simple variance definition standard deviation equation 7 requires us already know mean implies two passes data isnt feasible online algorithms need produce incremental results sample becomes available equation 12 solves problem since allows us calculate standard deviation two running sums n 1 x σ2 x µ2 7 n i1 n 1 x x22x µµ2 8 n i1 1 n n n 1 x 1 x 1 x x22µ x µ2 1 9 n n n i1 i1 i1 n 1 x n x22µµµ2 10 n n i1 n 1 x x2µ2 11 n i1 n n 2 1 x 1 x x2 x 12 n n i1 i1 3 incremental variance knuth notes 1 equation 12 prone loss precision takes difference two large sums similar size suggests equation 24 alternative avoids problem however say derived following equation 20 derived previous step using equation 5 let nσ2 13 n n pn x µ 2 14 i1 n pn x2nµ2 15 i1 n n n1 x x x2nµ2 x2n1µ2 16 n n1 n n1 i1 i1 x2 nµ2 n1µ2 17 n n n1 x2 µ2 nµ2 µ2 18 n n1 n1 n x2 µ2 nµ µ µ µ 19 n n1 n1 n n1 n x2 µ2 µ x µ µ 20 n n1 n1 n n1 n x2 µ2 µ2 x µ x µ µ µ 21 n n1 n1 n n n n1 n n1 x2 x µ x µ µ µ 22 n n n n n1 n n1 x µ x µ 23 n n1 n n x µ x µ 24 n n1 n n1 n n p σ n 25 n n mathworld 2 alternative derivation similar formula notation follows n x x µ 2 26 n n i1 n x x µ µ µ 2 27 n1 n n1 i1 n n n x x x x µ 2 µ µ 22 x µ µ µ 28 n1 n n1 n1 n n1 i1 i1 i1 simplify first summation n n1 x x x µ 2 x µ 2 x µ 2 29 n1 n n1 n1 i1 i1 x µ 2s 30 n n1 n1 n2µ µ 2 31 n1 n n1 2 simplify second summation n x µ µ 2 nµ µ 2 32 n n1 n n1 i1 simplify third summation n n x x x µ µ µ µ µ x µ 33 n1 n n1 n n1 n1 i1 i1 n1 x µ µ x µ x µ 34 n n1 n n1 n1 i1 n1 x µ µ x µ n1µ x 35 n n1 n n1 n1 i1 µ µ x µ n1µ n1µ 36 n n1 n n1 n1 n1 µ µ x µ 37 n n1 n n1 nµ µ 2 38 n n1 back complete formula n2µ µ 2nµ µ 22nµ µ 2 39 n n1 n n1 n n1 n n1 n2µ µ 2nµ µ 2 40 n1 n n1 n n1 nn1µ µ 2 41 n1 n n1 use equations 6 5 show equivalent equation 24 nn1µ µ 2 42 n n1 n n1 nµ µ x µ 43 n1 n n1 n n x µ x µ 44 n1 n n1 n n 4 weighted mean weighted mean defined follows pn w x µ i1 45 pn w i1 equivalent simple mean weights equal since µ pn i1wx wpn i1x 1 xn x 46 pn w nw n i1 i1 samples different weights thought sample frequencies p usedtocalculateprobabilitieswherep w w thefollowingderivationoftheincrementalformula equation 53 follows pattern derivation equation 4 brevity also define w n sum weights n x w w 47 n i1 n 1 x µ w x 48 n w n i1 n1 1 x w x w x 49 w n n n i1 3 1 w x w µ 50 w n n n1 n1 n 1 w x w w µ 51 w n n n n n1 n 1 w µ w x w µ 52 w n n1 n n n n1 n w µ nx µ 53 n1 w n n1 n useful identities derived formula w µ µ w µ x 54 n n1 n n n1 n w nµ µ x µ 55 w n n1 n n1 n w x µ nµ µ µ µ n n w n n1 n n1 n w w n nµ µ 56 w n n1 n w w n nx µ 57 w n n1 n 5 weighted variance similarly derive numerically stable formula calculating weighted variance equation 68 using pattern derivation unweighed wersion equation 24 n n 1 x 1 x σ2 w x µ2 w x2µ2 58 w w n n i1 i1 let w σ2 59 n n n n x w x2w µ2 60 n n i1 n n1 x x w x2w µ2 w x2w µ2 61 n n1 n n n1 n1 i1 i1 w x2 w µ2 w µ2 62 n n n n n1 n1 w x2 w µ2 w w µ2 63 n n n n n n n1 w cid0 x2 µ2 cid1 w µ2 µ2 64 n n n1 n n1 n w cid0 x2 µ2 cid1 w µ µ µ µ 65 n n n1 n n1 n n1 n w cid0 x2 µ2 µ x µ µ cid1 66 n n n1 n1 n n1 n w x µ x µ 67 n n n1 n n w x µ x µ 68 n n1 n n n1 n n p σ w 69 n n n 6 variable weights previous three sections assumed weights constant assigned however common requirement normalize weights n x w w 1 70 i1 repeatedly adding new data working set cant constant weights normalized weights allow us keep weights normalized need allow weight 4 sample change set samples changes indicate give weights two indices first identifying set samples using sample count µ etc n second index sample set make assumptions sum weights require normalized example n n x 1 x w w µ w x 71 n ni n w ni n i1 i1 done need reexamine logical steps previous sections ensure still valid equations 4951 used fact fixedweight setting n1 x w x w µ w w µ 72 n1 n1 n n n1 i1 new setting equality fairly obviously longer true example keeping weights normalized w w 1 fortunately different middle step justifies n n1 equation 72 weights vary results section 4 remain valid n1 x w x w w µ 73 ni n nn n1 i1 n x1 w x n x1 w pn 11w n1ix 74 ni ni pn1w i1 i1 i1 n1i pn1w x pn1w x i1 ni i1 n1i 75 pn1w pn1w i1 ni i1 n1i n1 n1 x w ni x x w n1i x 76 pn1w pn1w i1 i1 ni i1 i1 n1i w w nj n1j 1j n1 77 pn1w pn1w i1 ni i1 n1i says weighted mean formulae remain valid new old weights consistent equation 75 says get result calculate mean previous workingsetwhetherweusetheoldweightsorthenewweights equation77saysthatwhenwenormalize weights across previous set n1 get set weights whether start old weights new ones requirement isnt enough make weighted variance formulae work examine 7 expectation function point worth defining better notation reduce number summations need write expectation function generalized version mean whose argument arbitrary function sample n 1 x e fx w fx 78 n w ni n i1 e k k 79 n e afx ae fx 80 n n e fxgx e fxe gx 81 n n n µ e x 82 n n σ2 e xµ 2 83 n n n e x2µ2 2µ x 84 n n n e x2µ2 2µ e x 85 n n n n 5 e x2µ2 86 n n e x2e x2 87 n n incremental formula derived usual way equation 92 particularly useful n x w e fx w fx 88 n n ni i1 n1 x w fx w fx 89 nn n ni i1 pn1w fx w fx w w i1 ni 90 nn n n nn pn1w i1 ni pn1w fx w fx w w i1 n1i 91 nn n n nn pn1w i1 n1i w fx w w e fx 92 nn n n nn n1 w e fx e fx nn fx e fx 93 n n1 w n n1 n 8 variableweight variance equations 6163 made following assumptions true weights vary n n1 x x w x2w µ2 w x2w µ2 ni n n n1i n1 n1 i1 i1 6 w x2 w µ2 w µ2 nn n n n n1 n1 6 w x2 w µ2 w w µ2 nn n n n n nn n1 try redo short derivation incremental standard deviation formula starting soon get stuck fortunately longer derivation shows made work n n1 w σ2 94 n n n w e cid0 xµ 2cid1 95 n n n w e cid0 xµ µ µ 2cid1 96 n n n1 n n1 w e cid0 xµ 2µ µ 22xµ µ µ cid1 97 n n n1 n n1 n1 n n1 w e cid0 xµ 2cid1 w e cid0 µ µ 2cid1 2w e xµ µ µ 98 n n n1 n n n n1 n n n1 n n1 simplify first term w e cid0 xµ 2cid1 w x µ 2w w e cid0 xµ 2cid1 99 n n n1 nn n n1 n nn n1 n1 w x µ 2w w n1 100 nn n n1 n nn w n1 w w w2 n nns n µ µ 2 101 w n1 w n n1 n1 nn simplify second term w e cid0 µ µ 2cid1 w µ µ 2 102 n n n n1 n n n1 simplify third term w e xµ µ µ n n n1 n n1 µ µ w e xµ 103 n n1 n n n1 6 µ µ w x µ w w e xµ 104 n n1 nn n n1 n nn n1 n1 µ µ w x µ w w e xe µ 105 n n1 nn n n1 n nn n1 n1 n1 µ µ w x µ w w µ µ 106 n n1 nn n n1 n nn n1 n1 µ µ w x µ 107 n n1 nn n n1 w µ µ 2 108 n n n1 back complete formula w w w2 n nns n µ µ 2w µ µ 22w µ µ 2 109 n w n1 w n n1 n n n1 n n n1 n1 nn w w w2 w w n nns n µ µ 2 n nnµ µ 2 110 w n1 w n n1 w n n1 n1 nn nn w w w n nns w w n µ µ 2 111 w n1 n nn w n n1 n1 nn w w n nns w w µ µ x µ 112 w n1 n nn n n1 n n1 n1 w w n nns w x µ x µ 113 n w n1 nn n n n n1 n1 thisisthesameasequation68 exceptforthemultiplier wnwnn whichcapturesthechangeinweights wn1 old new sets w w pn1w w n nn i1 ni nj 1j n1 114 w n1 pn 11w n1i w n1j know rescaling trick makes work write short version w w n nns n w n1 n1 w cid0 e x2µ2cid1 w w cid0 e x2µ2 cid1 115 n n n n nn n1 n1 w cid0 e x2µ2cid1 w e x2w x2 w w µ2 116 n n n n n nn n n nn n1 w x2 w µ2 w w µ2 117 nn n n n n nn n1 w cid0 x2 µ2 cid1 w µ2 µ2 118 nn n n1 n n1 n w cid0 x2 µ2 cid1 w µ µ µ µ 119 nn n n1 n n1 n n1 n w cid0 x2 µ2 µ x µ µ cid1 120 nn n n1 n1 n n1 n w x µ x µ 121 nn n n n n1 9 exponentiallyweighted mean variance starting equation 53 lets set w w constant 0α1 let a1α produces nn n standard formula exponentially weighted moving average µ µ αx µ 122 n n1 n n1 1αµ αx 123 n1 n aµ 1ax 124 n1 n following convenient use lower bound 0 instead 1 ie 0 n going show weights renormalized time datum added first expand inductive definition mean µ aµ 1ax 125 n n1 n a2µ a1ax 1ax 126 n2 n1 n 7 a3µ a21ax a1ax 1ax 127 n3 n2 n1 n n x µ anx ani1ax 128 n 0 i1 allows us write weights directly note w independent n nn w 129 n0 w ani1a 1in 130 ni w 1a α 131 nn since w αw w see n w 1 weights always normalized nn nn n n get result summing geometric series xn n x1 1an ani aj 132 1a i1 j0 n n x x w ani1a1an 133 ni i1 i1 n x w w w an1an1 134 n n0 ni i1 weights satisfy consistency requirement w aw w nj n1j n1j 135 pn1w pn1aw pn1w i1 ni i1 n1i i1 n1i use expectation function write naıve formula variance w e fx e fx nnfx e fx 136 n n1 w n n1 n e fxαfx e fx 137 n1 n n1 e x2 e x2αx2 e x2 138 n n1 n n1 σ2 e x2µ2 139 n n using formula previous section write incremental version w w n nns w x µ x µ 140 n w n1 nn n n n n1 n1 1α αx µ x µ 141 n 1 n1 n n n n1 σ2 n 1ax µ x µ 142 n w n n1 n n n n1 n 1αs αx µ 2 143 n1 n n1 latter form slightly convenient code diff x mean incr alpha diff mean mean incr variance 1 alpha variance diff incr references 1 donald e knuth seminumerical algorithms volume 2 art computer programming chapter 422 page 232 addisonwesley boston third edition 1998 2 eric w weisstein sample variance computation mathworld wolfram web resource httpmathworldwolframcomsamplevariancecomputationhtml 8

chapter 4 mining data streams algorithms described book assume mining database data available want chapterweshallmakeanotherassumption dataarrivesinastreamorstreams andifitisnotprocessedimmediatelyorstoredthenitislostforever moreover shall assume data arrives rapidly feasible store allin activestorageie conventionaldatabaseandthen interactwith time choosing algorithms processing streams involve summarization streaminsomeway weshallstartbyconsideringhowtomakeausefulsample stream filter stream eliminate undesirable elements show estimate number different elements stream using much less storage would required listed elements seen anotherapproachtosummarizingastreamis tolookatonlyafixedlength window consisting last n elements typically large n query window relation database manystreamsandornis largewe maynotbe able storethe entire window every stream need summarize even windows address fundamentalproblemofmaintaininganapproximatecountonthenumberof1s inthewindowofabitstreamwhileusingmuchlessspacethanwouldbeneeded store entire window technique generalizes approximating various kinds sums 41 stream data model let us begin discussing elements streams stream processing explainthe difference betweenstreams anddatabasesandthe specialproblems arise dealing streams typical applications stream model applies examined 131 132 chapter 4 mining data streams adhoc queries streams entering 1 5 2 7 4 0 3 5 standing output streams q w e r u queries 0 1 1 0 1 0 0 0 stream processor time limited working storage archival storage figure 41 datastreammanagementsystem 411 datastreammanagement system analogy databasemanagementsystem view stream processor kind datamanagement system highlevel organization suggested fig 41 number streams enter system stream provide elements schedule need dataratesordatatypesandthetimebetweenelementsofonestreamneednot uniform fact rate arrival stream elements control system distinguishes stream processing processing ofdata goesonwithin databasemanagementsystem latter system controls rate data read disk therefore never worry data getting lost attempts execute queries streams may archived large archival store assume possible answer queries archival store could examined specialcircumstancesusing timeconsuming retrievalprocesses thereis also working store summaries parts streams may placed andwhichcanbe usedforansweringqueries workingstoremightbe disk oritmightbemainmemorydependingonhowfastweneedtoprocessqueries either way sufficiently limited capacity cannot store data streams 41 stream data model 133 412 examples stream sources beforeproceedingletusconsidersomeofthe waysinwhichstreamdataarises naturally sensor data imagine temperature sensor bobbing ocean sending back basestationareadingofthesurfacetemperatureeachhour thedataproduced sensor stream real numbers interesting stream since data rate low would stress modern technology entire stream could kept main memory essentially forever give sensor gps unit let report surface height instead temperature surface height varies quite rapidly compared tempera ture might sensor send back reading every tenth second sends 4byte real number time produces 35 megabytes per day still take time fill main memory let alone single disk one sensor might interesting learn something oceanbehaviorwemightwanttodeployamillionsensorseachsendingbacka stream rate ten per second million sensors isnt many would one every 150 square miles ocean 35 terabytes arriving every day definitely need think kept working storage archived image data satellites often send earth streams consisting many terabytes images per day surveillance cameras produce images lower resolution satellites many producing stream images intervals like one second london said six million cameras producing stream internet web traffic switching node middle internet receives streams ip packets many inputs routes outputs normally job switch transmit data retain query tendency put capability switch eg ability detect denialofservice attacks ability reroute packets based information congestion network websitesreceivestreamsofvarioustypes forexamplegooglereceivessev eral hundred million search queries per day yahoo accepts billions clicks per day various sites many interesting things learned streams example increase queries like sore throat enables us track spread viruses sudden increase click rate link could 134 chapter 4 mining data streams indicate news connected page could mean link broken needs repaired 413 stream queries therearetwowaysthatqueriesgetaskedaboutstreams weshowinfig41a placewithintheprocessorwherestandingqueriesarestored thesequeriesare sense permanently executing produce outputs appropriate times example 41 stream produced oceansurfacetemperature sen sor mentioned beginning section 412 might standing query output alert whenever temperature exceeds 25 degrees centigrade query easily answered since depends recent stream element alternativelywemighthaveastandingquerythateachtimeanewreading arrives produces average 24 recent readings query also canbeansweredeasilyifwestorethe24mostrecentstreamelements whena new streamelementarriveswe candropfromthe workingstore 25thmost recent element since never needed unless standing query requires another querywe mightask maximumtemperature everrecordedby sensor answer query retaining simple summary maximum stream elements ever seen necessary record entire stream new stream element arrives compare stored maximum set maximum whichever larger answer query producing current value maximum similarly want average temperature time record two values number readings ever sent stream sum readings adjust values easily time new reading arrives produce quotient answer query form query adhoc question asked current state stream streams store streams entirety normally cannot expect answer arbitrary queries streams idea kind queries asked adhoc query interface prepare storing appropriate parts summaries streams example 41 want facility ask wide variety adhoc queries common approach store sliding window stream working store sliding window recent n elements stream n elements arrived within last time units eg one day regardeachstreamelement asa tuple cantreatthe window asa relation query sql query course streammanagement system must keep window fresh deleting oldest elements new ones come 41 stream data model 135 example 42 web sites often like report number unique users pastmonth ifwethink ofeachloginasastreamelementwecanmaintain window logins recent month must associate arrival time login know longer belongs window think window relation loginsname time simple get number unique users past month sql query select countdistinctname logins time constant represents time one month current time note must able maintain entire stream logins past month working storage however even largest sites data terabytes surely stored disk 414 issues stream processing proceeding discuss algorithms let us consider constraints whichweworkwhendealingwithstreams firststreamsoftendeliverelements veryrapidly wemustprocesselementsinrealtimeorwelosetheopportunity process without accessing archivalstorage thus often important streamprocessing algorithm executed main memory without access secondary storage rare accesses secondary storage moreoverevenwhenstreamsareslowasinthesensordataexample ofsection412theremaybemanysuchstreams evenifeachstreambyitself canbeprocessedusingasmallamountofmainmemorytherequirementsofall streams together easily exceed amount available main memory thus many problems streaming data would easy solve enough memory become rather hard require invention new techniques order execute realistic rate machine realistic size two generalizations stream algorithms worth bearing mind read chapter often much efficient get approximate answer problem exact solution asinchapter 3avarietyoftechniques relatedtohashingturnoutto useful generally techniques introduce useful randomness algorithms behavior order produce approximate answer close true result 136 chapter 4 mining data streams 42 sampling data stream first example managing streaming data shall look extracting reliable samples stream many stream algorithms trick involves using hashing somewhat unusual way 421 motivating example thegeneralproblemweshalladdressisselectingasubsetofastreamsothatwe ask queries selected subset answers statistically representative stream whole know queries askedthenthereareanumberofmethodsthatmightworkbutwearelooking technique allow adhoc queries sample shall look particular problem general idea emerge running example following search engine receives stream queriesanditwouldliketostudythebehavioroftypicalusers1 weassumethe stream consists tuples user query time suppose want answer queries fraction typical users queries repeated pastmonth assume alsothatwe wishto store only110thofthe stream elements theobviousapproachwouldbetogeneratearandomnumbersayaninteger 0 9 response eachsearchquery store tuple random number 0 user average 110th queries stored statistical fluctuations introduce noise data users issue many queries law large numbers assure us users fraction quite close 110th queries stored however scheme gives us wrong answer query asking average number duplicate queries user suppose user issued search queries one time past month search queries twice searchqueriesmorethantwice ifwehavea110thsampleofqueriesweshall see sample user expected s10 search queries issued search queries issued twice d100 appear twice sample fraction times probability occurrences querywillbe inthe 110thsample ofthe queriesthatappeartwice inthe full stream 18d100 appear exactly see note 18100 probability one two occurrences 110thof stream selected 910th selected correct answer query fraction repeated searches dsd howevertheanswerweshallobtainfromthesampleisd10s19d toderivethelatterformulanotethatd100appeartwicewhiles1018d100 appearonce thusthe fractionappearingtwice inthe sampleis d100divided 1whileweshallrefertousersthesearchenginereallyreceivesipaddressesfromwhich thesearch querywasissued weshallassumethat ipaddresses identifyunique users whichisapproximatelytruebutnotexactlytrue 42 sampling data stream 137 d100s1018d100 ratio d10s19d positive values dsdd10s19d 422 obtaining representative sample query section 421 like many queries statistics typical users cannot answered taking sample users search queries thus muststrive pick 110thofthe usersandtake alltheir searchesfor sample taking none searches users store list users whether sample could following time search query arrives stream look user see whether sample add search query sample however record ever seen user generate random integer 0 9 number 0 add user list value number 0 add user value thatmethodworksas longas wecanaffordto keepthe listofallusersand theirinoutdecisioninmainmemorybecause thereisnttime togotodisk every search arrives using hash function one avoid keeping list users hash user name one ten buckets 0 9 user hashes bucket 0 accept searchquery sample note actually store user bucket fact data buckets effectively use hash function random numbergeneratorwiththeimportantpropertythatwhenappliedtothesame userseveraltimeswealwaysgetthe samerandomnumber thatiswithout storing inout decision user reconstruct decision time search query user arrives generally obtain sample consisting rational fraction ab users hashing user names b buckets 0 b1 add search query sample hash value less 423 general sampling problem running example typical following general problem stream consists tuples n components subset components key componentsonwhichtheselectionofthesamplewillbebased inourrunning example three components user query time user key however could also take sample queries making query key even take sample userquery pairs making components form key take sample size ab hash key value tuple b buckets accept tuple sample hash value less key consists one component hash function needs combine values components make single hashvalue 138 chapter 4 mining data streams result sample consisting tuples certain key values selectedkey values willbe approximatelyab ofallthe key values appearing stream 424 varying sample size often sample grow stream enters system running example retain search queries selected 110th users forever time goes searches users accumulated new users selected sample appear stream budget many tuples stream stored sample fraction key values must vary lowering time goes orderto assure thatat alltimes sample consistsof alltuples froma subset ofthe key values choose hashfunction h fromkey values largenumberofvalues01b1 wemaintainathresholdtwhichinitially largest bucket number b1 times sample consists tuples whose key k satisfies hk new tuples stream added sample satisfy condition number stored tuples sample exceeds allotted space lowerttot1andremovefromthesampleallthosetupleswhosekeyk hashes efficiency lower 1 remove tuples severalof highest hash values whenever need throw key values sample efficiency obtained maintaining index hash value find tuples whose keys hash particular value quickly 425 exercises section 42 exercise 421 suppose stream tuples schema gradesuniversitycourseid studentid grade assume universities unique courseid unique within uni versity ie different universities may different courses id eg cs101 likewise studentids unique within university different universities may assign id different students suppose want answer certain queries approximately 120th sample data queries indicate would construct sample tell key attributes universityestimate averagenumber students course b estimate fraction students gpa 35 c estimate fractionof courseswhereat leasthalfthe students gota 43 filtering streams 139 43 filtering streams another common process streams selection filtering want accept tuples stream meet criterion accepted tuples passed another process stream tuples dropped selection criterion property tuple calculated eg first component less 10 selection easy problem becomes harder criterion involves lookup membership set especially hard set large store main memory section shall discuss technique known bloom filtering way eliminate tuples meet criterion 431 motivating example let us start running example illustrates problem suppose set one billion allowedemail addresses allow believe spam stream consists pairs email address email since typicalemailaddressis20bytes ormoreitis notreasonableto store inmainmemory thuswecaneitherusedisk accessesto determinewhether ornottoletthroughanygivenstreamelementorwecandeviseamethodthat requires main memory availableand yet filter undesired stream elements suppose arguments sake one gigabyte available main memory technique knownas bloom filtering use main memory bit array case room eight billion bits since one byte equalseightbits deviseahashfunctionhfromemailaddressestoeightbillion buckets hash member bit set bit 1 bits array remain 0 since one billion members approximately 18th bits 1 exact fraction bits set 1 slightly less 18th possible two members hash bit shall discusstheexactfractionof1sinsection433 whenastreamelementarrives hash email address bit email address hashes 1 let email email address hashes 0 certain address drop stream element unfortunately spam email get approximately 18th stream elements whose email address happen hash bitwhosevalueis1andwillbeletthrough neverthelesssincethe majorityof emails spam 80 according reports eliminating 78th thespamisasignificantbenefit moreoverifwewanttoeliminateeveryspam need check membership good bad emails get filter checks require use secondary memory access arealso optionsas shall see study generalbloomfilteringtechnique asasimpleexamplewecoulduseacascade 140 chapter 4 mining data streams filters would eliminate 78th remaining spam 432 bloom filter bloom filter consists 1 array n bits initially 0s 2 collection hash functions h h h hash function maps 1 2 k key values n buckets corresponding n bits bitarray 3 set key values purpose bloom filter allow stream elements whose keys rejecting stream elements whose keys initialize bit array begin bits 0 take key value hash using k hash functions set 1 bit h k hash function h key value k test key k arrives stream check h kh kh k 1 2 k 1s bitarray 1s let stream element one bits 0 k could reject stream element 433 analysis bloom filtering key value element surely pass bloom filter however key value might still pass need understand calculate probability false positive function n bitarray length number members k number hash functions model use throwing darts targets suppose x targets darts dart equally likely hit target throwing darts many targets expect hit least analysis similar analysis section 342 goes follows probability given dart hit given targetis x1x probabilitythatnone ofthe dartswillhit giventargetis x1 x write expression 1 1x xy x usingtheapproximation11 1eforsmallrecallsection135 weconcludethattheprobabilitythatnoneoftheydartshitagiventarget eyx 43 filtering streams 141 example 43 consider running example section 431 use calculation get true expected number 1s bit array think bit target member dart probability given bit 1 probability corresponding target hit one darts since one billion members 109 darts eight billion bits x8109 targets thus probability given target hit eyx e18 probability hit 1e18 quantity 01175 section 431we suggestedthat 18 0125is goodapproximationwhich exact calculation apply rule general situation set members array n bits k hash functions number targets x n number darts km thus probability bit remains 0 ekmn want fraction 0 bits fairly large else probability nonmember hash least 0 becomes small many false positives example might choose k number hash functions nm less probability 0 least e1 37 general probability false positive probability 1 bit 1ekmn raised kth power ie 1ekmnk example 44 inexample43wefoundthatthefractionof1sinthearrayof ourrunningexampleis01175andthisfractionisalsotheprobabilityofafalse positive nonmember pass filter hashes 1 probability 01175 suppose used array used two different hash functions situation corresponds throwing two billion darts eightbillion targetsandthe probabilitythat bit remains 0 e14 inorder false positive nonmember must hash twice bits 1 probability 1e142 approximately 00493 thus adding secondhash function running example improvementreducing falsepositive rate 01175 00493 434 exercises section 43 exercise 431 situation running example 8 billion bits 1 billion members set calculate falsepositive rate use three hash functions use four hash functions exercise 432 suppose n bits memory available set members instead using k hash functions could divide n bits k arraysand hash array function n k probability false positive compare using k hash functions single array 142 chapter 4 mining data streams exercise 433 function n number bits number members set number hash functions minimizes false positive rate 44 counting distinct elements stream section look third simple kind processing might want stream previous examples sampling filtering somewhat tricky want reasonable amount main memory soweuseavarietyofhashingandarandomizedalgorithmtogetapproximately want little space needed per stream 441 countdistinct problem suppose stream elements chosen universal set would like know many different elements appeared stream counting either beginning stream known time past example 45 useful example ofthis problemconsider website gath eringstatisticsonhowmanyunique usersithasseenineachgivenmonth universalsetisthe setofloginsforthatsite andastreamelementisgenerated eachtime someone logsin measure appropriatefor site like amazon typical user logs unique login name similar problem web site like google require login issue search query may able identify users ip address send query 4 billion ip addresses2 sequences four 8bit bytes serve universal set case theobviouswaytosolvetheproblemistokeepinmainmemoryalistofall theelementsseensofarinthestream keeptheminanefficientsearchstructure hash table search tree one quickly add new elements check whether element arrived stream already seen aslongasthenumberofdistinctelementsisnottoogreatthis structure fit main memory little problem obtaining exact answer question many distinct elements appear stream howeverif number distinct elements great many streams need processed eg yahoo wants count number unique users viewing pages month cannot store needed data main memory several options could use machines machine handling one several streams could store data structure secondary memory batch stream elements whenever brought disk block main memory would many tests updates performed data block could use strategy discussed section 2atleastthatwillbethecaseuntilipv6becomes thenorm 44 counting distinct elements stream 143 estimate number distinct elements use much less memory number distinct elements 442 flajoletmartin algorithm possible estimate number distinct elements hashing ele ments universalsetto bitstring sufficiently long lengthof bitstringmustbe sufficientthattherearemorepossibleresultsofthehash function elements universal set example 64 bits sufficientto hashurls shallpickmanydifferenthashfunctions andhash eachelementofthestreamusingthesehashfunctions theimportantproperty ofahashfunctionisthatwhenappliedtothesameelementitalwaysproduces result notice property also essential sampling technique section 42 idea behind flajoletmartin algorithm different elements see stream different hashvalues shall see see different hashvalues becomes likely one values unusual particular unusual property shall exploit value ends many 0s although many options exist whenever apply hash function h stream element bit string ha end number 0s possibly none call number tail length h let r maximum tail length seen far stream shall use estimate 2r number distinct elements seen stream estimate makes intuitive sense probability given stream elementa hasha ending atleastr 0sis 2r suppose arem distinct elements stream thenthe probability none ofthem tail length least r 12rm sort expression familiar rewrite 12r2r m2r assuming r reasonably large inner expression form 11 approximately 1e thus probability finding stream element many r 0s end hash value em2r conclude 1 much largerthan 2r probability shall find tail length least r approaches 1 2 much less 2r probability finding tail length least r approaches 0 conclude two points proposed estimate 2r recall r largest tail length stream element unlikely either much high much low 144 chapter 4 mining data streams 443 combining estimates unfortunatelythereisatrapregardingthestrategyforcombiningtheestimates number distinct elements obtain using many dffierent hash functions first assumption would take average values 2r get hash function shall get value approaches true hash functions use however case reason influence overestimate average consider value r 2r much larger probabilitypthatwe shalldiscoverr largestnumberof0satthe end hash value stream elements probability finding r1 largestnumber of0s insteadis atleastp2 howeverif increase 1 number 0s end hash value value 2r doubles consequently contribution possible large r expectedvalueof2r growsasrgrowsandtheexpectedvalueof2r isactually infinite3 another way combine estimates take median estimates medianis notaffected occasionaloutsizedvalue of2r sothe worry described averageshould carry median unfortu nately median suffersfromanother defect alwaysa powerof 2 thus matter many hash functions use correct value two powers 2 say 400 impossible obtain close estimate solution problem however combine two methods first group hash functions small groups take average take median averages true occasional outsized 2r bias groups make large however taking median group averages reduce influence effect almost nothing moreover groups large enough averages essentially number enables us approach true value long use enough hash functions order guarantee possible average obtained groups size least small multiple log 2 444 space requirements observe read stream necessary store elements seen thing need keep main memory one integer per hash function integer records largest tail length seen far hash function stream element processing one stream could use millions hash functions far need get 3technically since hash value bitstring finite length contribution 2r rs larger length hash value however effect enoughtoavoidtheconclusionthattheexpected valueof2r ismuchtoolarge 45 estimating moments 145 closeestimate onlyifwearetryingto processmanystreamsatthe time wouldmainmemoryconstrainthe numberofhashfunctions wecouldassociate one stream practice time takes compute hash values eachstreamelementwouldbe moresignificantlimitationonthe number hash functions use 445 exercises section 44 exercise 441 suppose streamconsists integers 3 1 4 1 5 9 2 65 ourhashfunctions willallbe ofthe formhxaxb mod 32for b treat result 5bit binary integer determine taillength foreachstreamelement andthe resulting estimate ofthe number distinct elements hash function hx2x1 mod 32 b hx3x7 mod 32 c hx4x mod 32 exercise 442 yousee anyproblemswith choiceofhashfunctions exercise 441 advice could give someone going use hash function form hxaxb mod 2k 45 estimating moments section consider generalizationof problem counting distinct elements stream problem called computing moments involves distribution frequencies different elements stream shall define moments orders concentrate computing second moments general algorithm moments simple extension 451 definition moments suppose streamconsists ofelements chosenfroma universalset assume universal set ordered speak ith element let number occurrences ith element kthorder moment kth moment stream sum k example 46 the0thmomentisthesumof1foreachm thatisgreaterthan 04 0th moment count number distinct elements stream use method section 44 estimate 0th moment stream 4technicallysincem couldbe0forsomeelementsintheuniversalsetweneedtomake explicit definition moment 00 taken 0 moments 1 thecontributionofm isthatare0issurely0

